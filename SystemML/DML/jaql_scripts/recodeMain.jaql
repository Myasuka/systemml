//
// Author: reinwald@almaden.ibm.com
// Date: July, 2011
//
// Main Driver
//
//   jaqlshell -c -jp "." -j /home/hadoop/bin/SystemML.jar -e "cfg='WF.json'"
//
// Main Flow:
//  
//    Initialize
//    Pre-Processing
//       Generate metadata
//       Enumerate records and add derived attributes
//       Generate recode maps for each attribute
//       Recode all the data 
//       Generate the matrices
//    SystemML
//       Invoke SystemML for univariate stats
//       Invoke SystemML for bivariate stats
//    Post-Processing
//       Collect univariate statistics
//       Collect bivariate statistics
//
//    DummyCoding for Logistic Regression
//

//
// --------------------------------------------------------------------------------
// Initialize
// --------------------------------------------------------------------------------
//

dml = javaudf("dml.JaqlDML");

import recode as rcd;

setOptions( { conf: { "mapred.reduce.tasks.speculative.execution" : false } });
setOptions( { conf: { "mapred.child.java.opts" : "-Xmx2000M" } });

cfgFile = read (file(cfg));

inputDir   = singleton (cfgFile -> transform $.inputDir);
datafile   = singleton (cfgFile -> transform $.datafile);
outputDir  = singleton (cfgFile -> transform $.outputDir);
localDir   = singleton (cfgFile -> transform $.localDir);

dta        = inputDir + "/" + datafile;
dta_drv    = outputDir + "/" + datafile + ".dotv";
dta_rcd    = outputDir+"/rcd/"+datafile+".rcd";

mta        = localDir + "/" + datafile + "_metadata.json";
dmta       = outputDir + "/" + datafile + "_metadata_derived.json";

univar     = outputDir + "/" + datafile + "_univar.json";
bivar      = outputDir + "/" + datafile + "_bivar.json";


// --------------------------------------------------------------------------------
// Generate metadata
// --------------------------------------------------------------------------------

rcd::genMetadata (hdfs(dta), file(mta));

// Manually update the metadata json file to 
//   A. attributeKind (e.g. severity to "ordinal")
//   B. attributeType (e.g. "date")
//   C. remove attributes (e.g. "detail")

rcd::genDrvdMetadata (file(mta), hdfs(dmta));


// --------------------------------------------------------------------------------
// Enumerate records and add derived attributes
// --------------------------------------------------------------------------------

// Make sure, that metadata is pinned in memory. This code *cannot* be wrapped in a 
// function because of "unroll" in genDerivedRecord and ":=" not supported in functions.

mdmta := read (hdfs(dmta));

rcd::parallelEnumerate(hdfs(dta)) 
   -> transform {rowId: $[0] + 1, $[1].* }
   -> transform {rowId: $.rowId, record(rcd::genDerivedRecord ($, mdmta)).*}
   -> write(hdfs(dta_drv)); 


// --------------------------------------------------------------------------------
// Generate recode maps for each attribute
// --------------------------------------------------------------------------------

mdmta := read (hdfs(dmta));

rcd::genRcdMaps (dta_drv, mdmta, outputDir+"/rcd");

// Update metadata (distinct counts) for derived attributes

mdmta := read (hdfs(dmta));
mdmta 
  -> transform each a 
      ( if (isnull(a.attributeDistinctCount) and a.attributeKind != "scale" ) (
          dstcnt = read (hdfs(outputDir+"/rcd/"+a.attributeName+".rcd_map")) -> count(),
 	  { a{*-.attributeDistinctCount}
           ,attributeDistinctCount : dstcnt
          }			
        ) else (
          a
        )
      )
  -> write (hdfs(dmta));

// For ordinal attributes, write JSON files for manual editing of order. 
// And write back modified json file as updated rcd_map file after editing

mdmta -> filter $.attributeKind == "ordinal" -> transform rcd::getRcdMap (outputDir+"/rcd", $.attributeName, localDir);
mdmta -> filter $.attributeKind == "ordinal" -> transform rcd::putRcdMap (localDir, outputDir+"/rcd", $.attributeName);


// --------------------------------------------------------------------------------
// Recode all the data 
// --------------------------------------------------------------------------------

// outputDir has the recode maps in it.

mdmta := read (hdfs(dmta));

rcd::recodeData (dta_drv, mdmta, outputDir+"/rcd", dta_rcd);


// --------------------------------------------------------------------------------
// Generate the matrices
//    <rowId> <colId> <recodeId>
//    NULL cells are written as "NaN".
--------------------------------------------------------------------------------

// Write matrix w/ all columns, and write all single column matrices
//    Input: <datafile>.rcd       Output: <datafile>.mtx
//                                Output: <datafile_attributeName>.mtx

mdmta := read (hdfs(dmta));

rcd::writeAllMtxs(dta_rcd, mdmta, outputDir+"/mtx", datafile);


// --------------------------------------------------------------------------------
// Invoke SystemML for univariate stats
// --------------------------------------------------------------------------------

// For each attribute, we produce output files
//    .../stats/uni/<attributeName>.R
//    .../stats/uni/<attributeName>.NC
//    .../stats/uni/<attributeName>.PC
//    .../stats/uni/<attributeName>.MODE

mdmta := read(hdfs(dmta));    // pin in memory to avoid JAQL MR

dmlscript := strJoin( read(lines("file:///local2/WF/categorical.dml")), "\n");

mdmta
   -> filter $.attributeKind != "scale"
   -> transform  {args : " '" + outputDir + "/mtx/" + datafile + "_" + $.attributeName + ".mtx'"  
                        +"  " + string($.caseCount)
   	                +" '" + outputDir + "/stats/uni/" + $.attributeName + ".R'"
  	                +" '" + outputDir + "/stats/uni/" + $.attributeName + ".NC'"
	                +" '" + outputDir + "/stats/uni/" + $.attributeName + ".PC'"
	                +" '" + outputDir + "/stats/uni/" + $.attributeName + ".MODE'"
                 }
 -> transform dml (dmlscript, $.args);


// --------------------------------------------------------------------------------
// Invoke SystemML for bivariate stats
// --------------------------------------------------------------------------------

// For the generated attribute pairs, we produce output files
//    .../stats/bi/<attributeName1>.<attributeName2>.CTABLE
//    .../stats/bi/<attributeName1>.<attributeName2>.CHISQ
//    .../stats/bi/<attributeName1>.<attributeName2>.CRAMERSV


dmlscript := strJoin( read(lines("file:///local2/WF/categoricalCategorical.dml")), "\n");

mdmta := read(hdfs(dmta));    // pin in memory to avoid JAQL MR

rcd::enumPairs (mdmta)
   -> transform {args : " '" + outputDir + "/mtx/" + datafile + "_" + $.a1 + ".mtx'"  
                       +" '" + outputDir + "/mtx/" + datafile + "_" + $.a2 + ".mtx'"  
                       +"  " + string($.nrows)
   	               +" '" + outputDir + "/stats/bi/" + $.a1 + "." + $.a2 + ".CTABLE'"
  	               +" '" + outputDir + "/stats/bi/" + $.a1 + "." + $.a2 + ".CHISQ'"
	               +" '" + outputDir + "/stats/bi/" + $.a1 + "." + $.a2 + ".CRAMERSV'"
      		}
   -> transform dml (dmlscript, $.args);


//--------------------------------------------------------------------------------
// Post-Preprocessing: Collect Stats
//--------------------------------------------------------------------------------

//
// Collect univariate statistics
//

lsuniR    := ls (outputDir + "/stats/uni/*.R",    directory=true);
lsuniMode := ls (outputDir + "/stats/uni/*.MODE", directory=true);
			   {a.*, Mode: "not or not yet computed"}
                        )
                       )
   -> write (hdfs(univar));		       

// Pretty print Univar into DEL file

read (hdfs(univar))
   -> sort by [$.attributeName asc]
   -> transform [$.attributeName, $.attributeKind, $.NbrOfCategories, $.Mode]
   -> write (del(outputDir + "/" + datafile + "_univar.del"));
 

//
// Collect bivariate statistics
//

lsbi  := ls (outputDir + "/stats/bi/*", directory=true);

read(hdfs(dmta))
   -> rcd::enumPairs()
   -> transform each a (filebase = outputDir + "/stats/bi/", 
			isThere = lsbi -> filter each f endsWith (f.path , a.a1 + "." + a.a2 + ".CRAMERSV") -> exists(),
      		        if (isThere ) (
                           chisq    = singleton (read (del(filebase + a.a1 + "." + a.a2 + ".CHISQ"	 , {delimiter: " "})) -> transform $[2]), 
                           cramersv = singleton (read (del(filebase + a.a1 + "." + a.a2 + ".CRAMERSV", {delimiter: " "})) -> transform $[2]), 
			   {
			     Attribute1     : a.a1
			    ,Attribute2     : a.a2
			    ,NbrOfCases     : "null"
			    ,CTable_Nrow    : "null"
			    ,CTable_Ncol    : "null"
			    ,DegreeOfFreedom: "null"
			    ,ChiSq          : chisq
			    ,CramersV       : cramersv
                           }
		        ) else (
			   {
			     Attribute1     : a.a1
			    ,Attribute2     : a.a2
			    ,NbrOfCases     : "null"
			    ,CTable_Nrow    : "null"
			    ,CTable_Ncol    : "null"
			    ,DegreeOfFreedom: "null"
			    ,ChiSq          : "not or not yet computed"
			    ,CramersV       : "not or not yet computed"
                           }
                        )
                       )
   -> write (hdfs(bivar));		       


// Pretty print bivar into DEL file

read (hdfs(bivar))
   -> sort by [$.Attribute1 asc]
   -> transform [$.attributeName, $.attributeKind, $.NbrOfCategories, $.Mode]
   -> write (del(outputDir + "/" + datafile + "_bivar.del"));




//--------------------------------------------------------------------------------
// DummyCoding for Logistic Regression
//--------------------------------------------------------------------------------

//
// Create a vector that records cumulative counts of number of distinct
// values: set : A(1) = 0; A(i) = A(i-1) + #distinct(i-1) where i=1...number
// of categorical variables. We use this to have "ranges of columns" for the
// different matrices.
// Exclude the scale variables.
//

mdmta := read (hdfs(dmta));
mdmta
   -> sort by [$.attributeColumnId asc]
   -> slidingWindow (fn (first, cur) first.attributeColumnId == 1, 
                     fn (curr, last) last.attributeColumnId < curr.attributeColumnId
                    ) 
   -> transform 
             {$.cur.*, 
              attributeRangeStartIndex : 
	         if ($.cur.attributeKind == "scale") (
		    null
                 ) else if (isnull ($.window[0]) ) (
   	            0
                 ) else (
                    $.window 
                       -> group into 
                             sum (if ($.window[].attributeKind == "scale") (
                                           0
                                        ) else (
                                           $.attributeDistinctCount
                                        )
                                  ) 
                       -> singleton()
                 )
             }
   -> sort by [$.attributeColumnId asc]
   -> write (hdfs(dmta));


//
// Dummy code the matrix: <datafile>_dummy.mtx
//   Transform <datafile>.mtx from <rowId> <colId> <recodeId> to  <rowId> <A(colId)+recodeID> 1  
//   A has the cumulative counts.
//   Only include attributes that are not scalar.
//   <i,j,v>'s with NaN's are removed.
//
//   !!!! modify this query to work off the RCD file as it will perform better
//   (less key-value pairs)


readIJV = fn (fname)
(
   read (del(fname, {delimiter: " ", quoted: false, schema:schema[long, long, string] }))
// -> transform [$[0], $[1], if (not isnull($[2])) ($[2]) else ("NaN")]
);


mdmta := read (hdfs(dmta));
keep = mdmta -> filter $.attributeKind != "scale" -> transform $.attributeColumnId;
readIJV(outputDir + "/mtx/" + datafile + ".mtx")
   -> filter $[1] in keep
   -> filter $[2] != "NaN"
   -> transform each ijv (
      		 idx = singleton (mdmta -> filter $.attributeColumnId == ijv[1]).attributeRangeStartIndex,
                 [ijv[0], idx + long(ijv[2]), 1]
                )
   -> rcd::writeIJV (outputDir + "/mtx/" + datafile + ".dummy.mtx");


//
// Query to produce a column index map that shows start/stop column index and attributeName
//

mdmta := read (hdfs(dmta));
mdmta
   -> filter $.attributeKind != "scale" 
   -> transform {Idx1 : $.attributeRangeStartIndex + 1, 
                 Idx2 : $.attributeRangeStartIndex + $.attributeDistinctCount, 
                 Name : $.attributeName} 
   -> sort by [$.Idx1 asc]
   -> transform [$.Idx1, $.Idx2, $.Name]
   -> write (del(outputDir + "/mtx/" + datafile + ".dummy.colIdx.map", {delimiter: " ", quoted: false}));

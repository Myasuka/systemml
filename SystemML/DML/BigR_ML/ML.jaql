dummy1 := addClassPath("/opt/ibm/biginsights/machine-learning/metaDataParse.jar");
dummy2 := addClassPath("/opt/ibm/biginsights/machine-learning/SystemML.jar");

dml = javaudf("com.ibm.bi.dml.api.JaqlUdf", {hasSideEffects:"true"});
dmlException = javaudf("com.ibm.bi.dml.api.AppExceptionJaqlUdf");

scripts_dir = "file:/opt/ibm/biginsights/machine-learning/scripts";

lm = fn(x_file, y_index, intercept, maxiter, tol, lambda, results_file) (
    dmlScript = scripts_dir + "/lm_new.dml",
    args = "-config=" + scripts_dir + "/SystemML-config.xml"
           + " -args " 
           + x_file 
           + " " + serialize(y_index)
           + " " + serialize(intercept) 
           + " " + serialize(maxiter)
           + " " + serialize(tol) 
           + " " + serialize(lambda)
           + " " + results_file,   
	
    dml(dmlScript, args)
);

cleanupML = fn(results_file)(
    hdfsShell("-rmr " + results_file),
    hdfsShell("-rmr " + results_file + ".mtd"),
    0
);

jsonLines = fn(location)
        lines ( location,
            inoptions = { converter: "com.ibm.jaql.io.hadoop.converter.FromJsonTextConverter"},
            outoptions = { converter: "com.ibm.jaql.io.hadoop.converter.ToJsonTextConverter"});

// --------------------------------------------------------------------------------
// Parallel Enumerate
// --------------------------------------------------------------------------------

// helper function for parallel enumeration

runningSum = fn(input, dataFn, intoFn) 
   input -> runningCombine(0, fn(sum,i) sum + dataFn(i), intoFn);

worklist = fn(array) 
  { type: 'array', inoptions: { array }};

parallelEnumerate = fn(fd)
(
   annotatedSplits =
      read( worklist(inputSplits(fd)) )
      -> transform each split { split, n: readSplit(fd, split) -> count()}
      -> runningSum( fn(i) i.n, fn(sum,i) { i.split, n: sum - i.n } ),

      read (worklist(annotatedSplits))
      -> expand each s (readSplit(fd, s.split) -> enumerate() -> transform [s.n+$[0], $[1]])
);

// --------------------------------------------------------------------------------
// Recode the data using metadata and recode maps, and write recoded data.
// --------------------------------------------------------------------------------

// Think of the enumerated data file as the fact table, and all the recode
// maps as dimension tables. Do a starjoin over all attributes in the fact
// table for recoding. This requires a metadata-driven starjoin for variable
// number of attributes. We pivot fact to <rowid, attrName, attrValue>, and 
// union with pivot-ted dimension tables. Group by <attrName, attrValue>, 
// replace attrValue w/ recodeId. Then, pivot back the fact table. 

// A. Use JAQL CompositeInputAdapter for veritcalized dimension recode maps 
//    to make sure that parallelizes
//    (error is Java Heap Space on single node)
//    --> use attr_descs (descriptors) and CompositeInputAdapter
// B. Use "expand each" for triplet representation of fact to avoid Jaql problem in union
//    query (cast JsonArray to JsonRecord for $.rowid). It gets confused what "$" is.

makeGlob = fn(ps) (

    strcat("/{", ps 
                 -> transform strcat(".",$) 
                 -> strJoin(","), 
           "}")

  );

decodeWeights = fn(weights_file, json_data_loc, label_index)(
    json_data_loc_minus_hdfs_prefix =
    if (strPos(json_data_loc, ":") < 0)
        json_data_loc
    else(
       		// This is the case where we get hdfs:// as the prefix, we need to chop it
       		// otherwise the later recode step code will get into trouble
       		strTmp = substring(json_data_loc, strPos(json_data_loc, "//") + 2, strLen(json_data_loc)),
       		substring(strTmp, strPos(strTmp, "/"), strLen(strTmp))   		
    ),

    // DFS Path + filename for the JSON text enhanced metadata file
    metadata_file = strReplace(json_data_loc, regex('.json'), '.meta.json'),
    
    // DFS Path for the recode map directory 
    recodeMapDir = strReplace(json_data_loc_minus_hdfs_prefix, regex('.json'), '.map'),

    meta = read(jsonText(metadata_file)),

    weightsMetadata =
    meta
    -> filter $."colId" != label_index
    -> transform {"name": $."name",
                  "colId": $."colId",
                  "kind": $."kind",
                  "distCount": $."distCount"}
    -> sort by [$."colId"]
    -> prevElement()
    -> runningSum(fn(i) if(i."cur"."colId" == 1) 0
                        else( if(i."prev"."kind" == "scale") 1
                              else i."prev"."distCount" ),
                  fn(sum, i) {"name": i."cur"."name",
                              "kind": i."cur"."kind",
                              "distCount": i."cur"."distCount",
                              "colOffset": sum})
    -> transform each rec (if(rec."kind" == "scale")(
                             [{"name": rec."name",
                               "kind": rec."kind",
                               "id": null,
                               "colId": rec."colOffset"+1}]
                          )else(
                             range(1,rec."distCount")
                             -> transform {"name": rec."name",
                                           "kind": rec."kind",
                                           "id": $,
                                           "colId": rec."colOffset"+$}))
    -> expand,

    numWeights =
    (weightsMetadata
    -> group into max($."colId"))[0],

    fullWeightsMetadata =
    weightsMetadata
    -> union([{"name": "(Intercept)",
               "kind": null,
               "id": null,
               "colId": 1+numWeights}]),

    nominal_map_paths =
    meta
    -> filter $."kind" == "nominal"
    -> transform {"name": $."name",
                  "path": recodeMapDir + "/" + $."name" + ".map.json"},

    recodeMaps =
    ls(makeGlob(nominal_map_paths[*]."path"))
    -> transform $.path
    -> batch(10)
    -> arrayRead()
    -> expand
    -> transform (read(jsonLines($)))
    -> expand
    -> transform {"name": $[0], "value": $[1], "id": $[2]},

    weights = 
    read(del(weights_file,
             {delimiter: " ",
              quoted: false,
              schema: schema[long, long, double]})),

    parameter_names_with_colIds =
    join preserve fullWeightsMetadata, recodeMaps
    where fullWeightsMetadata."name" == recodeMaps."name"
        and fullWeightsMetadata."id" == recodeMaps."id"
    into {"name": fullWeightsMetadata."name",
          "kind": fullWeightsMetadata."kind",
          "value": recodeMaps."value",
          "colId": fullWeightsMetadata."colId"},
    
    join parameter_names_with_colIds, weights
    where parameter_names_with_colIds."colId" == weights[0]
    into [parameter_names_with_colIds."name", 
          parameter_names_with_colIds."value", 
          weights[2]]
);

// *******************************************************************************

//persistJsonFile(data_loc, mtd_loc, cnames, ctypes) (

//);

// *******************************************************************************

genMTDJson = fn(cnames, ctypes, mtd_file) (
    //cnames=['col1', 'col2', 'col3', 'col4'],
    //ctypes=['numeric', 'character', 'integer', 'logical'],

    txtOutOpt = {format: "org.apache.hadoop.mapred.TextOutputFormat",                      
                 converter: "com.ibm.jaql.io.hadoop.converter.ToJsonTextConverter"},

    cnames_en = cnames -> enumerate(),
    ctypes_en = ctypes -> enumerate(), 
    cnames_en 
       // can also keyMerge() but keyLookup always performs hash join.
       // it will be more efficient in this particular scenario since #cols likely to be small.
    -> keyLookup(ctypes_en)
    -> transform (if($[2] == "character" or $[2] == "logical" ) ( 
                      { "colID": $[0]+1,
                        "distCount": "null",
                        "kind": "nominal",
                        "name": $[1],
                        "recodedByBI": "No",
                        "type": "string"
                      } 
                  )
                  else(
                      if ($[2] == "numeric" or $[2] =="integer") (
                           { "colID": $[0]+1,
                             "distCount": "null",
                             "kind": "scale",
                             "name": $[1],
                             "recodedByBI": "No",
                             "type": "double"
                           } 
                      )
                      else (
                           {$.badFieldAccess}
                      )
                  )
                 )
    -> toArray()
    -> write(jsonText(mtd_file)) //write(seq(location=mtd_file, outoptions=txtOutOpt))
);

// *******************************************************************************

genRecodeMaps = fn(inputData, inputMetaData, outputMapDir) (

    //inputFormat='JSON',
    //inputHeader='No',
    //inputDelimiter=',',
    
    // construct schema
    schemaString = read(jsonText(inputMetaData)) 
                       -> transform { "text": strcat($.name,":",$.type)} 
                       -> transform values($) 
                       -> expand 
                       -> strJoin(","),
    eval = javaudf("com.ibm.systemML.dataTransformation.metaDataParse"),
    inputSchema = eval("schema {" + schemaString + "}"),
    
    // get the list of categorical attributes to recode
    meta = read(jsonText(inputMetaData)),
    keep = (meta
    -> filter $.kind != "scale" or $.type == "string")[*].name,
    
    // read the input data, and construct recode maps
    //read (del(location=inputData, schema=inputSchema, delimiter=inputDelimiter ) )
    read(jsonLines(inputData))
    -> expand(fields($))
    -> filter $[0] in keep
    -> group by d = $ into d
    -> group by aN = $[0] as dVals into (
        x = dVals[*][1] -> filter not isnull $,              
        attrKind = singleton(meta -> filter $.name == aN).kind,
        y = if (attrKind != "ordinal") 
        (
            x 
        ) else (
            x -> sort by [$ asc]
        )
        -> enumerate()
        // Transform to [name, recodeId, catValue]
        -> transform [aN, $[1], $[0] + 1], 
        y -> write(jsonLines(outputMapDir+aN+".map.json"))
    )

);

// *******************************************************************************

updateRecodedMeta = fn (metadataMem, attributesToRecode)
(
  join preserve orig in metadataMem, new in attributesToRecode
     where orig.name == new.name
   into {
      orig.name,
      "colId":(orig.colId),
      "kind":(orig.kind),
      "type":(orig.type),
      //At this point, we do not calculate what's the total distinct count of values.
      "distCount": (orig.distCount),
      "recodedByBI": (if (new.recodedByBI == "Yes") new.recodedByBI else orig.recodedByBI)
   }
   -> sort by [$.colId] 
);

recodeData = fn (enumeratedData, attributesToRecode, recodeMapDir, recodedData)
(
	attr_descs = attributesToRecode->transform {name: $.name,
                                  path: recodeMapDir+"/"+$.name+".map.json"},
   
   	attr_descs = if ( (attr_descs->count()) != (ls( 
					if ((attr_descs[*] -> count()) > 1) 
						(makeGlob(attr_descs[*].path))
					else
					(
						attr_descs[0].path
					) 
               	) -> count()))
		(dmlException('Cannot find some map files in the map directory'))
		else
		(
			attr_descs
		),
	union (
        
		// pivot fact: <rowId, attrName, attrValue, null>
		enumeratedData
			-> expand each dta (fields(dta{* - .rowId}) 
			-> transform each f [dta.rowId, f[0], f[1]]),
   
		// pivot ALL dimensions: <null, attrName, recodeId (attrValue), catValue >
		ls( if ((attr_descs[*] -> count()) > 1) 
				(makeGlob(attr_descs[*].path))
			else
			(
				attr_descs[0].path
			) 
       	)
				
		->  transform $.path
		//TODO: finding a better way to schedule mappers in parallel file read
		-> batch(10)
		-> arrayRead()
		//TODO: if we get problems reading one of the files
		-> expand -> transform (read(jsonLines($))) -> expand
		// Format of a map file is [attrName, recodeId, catValue]
		-> transform [null, $[0], $[1], $[2]]
	)
   
	// recode attrValue and produce: <rowId, attrName, recodeId>
	-> group by d = [$[1],$[2]] 
		into (m = max($[*][3]), 
		rids = $[*][0] -> filter not isnull($),
		// copy recodeId into grouped facts iterating over the rids
		rids -> transform each r [r, d[0], if (isnull m) d[1] else m]) 
	-> expand $
    // un-pivot fact
	-> group by r = $[0] 
	// for all <attrName, recodeId > of a given rowId
		into (x = record($ -> transform each f {(f[1]): f[2]}),
				{x.*, rowId: r} 
				)
	-> write(jsonLines(recodedData)),
	true
  
);

recodeBigFrame = fn(input_file, recodeAttrs) (
    input = 
    if (endsWith(input_file, ".json")) 
    (
       if (strPos(input_file, ":") < 0)
       (
           	input_file
       )
       else
       (
       		// This is the case where we get hdfs:// as the prefix, we need to chop it
       		// otherwise the later recode step code will get into trouble
       		strTmp = substring(input_file, strPos(input_file, "//") + 2, strLen(input_file)),
       		substring(strTmp, strPos(strTmp, "/"), strLen(strTmp))   		
       )
    )
    else 
    (
        dmlException('Unrecognizable file format, file name must be with json suffix')
    ),

    // DFS Path + filename for the JSON text enhanced metadata file
    metadata = strReplace(input, regex('.json'), '.meta.json'),
    
    // DFS Path for the recode map directory 
    recodeMapDir = strReplace(input, regex('.json'), '.map'),
    
    // DFS Path + filename for the JSON text recoded data generated by this script 
    // We always output to to same input data file (overwriting)
    output = input,
    
    // DFS Path + filename for a recode spec file provided by the user (optional)
    userAttrsFile = if (recodeAttrs == "" or recodeAttrs == "null")
    (
       null
    )
    else 
    (
      recodeAttrs
    ),

    setOptions( { conf: { "mapred.reduce.tasks.speculative.execution" : false } }),
    setOptions( { conf: { "mapred.child.java.opts" : "-Xms2000M -Xmx4000M" } }),
    setOptions( { conf: { "hadoop.job.history.user.location" : "none"} }),
    
    // Pin metadata in memory
    metadataMem = read(jsonText(metadata)),
    // Compute how many attributes to recode
    //attributesToRecode = computeToRecodeAttributes(metadataMem, userAttrsFile),
    
    attributesToRecode = read(jsonText(metadata))
                         -> filter $.type == "string" or $.kind != "scale"
                         -> transform ({$.name, "recodedByBI":"Yes"}),

    // construct schema
    schemaString = read(jsonText(metadata)) 
                       -> transform { "text": strcat($.name,":",$.type)} 
                       -> transform values($) 
                       -> expand 
                       -> strJoin(","),
    eval = javaudf("com.ibm.systemML.dataTransformation.metaDataParse"),
    inputSchema = eval("schema {" + schemaString + "}"),
    inputDelimiter = ',', 


    if (not isnull(attributesToRecode))
    (
       	inputData_FileDescriptor = jsonLines(input),
        
    	// --------------------------------------------------------------------------------
    	// Enumerate records
    	// --------------------------------------------------------------------------------
    
    	enumeratedData = parallelEnumerate(inputData_FileDescriptor) 
       		-> transform {rowId: $[0] + 1, $[1]{* - rowId} }, 
    
    	flag = recodeData (enumeratedData, attributesToRecode, recodeMapDir, output),
    
    	if (flag)
    	(
    		//---------------------------------------------------------------------------------
    		// Copy the metadata file
    		//---------------------------------------------------------------------------------
    		updatedMeta = updateRecodedMeta(metadataMem, attributesToRecode),
    
    		updatedMeta
                -> write(jsonText(strcat(metadata,"_temp"))),
            
            hdfsShell(strcat("-rmr ", metadata)),
            hdfsShell(strcat("-mv ", strcat(metadata, "_temp"), " ", metadata))
            
            //updatedMeta
       		//	-> write(jsonText(metadata))
       	)
       	else
       	(
       		dmlException('Recode process failed')
       	)
    )

);

genMatrixMTD = fn(input_file, matrix_mtd_file) (
    // DFS Path + filename for the JSON text recoded data
    infile = input_file,
    // Temp files used for computing number of rows and number of non-zero entries
    nnzTmp = strReplace(input_file, regex('.json'), '.nnztmp.json'),
    // DFS Path + filename for the meta data of the input data
    metaData = strReplace(input_file, regex('.json'), '.meta.json'),
    
    metadataStream = localRead(jsonText(metaData)),
    
    // Compute number of columns from metadata
    numCols = metadataStream -> group into count($) -> singleton(),

    // Pass #2: Create and write the SystemML matrix metadata file
    // First compute the number of rows in the original matrix
    read(jsonLines(infile))
    -> group into count($)
    -> transform
        {
           "data_type": "matrix",
           "value_type": "double",
           "rows": $,
           "cols": numCols,
           "nnz": (localRead(jsonLines(nnzTmp)))[0],
           "format": "text",
           "description": { "author": "SystemML" }
        }
    -> write (jsonLines (matrix_mtd_file))
);

genMatrixMTD2 = fn(input_file, matrix_mtd_file) (
    // DFS Path + filename for the JSON text recoded data
    infile = input_file,
    // Temp files used for computing number of rows and number of non-zero entries
    nnzTmp = strReplace(input_file, regex('.json'), '.nnztmp.json'),
    //Temp files used for computing number of rows and number of non-zero entries
    ncolsTmp = strReplace(input_file, regex('.json'), '.ncols.json'),
  
    // First compute the number of rows in the original matrix
    read(jsonLines(input_file))
    -> group into count($)
    -> transform
        {
           "data_type": "matrix",
           "value_type": "double",
           "rows": $,
           "cols": (localRead(jsonLines(ncolsTmp)))[0],
           "nnz": (localRead(jsonLines(nnzTmp)))[0],
           "format": "text",
           "description": { "author": "SystemML" }
        }
    -> write (jsonLines (matrix_mtd_file))
);

addColOffsets = fn(input_file, label_index)(
  //Temp files used for computing number of rows and number of non-zero entries
  ncolsTmp = strReplace(input_file, regex('.json'), '.ncols.json'),
  
  read(jsonText(strReplace(input_file, regex('.json'), '.meta.json')))
  -> sort by [$."colId"]
  -> prevElement()
  -> runningSum(fn(i) if(i."cur"."colId" == 1) 0
                      else( if(i."prev"."kind" == "scale" or i."prev"."colId" == label_index) 1
                            else i."prev"."distCount" ),
                fn(sum, i) {i."cur".*, "colOffset": sum})
  -> tee(
         -> transform if($."colId" == label_index or $."kind" == "scale") 
                        $."colOffset"+1 
                      else $."colOffset"+$."distCount"
         -> group into max($)
         -> write(jsonLines(ncolsTmp))
     )
  -> transform {$.*, "isLabel": if($."colId" == label_index) 1 else 0}
);

dummy_code = fn(input_file, label_index, matrix_file)(
  // Temp files used for computing number of rows and number of non-zero entries
  nnzTmp = strReplace(input_file, regex('.json'), '.nnztmp.json'),
  
  recodedData =
  read(jsonLines(input_file)),

  metadataWithColOffsets =
  addColOffsets(input_file, label_index),

  unpivotedRecodedData =
  recodedData
  -> transform each rec (fields(rec) 
                         -> transform {"rowId": rec."rowId", 
                                       "attr": $[0],
                                       "value": $[1]} -> filter $."attr" != "rowId")
  -> expand,

  //change to hash lookup?
  writeMtxFile =
  join unpivotedRecodedData, metadataWithColOffsets
  where metadataWithColOffsets."name" == unpivotedRecodedData."attr"
  into {"rowId": unpivotedRecodedData."rowId",
        "kind": metadataWithColOffsets."kind",
        "colOffset": metadataWithColOffsets."colOffset",
        "isLabel": metadataWithColOffsets."isLabel",
        "value": unpivotedRecodedData."value"}
  -> transform {"rowId": $."rowId",
                "colId": if($."kind" == "nominal" and $."isLabel" == 0) $."colOffset"+$."value"
                         else $."colOffset"+1,
                "value": if($."kind" == "nominal" and $."isLabel" == 0) 1
                         else $."value"}
  -> filter not isnull($."value") or $."value" != 0
  -> transform [$."rowId", $."colId", $."value"]
  -> tee( 
         -> group into count($)
         -> write(jsonLines(nnzTmp))
     )
  -> write(del(matrix_file, {delimiter: " ", quoted: false})),

  new_label_index =
  (metadataWithColOffsets
  -> filter $."isLabel" == 1
  -> transform [$."colOffset"+1]
  -> expand)[0],

  [serialize(new_label_index) + substring(serialize(writeMtxFile),0,0)]
);


unpivot = fn(input_file, matrix_file) (
    // DFS Path + filename for the JSON text recoded data
    recodedData = input_file,
    
    // Optional temp file, only used if we need to add a row ID to the input.
    rowIdTmp = strReplace(input_file, regex('.json '), '.tmp.json'),
    
    // Temp files used for computing number of rows and number of non-zero entries
    nnzTmp = strReplace(input_file, regex('.json'), '.nnztmp.json'),
    
    // DFS Path + filename for the meta data of the input data
    metaData = strReplace(input_file, regex('.json'), '.meta.json'),
    
    // DFS Path + filename for the SystemML delimited IJV matrix file
    //matrixData = strReplace(input_file, regex('.json'), ''),
    
    // DFS Path + filename for the SystemML matrix metadata file
    //matrixMetadata = strReplace(input_file, regex('.json'), '.mtd'),
    matrixMetadata = strcat(matrix_file, '.mtd'),

    setOptions( { conf: { "mapred.reduce.tasks.speculative.execution" : false } }),
    setOptions( { conf: { "mapred.child.java.opts" : "-Xmx2000M" } }),
    setOptions( { conf: { "hadoop.job.history.user.location" : "none"} }),

    // --------------------------------------------------------------------------------
    // Generate the matrix
    //    <rowId> <colId> <recodeId>
    // If the input has any null recodeId fields, then those (rowId, colId) pairs
    // will be missing from the output, thereby supporting sparse matrices.
    //--------------------------------------------------------------------------------
    
    if (ls(metaData) == []) (
     dmlException('Meta data file: ' + metaData + ' does not exist')
    ),
    
    metadataStream = localRead(jsonText(metaData)),
    
    // Compute number of columns from metadata
    numCols = metadataStream -> group into count($) -> singleton(),
    
    fieldsVar = record(localRead(jsonLines(recodedData)) -> top(1)) -> names(),

    // If necessary, add an artifical row ID.
    // Need to temp the result in order for the rest of this script to run in
    // parallel.
    if (not ("rowId" in fieldsVar)) (
      parallelEnumerate(jsonLines(recodedData))
        -> transform {rowId: $[0] + 1, $[1].* }
        -> write(jsonLines(rowIdTmp))
    ),

    infile =
        if (not ("rowId" in fieldsVar)) (
            rowIdTmp
        ) else (
            recodedData
        ),

    if (ls(infile) == []) (
     dmlException('input data file: ' + infile + ' does not exist')
    ),

    //input = read(jsonLines(infile)),

    // BEGIN MAIN PIPELINE
    
    // Pass #1: Convert data to <i j v> format.
    // convert to <i j v>  and filter out null value and zero
    
    // First make a lookup table for decoding column names to column IDs
    colNameTable =
        metadataStream
        -> transform { $.name: $.colId }
        -> record(),

    read(jsonLines(infile))
    // Change to { rowId, [ [C1, val1], [C2, val2] ... ] }
    -> transform each rec
        {
            rowId: rec.rowId,
            cols:
            (
                fields( removeFields(rec, ["rowId"]) )
                // Filter out null values in any of 3 formats
                -> filter ( not isnull ($[1]) and ($[1] != 0 or $[1] != "0"))
            ) // END definition of "cols" field
        }
    -> filter not $.cols == []
    -> expand unroll $.cols
    // Format for output.
    -> transform [$.rowId, colNameTable.($.cols[0]), $.cols[1]] 
    -> tee( 
         -> group into count($)
         -> write(jsonLines(nnzTmp))
       )
    -> write(del(matrix_file, {delimiter: " ", quoted: false}))

    // Clean up temporary files
    //hdfsShell(strcat("-rmr ",  nnzTmp)),
    
    
    // Clean up temp file, if we created one
    //if (not ("rowId" in fieldsVar)) (
    //    hdfsShell(strcat("-rmr ",  rowIdTmp))
    //)

);

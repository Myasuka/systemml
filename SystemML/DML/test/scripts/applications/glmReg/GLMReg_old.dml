#-------------------------------------------------------------
# IBM Confidential
# OCO Source Materials
# (C) Copyright IBM Corp. 2010, 2013
# The source code for this program is not published or
# otherwise divested of its trade secrets, irrespective of
# what has been deposited with the U.S. Copyright Office.
#-------------------------------------------------------------

# Intended to solve GLM Regression using Iteratively Reweighted Least Squares without trust regions
# Assume LR_HOME is set to the home of the dml script
# Assume input and output directories are on hdfs as INPUT_DIR and OUTPUT_DIR
# INPUT  1: Matrix X_train [rows_train, columns]
# INPUT  2: number of rows_train
# INPUT  3: number of columns
# INPUT  4: Matrix X_test [rows_test, columns]
# INPUT  5: number of rows_test
# INPUT  6: Matrix y_train [rows_train, 1]
# INPUT  7: Matrix y_test [rows_test, 1]
# INPUT  8: distribution_family: 1 = poisson, 2 = logistic
# INPUT  9: double scale_parameter
# INPUT 10: lambda_regularizer
# INPUT 11: double epsilon
# INPUT 12: the output file
# OUTPUT:  Matrix beta [cols, 1]
# Assume rows = 50 and cols = 30, distribution_family = 2, scale_parameter = 1.0, epsilon = 0.00000001
# hadoop jar SystemML.jar -f $GLMR_HOME/GLMReg.dml -args "$INPUT_DIR/X" 50 30 "$INPUT_DIR/Xt" 50 "$INPUT_DIR/y" "$INPUT_DIR/yt"
#                         2 1.0 0.0 0.00000001 "$OUTPUT_DIR/beta"

print("BEGIN GLM SCRIPT");
print("Reading X for the training records...");
X_train = read($1, rows=$2, cols=$3, format="text");
print("Reading X for the test records...");
X_test = read($4, rows=$5, cols=$3, format="text");
# read (training and test) labels
print("Reading Y for the training and test records...");
y_train = read($6, rows=$2, cols=1, format="text");
y_test = read($7, rows=$5, cols=1, format="text");

glm_dist = function(double dist_family, matrix[double] linear_terms, double scale_param) 
           return(matrix[double] mean, matrix[double] link_gradient, matrix[double] variance)
{
    if(0.99 < dist_family & dist_family < 1.01) {
        mean = exp(linear_terms);
        link_gradient = 1.0 / mean;
        variance = scale_param * mean;
    }
    else {
        if(1.99 < dist_family & dist_family < 2.01) {
            mean = 2.0 / (1.0 + exp(- linear_terms)) - 1.0;
            link_gradient = 2.0 / (1.0 - mean * mean);
            variance = scale_param * (1.0 - mean * mean);
        }
        else {
            mean = linear_terms;
            link_gradient = 1.0 + 0.0 * mean;
            variance = scale_param * link_gradient;
        }
    }
}

num_records = $2;
num_features = $3;
num_test_records = $5;
distribution_family = $8;
scale_parameter = $9;
lambda_regularizer = $10;
eps = $11;
eps_simpleLS = 0.0;

X = X_train;
y = y_train;
newbeta = Rand(rows = num_features, cols = 1, min = 0, max = 0);
beta = newbeta;
max_iteration_IRLS = 100;
converged = 0;

print("BEGIN IRLS ITERATIONS...");
i_IRLS = 0;
while(i_IRLS < max_iteration_IRLS & converged == 0) {
    beta = newbeta;
    all_linear_terms = X %*% beta;
    [y_mean, link_grad, y_var] = glm_dist(distribution_family, all_linear_terms, scale_parameter);
    z = (y - y_mean) * link_grad + all_linear_terms;
    weight_diagonal = Rand(rows = num_records, cols = 1, min = 1.0, max = 1.0);
    weight_diagonal = (weight_diagonal / y_var);
    weight_diagonal = (weight_diagonal / link_grad);
    weight_diagonal = (weight_diagonal / link_grad);

# Apply the Simple Least Squares to V and z
# BEGIN the Linear Regression script

    newbeta = beta;
    A = t(X) %*% diag(weight_diagonal) %*% X;

    r = (A %*% newbeta) - (t(X) %*% (weight_diagonal * z));
    p = -r;
    norm_r2 = sum(r * r);

    max_iteration_simpleLS = num_features;

    i_simpleLS = 0;
    while(i_simpleLS < max_iteration_simpleLS & max(abs(r)) >= eps_simpleLS)
    {
        q = (A %*% p) + lambda_regularizer * p;
        alpha = norm_r2 / castAsScalar(t(p) %*% q);
        newbeta = newbeta + alpha * p;
        old_norm_r2 = norm_r2;
        r = r + alpha * q;
        norm_r2 = sum(r * r);
        p = -r + (norm_r2 / old_norm_r2) * p;
        i_simpleLS = i_simpleLS + 1;
    }

# END the Linear Regression script from the library

    i_IRLS = i_IRLS + 1;
    beta_difference = max(abs(newbeta - beta));
    if(beta_difference < eps) {
        converged = 1;
    }
    
    print("Iteration #" + i_IRLS + " completed, beta_difference = " + beta_difference);
}

beta = newbeta;

if(converged == 1) {
    print("Converged in " + i_IRLS + " steps.");
}
else {
    print("Did not converge.");
}

print("beta[1] = " + castAsScalar(beta[1, 1]));
print("beta[2] = " + castAsScalar(beta[2, 1]));
print("beta[3] = " + castAsScalar(beta[3, 1]));


write(beta, $12, format="text");

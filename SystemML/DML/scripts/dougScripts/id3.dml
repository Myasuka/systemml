#assumes labels are numbered from 1 to #class_labels
#assumes each feature column is numbered from 1 to max_domain_val of feature
#second assumption should go away if we can determine the domain of a feature col
#second assumption is unrealistic because features can be binary 0/1 valued
#should allow 0 in feature columns

compute_entropy = function(Matrix[Double] samples_bit_vector, Matrix[Double] y) return (Double entropy){
	hist_labels = groupedAggregate(target=samples_bit_vector, groups=y, fn="sum")
	num_samples = sum(samples_bit_vector)
	entropy = 0
	for(i in 1:nrow(hist_labels)){
		pi = castAsScalar(hist_labels[i,1])/num_samples
		if(pi != 0){
			entropy = entropy - pi*log(pi)
		}
	}
}

id3_learn = function(Matrix[Double] X, Matrix[Double] y, Matrix[Double] X_subset, Matrix[Double] attributes) return (Matrix[Double] nodes, Matrix[Double] edges){
	one = Rand(rows=1, cols=1, min=1, max=1)
	
	#try the two base cases first

	#of the remaining samples, compute a histogram for labels
	hist_labels = groupedAggregate(target=X_subset, groups=y, fn="sum")
	
	#go through the histogram to compute the number of labels
	#with non-zero samples
	#and to pull out the most popular label
	num_non_zero_labels = 0
	most_popular_label = -1
	num_samples_w_most_popular_label = -1
	for(i in 1:nrow(hist_labels)){
		if(castAsScalar(hist_labels[i,1]) != 0){
			num_non_zero_labels = num_non_zero_labels + 1
			if(castAsScalar(hist_labels[i,1]) >= num_samples_w_most_popular_label){
				most_popular_label = i
				num_samples_w_most_popular_label = castAsScalar(hist_labels[i,1])
			}
		}
	}
	
	num_remaining_attrs = sum(attributes)
	
	#if all samples have the same label then return a leaf node
	#if no attributes remain then return a leaf node with the most popular label	
	if(num_samples_w_most_popular_label == -1 | num_non_zero_labels == 1 | num_remaining_attrs == 0){
		nodes = Rand(rows=1, cols=2, min=0, max=0)
		nodes[1,1] = -1*one
		nodes[1,2] = most_popular_label*one
		edges = Rand(rows=1, cols=1, min=-1, max=-1)
	}else{
		#computing gains for all available attributes using parfor
		ht = compute_entropy(X_subset, y)
		
		sz = 0
		if(1==1){
			sz = nrow(attributes)
		}
		gains = Rand(rows=sz, cols=1, min=0, max=0)
		parfor(i in 1:nrow(attributes)){
			if(castAsScalar(attributes[i,1]) == 1){
				attr_vals = X[,i]
				attr_domain = groupedAggregate(target=X_subset, groups=attr_vals, fn="sum")
				hxt = 0
				for(j in 1:nrow(attr_domain)){
					if(castAsScalar(attr_domain[j,1]) != 0){
						val = j
						Tj = X_subset * ppred(X[,i], val, "==")
						entropy = compute_entropy(Tj, y)
						hxt = hxt + sum(Tj)/sum(X_subset)*entropy
					}
				}
				gains[i,1] = (ht - hxt)*one
			}
		}
		
		#pick out attr with highest gain
		best_attr = -1
		max_gain = 0
		for(i in 1:nrow(gains)){
			if(best_attr == -1 | max_gain <= castAsScalar(gains[i,1])){
				max_gain = castAsScalar(gains[i,1])
				best_attr = i
			}
		}
		
		#creating root node
		nodes = Rand(rows=1, cols=2, min=0, max=0)
		nodes[1,1] = best_attr*one
		numNodes = nrow(nodes)
		
		print("best attribute is: " + best_attr)
		
		attr_vals = X[,best_attr]
		attr_domain = groupedAggregate(target=X_subset, groups=attr_vals, fn="sum")
		
		#edges from root to children
		if(1==1){
			sz = nrow(attr_domain)
		}
		edges = Rand(rows=sz, cols=3, min=1, max=1)
		for(i in 1:nrow(attr_domain)){
			val = i
			edges[i,2] = val*one
		}
		numEdges = nrow(edges)
		
		new_attributes = attributes
		new_attributes[best_attr, 1] = 0*one
		for(i in 1:nrow(attr_domain)){
			print("#nodes = " + nrow(nodes))
		
			val = i
			Ti = X_subset * ppred(X[,best_attr], val, "==")
			[nodesi, edgesi] = id3_learn(X, y, Ti, new_attributes)
			
			#adding subtree
			if(1==1){
				sz = numNodes + nrow(nodesi)
			}
			new_nodes = Rand(rows=sz, cols=ncol(nodes), min=0, max=0)
			new_nodes[1:numNodes,] = nodes
			new_nodes[numNodes+1:,] = nodesi
			
			if(nrow(edgesi)!=1 | ncol(edgesi)!=1 | castAsScalar(edgesi[1,1])!=-1){
				edgesi[,1] = edgesi[,1] + numNodes
				edgesi[,3] = edgesi[,3] + numNodes
				if(1==1){
					sz = numEdges+nrow(edgesi)
				}
				new_edges = Rand(rows=sz, cols=3, min=0, max=0)
				new_edges[1:numEdges,] = edges
				new_edges[numEdges+1:,] = edgesi
				edges = new_edges
				numEdges = nrow(new_edges)
			}
			
			edges[i,3] = numNodes + 1*one
			nodes = new_nodes
			numNodes = nrow(new_nodes)
		}
	}
}

X = read($1)
y = read($2)

n = nrow(X)
m = ncol(X)

sz = 0
if(1==1){
	sz = n
}
X_subset = Rand(rows=sz, cols=1, min=1, max=1)
if(1==1){
	sz = m
}
attributes = Rand(rows=sz, cols=1, min=1, max=1)

sz1 = 0
if(1==1){
	sz1 = n
}
# recoding inputs
featureCorrections = 1 - colMins(X)
onesMat = Rand(rows=sz1, cols=sz, min=1, max=1)
X = onesMat %*% diag(featureCorrections) + X
labelCorrection = 1 - min(y)
y = y + labelCorrection;

[nodes, edges] = id3_learn(X, y, X_subset, attributes)

# decoding outputs
nodes[,2] = nodes[,2] - labelCorrection * ppred(nodes[,1], -1, "==")
nodes = Rand(rows = 10, cols = 10);
parfor(i in 1:nrow(edges)){
	e_parent = edges[i,1:2]
	parent_feature = castAsScalar(nodes[1,1]);
	correction = featureCorrections[1,parent_feature]
	edges[i,2] = e_parent[1,2] - correction
}

write(nodes, $3, format="text")
write(edges, $4, format="text")



sigmoid = function(Matrix[double] v) return (Matrix[double] g) {
	g = 1.0 / (1.0 + exp(-v));
} 

trainLogisticRegression = function(Matrix[double] X, Matrix[double] y, double alpha) return (Matrix[double] theta) {
	
	# number of training samples
	m = nrow(X);
	
	# intialize model
	theta = matrix(0, rows = ncol(X), cols = 1);
	
	print(sum(y));
	
	
	
	# train model with batch gradient descent
	for (iteration in 1:1) {
	
		/*cost = 0.0;
		for (i in 1:m) {			
			#prediction = (1.0 / (1.0 + exp(-(castAsScalar(X[m,] %*% theta)))));
			#print(prediction);	
			
			y_i = castAsScalar(y[m,1]);
			cost = cost + y_i;
			#print(m + " --> "+ y_i + " " + prediction); 
			
			#pc = -y_i * log(prediction) - (1 - y_i) * log(1 - prediction);
			#print(pc);		
		}
		print("current cost: " + cost);*/
	
		# have to inline sigmoid manually unfortunately
		#theta = theta - alpha * (1.0 / m) * (t(X) %*% (1.0 / (1.0 + exp(-(X %*% theta)))) - t(X) %*% y);
		
		gradient = (1.0 / m) * (t(X) %*% ((1.0 / (1.0 + exp(-(X %*% theta)))) -  y));
		
		#print("gradient = [" + castAsScalar(gradient[1,1]) + ", " + castAsScalar(gradient[2,1]) + ", " + castAsScalar(gradient[3,1]) + "]");
		
		theta = theta - alpha * gradient;
		
	}
}


data = read("./scripts/ssc/labeled-datapoints.txt", rows=100, cols=3, format="text");

# first columns contain the features
intercepts = matrix(1, rows = nrow(data), cols = 1);
X = append(data[,1:ncol(data) - 1], intercepts);


# last column is the labels
y = data[,ncol(data)];


# normalization 
means = colMeans(X);

#print(castAsScalar(means[1,1]) + " " + castAsScalar(means[1,2]));

X[,1] = X[,1] - castAsScalar(means[1,1]);
X[,2] = X[,2] - castAsScalar(means[1,2]); 

stdDev1 = sqrt(sum((X[,1] * X[,1])));
stdDev2 = sqrt(sum((X[,2] * X[,2])));

X[,1] = X[,1] / stdDev1;
X[,2] = X[,2] / stdDev2; 


print(castAsScalar(X[56,1]) + " " + castAsScalar(X[56,2]));

theta = trainLogisticRegression(X, y, -0.00001);

print("theta = [" + castAsScalar(theta[1,1]) + ", " + castAsScalar(theta[2,1]) + ", " + castAsScalar(theta[3,1]) + "]");

predictions = sigmoid(X %*% theta);
predictedClasses = ppred(predictions, 0.5, ">=");

#for (i in 1:100) {
#	print(castAsScalar(predictions[i,1]) + " " + castAsScalar(predictedClasses[i,1]));
#}

numPositive = sum(predictedClasses);
numNegative = nrow(X) - numPositive;

numMisclassifications = sum(abs(predictedClasses - y));
accuracy = 1 - (numMisclassifications / nrow(X));

print("Accuracy: " + accuracy + ",  Misses: " + numMisclassifications + ", positive: " + numPositive + ", negative: " + numNegative);


We use GNMF shown in Script~\ref{scpt:gnmf} as a running example to demonstrate
scalability on both the 40-core cluster and the 100-core cluster.

The input matrix V is a sparse matrix with $d$ rows and $w$ columns. We fix $w$ to be 100,000 and vary $d$. We
set the sparsity of V to be 0.001, thus each row has 100 non-zero entries on average. The goal of GNMF
algorithm is to compute dense matrices W of size $d\times t$ and H of size $t\times w$, where
$V\approx W H$. $t$ is set to 10 (As described in Section~\ref{sec:intro} in the context of topic modeling, $t$ is the number of topics.). Table~\ref{tab:matrix-stats} lists the characteristics of V, W and H used in our setup.

\some
{plots/gnmf_cmp_all.eps}
{fig:gnmf-cmp-all}
{plots/gnmf_ec2.eps}
{fig:gnmf-ec2}
{plots/ec2_scale.eps}
{fig:ec2-scale}
{Scalability of GNMF: (a) increasing data size on 40-core cluster, (b) increasing data size on 100-core cluster, (c) increasing data size and cluster size}

\noindent {\bf Baseline single machine comparison:} As a baseline for
comparing \systemmltext, we first run GNMF using 64-bit version of R on a single machine with 64 GB
memory. Figure~\ref{fig:gnmf-cmp-all} shows the execution times for one iteration of the algorithm
with increasing sizes of V. For relatively small sizes of V, R runs very efficiently as the data fits in memory. 
However, when the number of rows in V increases to 10 million (1 billion
non-zeros in V), R runs out of memory, while \systemmltext\ continues to scale.

\noindent {\bf Comparison against best known published result:} \cite{msrwww10} introduces a hand-coded MapReduce implementation of GNMF. We use this MapReduce
implementation as a baseline to evaluate the efficiency of the execution plan generated
by \systemmlit\ as well as study the performance overhead of our generic runtime\footnote {Through
personal contact with the authors of~\cite{msrwww10}, we were informed that all the scalability
experiments for the hand-coded GNMF algorithm were conducted on a proprietary SCOPE cluster with
thousands of nodes, and the actual number of nodes scheduled for each execution was not known.}. For a
fair comparison, we re-implemented the algorithm as described in the paper and ran it on the same
40-core cluster as the \systemmltext\ generated plan. The hand-coded algorithm contains 8 full
MapReduce jobs and 2 map-only jobs, while the execution plan generated by \systemmltext\ consists of
10 full MapReduce jobs. For the hand-coded algorithm, the matrices are all prepared in the required
formats: V is in cell representation, W is in a row-wise representation and H is in a column-wise
representation. For the \systemmltext\ plan, the input matrices are all in block representation with block
size $1000\times 1000$. Figure~\ref{fig:gnmf-cmp-all} shows the performance comparison
of \systemmltext\ with the hand-coded implementation. Surprisingly, the performance
of \systemmltext\ is significantly better than the hand-coded implementation. As the number of
non-zeros increases from 10 million to 750 million, execution time on \systemmlit\ increases steadily from 519 seconds
to around 800 seconds, while execution time for the hand-coded plan increases dramatically from 477 seconds to 4048
seconds! There are two main reasons for this difference. First, \systemmltext\ uses the block
representation for V, W, and H, while in the hand-coded implementation, the largest matrix V is in
cell representation. As discussed in Section~\ref{sec:blocking} and to be demonstrated in
Section~\ref{sec:optimizations}, the block representation provides significant performance advantages over
the cell representation. Second, the hand-coded implementation employs an approach very similar to
CPMM for the two most expensive matrix multiplications in GNMF: \texttt{t(W)\mmult V}
and \texttt{V\mmult t(H)}, but without the local aggregator (see Section~\ref{sec:localagg}). As will
be shown in Section~\ref{sec:optimizations}, CPMM with local aggregation significantly outperforms
CPMM without local aggregation.

\noindent {\bf Scalability on 100-core EC2 cluster:} To test \systemmltext\ on a large cluster, we ran GNMF on a 100-core EC2 cluster. In the first
experiment, we fixed the number of nodes in the cluster to be 100, and ran GNMF by varying the
number of non-zero values from 100 million to 5 billion. Figure~\ref{fig:gnmf-ec2} demonstrates the
scalability of \systemmltext\ for one iteration of GNMF. In the second experiment (shown in Figure~\ref{fig:ec2-scale}), 
we varied the number of worker nodes from 40 to 100 and scaled the problem size proportionally from 800 million non-zero values to 2 billion non-zeros. The
ideal scale-out behavior would depict a flat line in the chart. However, it is impossible to realize this ideal
scale-out behavior due to many factors such as network overheads. Nevertheless, Figure~\ref{fig:ec2-scale} presents a steady increase in execution time 
with the growth in data and cluster size.

Besides scalability, \dmlr\ improves productivity and reduces development time of ML algorithms significantly. For example, GNMF is implemented in 11 lines of DML script, but requires more than 1500 lines of Java code in the hand-coded implementation. Similar observations have been
made in~\cite{boom10} regarding the power of declarative languages in substantially simplifying
distributed systems programming.


%The characteristics of the synthetic matrices are listed in Table~\ref{tab:matrix-stats}. In the
%context of the GNMF algorithm, the input matrix V is a sparse document-word matrix with m rows (m
%documents) and n columns (n words). We fix the dictionary size n to be 100,000 and vary the number
%of documents m. On average, the each document has 100 words, thus the sparsity of the matrix is
%0.1\%. The number of topic we want to find out for V is k, thus W and H are both dense matrices of
%size $m\times k$ and $k \times n$, respectively. We set $k=10$ in our experiment. In the context of
%the linear regression algorithm, V again is a document-word matrix with sparsity 0.1\% and y is a
%dense vector of 1s and -1s indicating whether a document belongs to a certain topic or not. Except
%for the experiments in Section~\ref{sec:block-exp}, all the other experiments use block size
%$1000\times 1000$. Note the square blocking works even in the case when one dimension is less than
%1000 (refer to Section~\ref{sec:mrruntime}).

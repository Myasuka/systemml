\subsection{Choosing Block Size}

In Section~\ref{sec:blocking}, we introduced the blocking technique to represent matrices as key-value pairs. A natural issue arises with respect to how to choose the right block size for matrices in SystemML. The first question deals with what shape of block we want to support. A block can be square or rectangular, and can even be a row or a column of a matrix in the extreme case. However, rectangular blocks, as well as rows and columns, will cause frequent reblocking during the execution of multiple operations. Take $A^t+B*A$ as an example, if $A$ and $B$ are blocked into rectangular blocks $100\times 50$, then the result of $A^t$ will be blocked $50 \times 100$, which is incompatible with the block size $100\times 50$ of $B*A$. A reblock operation has to apply either to the result of $A^t$ or $B*A$ to continue the execution. To avoid frequent reblocking caused by irregular block sizes, SystemML adopts square blocks for the matrix representation.  

%In the context of $A+B$, if the block sizes of both $A$ and $B$ are $100\times 50$, then they are compatible. However, in the context of $A\times B$, such block sizes are incompatible, because the block level multiplication cannot be performed. On the other hand, $100\times 50$ and $50 \times 50$ are compatible block sizes for $A\times B$. 

After deciding on square block, another question is how large each block should be. This is a very complex issue. On one hand, larger block sizes will improve the performance of matrix operations. On the other hand, a block cannot be too large to fit in memory. Large blocks can also reduce the degree of parallelization. In the current SystemML, the block sizes are decided heuristically. Through empirical study, we found that $1000\times 1000$ is a good heuristic for most cases.

{\it Optimization}: Optimizations can be performed by re-arranging hops in a Hop DAG. For example,
Hop properties include data characteristics such as matrix dimensions (if known at compile
time). These data characteristics can be used in a dynamic programming model to decide on the order
of matrix multiplications as motivated in Figure~\ref{fig:order}. Re-ordering of matrix
multiplications triggers algebraic rewrites by re-arranging hops in the Hop DAG. E.g.,
Figure~\ref{fig:hopdag} shows the computation of $W(HH^T)$ (as opposed to $(WH)H^T$) in a step to
update matrix $W$ to avoid the computation of a large $(WH)$ matrix. Another optimization is common
subexpression elimination that can be detected in Hop DAGs. In Figure~\ref{fig:hopdag}, HOP ``Read
W'' feeds into 2 HOPs ``r''' to transpose the $W$ matrix, which can be combined into 1 HOP. {\tt
  talk about computing optimal block sizes.}

{\it Crossvalidation}: {\tt do we want to mention it?}

{\it Integration with external libraries}: {\tt do we want to talk about it, e.g. Eigen() in JLaPack?}



There are three main considerations in the runtime component of \systemmltext: 
\textit{key-value representation} of matrices, an \textit{MR runtime} to execute 
individual LOP-Dags over MapReduce, and a \textit{control module} to orchestrate the 
execution of all the MapReduce jobs for a \dmlr\ script. 

\subsubsection{Matrices as Key-Value Pairs}
\label{sec:blocking}

\systemmltext\ partitions matrices into blocks (using a blocking operation) and exploits local sparsity within a block to reduce 
the number of key-value pairs when representing matrices. 

\noindent {\bf Blocking:} A matrix is partitioned into smaller rectangular sub-matrices called blocks. Each block is represented as a key-value pair with the key denoting the block id and the value carrying all the cell values in the block. Figure~\ref{fig:block-example} shows a matrix partitioned into $2 \times 2$ blocks. Note that cell, row and column representations are special cases of blocks. Varying the block sizes results in a trade-off between the number of key-value pairs flowing through MapReduce and the degree of parallelism in the system.

%The block representation allows some hops to be implemented more
%efficiently in a two-step process. E.g., for computing the column sum
%of a matrix, we first pre-aggregate at the block-level and then aggregate
%across all the blocks. Another well-known optimization technique for
%matrix multiplication can be implemented at the block level as
%follows: $C_{i,j} =\sum_k{ A_{i,k} \times B_{k,j}}$, where $A_{i,k}
%\times B_{k,j}$ performs matrix multiplication on individual blocks of
%A and B.


\onefigure
{figures/block_example.eps} 
{Example Block Representation}
{fig:block-example}


\noindent {\bf Local Sparsity:} Local Sparsity refers to the sparsity of an individual 
block, i.e. the fraction of non-zero values in the block. To achieve storage efficiency, 
the actual layout of the values in a block is decided based upon its local sparsity. A 
parameter $T_{sparse}$ provides a threshold to choose between a \textit{sparse} and a 
\textit{dense} representation on a per-block basis. For example with $T_{sparse} = 0.3$ in 
Figure~\ref{fig:block-example}, the block $A_{1,2}$ (local sparsity $0.25$) is treated as 
sparse, and hence, only its non-zero cells are stored. In comparison, the block $A_{1,1}$ 
with local sparsity $0.75$ is considered dense and all its cell values are stored in a 
one-dimensional array.

%For instance, a \binarylop\ is implemented as an iteration over the input arrays if both 
%of the inputs are in dense format, but in a different way if one or both the inputs are 
%in sparse format.

\noindent {\bf Dynamic Block-level Operations Based on Local Sparsity:} 
When employing blocking, all matrix operations are translated into operations on blocks at the lowest level. Local sparsity information is also used to dynamically decide on the appropriate execution of per-block operations at runtime. For every block-level operation, there are separate execution plans to account for the fact that individual blocks may be dense or sparse. Suppose we want to perform matrix multiplication on two individual blocks. The actual algorithm chosen for this operation is based on the local sparsity of the two input blocks. If both blocks are dense, the runtime chooses an algorithm that cycles through every cell in both blocks. If, however, one or both of the blocks is sparse, the runtime chooses an algorithm that operates {\em only} on the non-zero cells from the sparse block(s). 
%, multiplying them with the corresponding cells in the dense block.

%Given the local sparsity of $A_{i,k}$ and $B_{k,j}$, 
%we can first estimate the local sparsity of $C_{i,j}$ then decide on
%whether to use sparse or dense representation for $C_{i,j}$. 

%Besides the benefit in storage efficiency, blocking also has the
%advantage in computation efficiency. The formula of many the matrix
%operations can be rewritten using the block representation.
%For matrix multiplication, the formula can be rewritten as 
%By choosing the proper size for blocks, all these operations on block
%can be computed efficiently in memory. In
%Section~\ref{sec:runtime-opt}, we will discussion the optimization
%opportunities to choose efficient execution for individual block-level
%operations based on local sparsity.

%Note that SystemML doesn't require users to prepare matrices in the
%block representation. Instead, SystemML supports multiple formats,
%such as the sparse matrix format supported in R, then transforms them
%into the block representation internally for computation, and finally
%writes the output of a DML script in the formats desired by the users.

%\onemediumfigure
%{figures/block_example.eps} {Example Block Representation}
%{fig:block-example}

\input{runtimejobs}


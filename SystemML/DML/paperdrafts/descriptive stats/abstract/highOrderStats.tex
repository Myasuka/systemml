\textbf{Stable High Order Statistics.}
We now describe our stable algorithms to compute higher order statistics, such as variance, skewness and kurtosis. The core computation is to calculate the $p^{th}$ central moment $m_p=\frac{1}{n}\sum\limits_{i=1}^{n}(x_i-\bar{x})^p$. For instance, variance can be written as $s^2=\frac{n}{n-1}m_2$ and skewness can be expressed as $g_1=\frac{m_3}{m_2^{1.5}}$. The standard two-pass algorithm produces the numerically stable result but it requires two scans of data -- one scan to compute $\bar{x}$ and the second to compute $m_p$. A common technique (pitfall) is to apply a simple textbook rewrite to get a one-pass algorithm. For instance, $m_2$ can be rewritten as $\frac{1}{n}\sum\limits_{i=1}^{n}x_i^2-\frac{1}{n^2}(\sum\limits_{i=1}^{n}x_i)^2$. The sum and the sum of squares can be computed in a single pass. However, this algebraically equivalent rewrite is known to suffer from serious stability issues resulting from cancellation errors when performing substraction of two large and nearly equal numbers. This rewrite, in fact, may produce a negative result for $m_2$. Surprisingly, this notoriously unstable algorithm is still used in popular distributed database platforms~\cite{teradata} -- emphasizing the need for immediate attention to numerical stability issues in large scale data processing.


%A pitfall that people often run into is to apply a simple textbook rewrite rule to get a one-pass algorithm. For example, $2^{nd}$ central moment is rewritten as $m_2=\sum\limits_{i=1}^{n}x_i^2-\frac{1}{n}(\sum\limits_{i=1}^{n}x_i)^2$, where the sum as well as sum of squares are computed in a single pass. Even though the rewrite is algebraically equivalent, it is known to suffer from serious stability issues resulting from cancellation errors while performing the substraction of two large nearly equal numbers.


%Besides simple summation, descriptive statistics contain a number of high order statistics, such as central moment, variance, skewness and kurtosis. The $k^{th}$ central moment is defined as $m_k=\frac{1}{n}\sum\limits_{i=1}^{n}(x_i-\bar{x})^k$. Central moment is also the building block for the other high order statistics. For example, variance can be written as $s^2=\frac{n}{n-1}m_2$ and skewness can be expressed as $g_1=\frac{m_3}{m_2^{1.5}}$. Therefore, to produce efficient and numerically stable high order statistics, we need an efficient and numerically stable central moment algorithm. The standard two-pass algorithm using stable summation produces the numerically stable central moment, but requires two scans of data: the first to produce the mean and the second to calculate the central moment. A pitfall people often run into is to apply a simple textbook rewrite on the definition of central moment to get a one-pass algorithm. For example, $2^{nd}$ central moment can be rewritten as $m_2=\sum\limits_{i=1}^{n}x_i^2-\frac{1}{n}(\sum\limits_{i=1}^{n}x_i)^2$. Then this textbook one-pass algorithm can calculate the sum and sum of squares in one scan of the data followed by simply arithmetic on the results. As elaborated in~\cite{numStabBook}, this seeming mathematically equivalent and more efficient algorithm can lead to numerical disasters, due to the severe cancellation problem when performing a substraction on two large and nearly equal numbers. The computed $2^{nd}$ central moment (or variance) is often zero and sometimes even negative. But surprisingly, this notoriously unstable algorithm is still in use in some database products~\cite{netezza, teradata}.

In SystemML, we use a stable MapReduce algorithm based on an existing technique~\cite{cm} to compute arbitrary order central moment in an incremental fashion. It makes use of an update rule (shown below) that combines partial results obtained from two disjoint subsets of data (denoted as subscripts $a$ and $b$). Here, $n$, $\mu$, $M_p$ refer to the cardinality, mean, and $n\times m_p$ of a subset, respectively. Note that the same update rule is used in each mapper that maintains running values for these variables seen thus far, as well as in the (single) reducer that combines partial results computed by mappers. 

%\begin{equation}
%M_p=M_{p,a}+M_{p,b}+\sum\limits_{j=1}^{p-2}{p \choose j}[(-\frac{n_2}{n})^j M_{p-j,a}+(\frac{n_1}{n})^j M_{p-j,b}]\delta + (\frac{n_1n_2}{n}\delta)^p[\frac{1}{n_2^{p-1}}-(\frac{-1}{n_1})^{p-1}]$
%\STATE \ \ \textbf{return} $(n,\mu,M_2,..., M_k)
%\label{eq:cmeq}
%\end{equation}
\Crunch
\begin{small}
\begin{displaymath}
\begin{split}
n=& n_a+n_b, \;\; \delta=\mu_b-\mu_a, \;\; \mu=\mu_a+n_b\frac{\delta}{n} \\
M_p =& M_{p,a}+M_{p,b}+\sum\limits_{j=1}^{p-2}{p \choose j} [(-\frac{n_b}{n})^j M_{p-j,a} \\
  & + (\frac{n_a}{n})^j M_{p-j,b}]\delta^j + (\frac{n_an_b}{n}\delta)^p[\frac{1}{n_b^{p-1}}-(\frac{-1}{n_a})^{p-1}]
\end{split}
\label{eq:cmeq}
\end{displaymath}
\end{small}
\Crunch

%We adopt the algorithm introduced in~\cite{cm} to numerically stably calculate arbitrary order central moment. As shown in Algorithm~\ref{algo:cm}, the core of the algorithm is to calculate the value of $M_k=\sum\limits_{i=1}^{n}(x_i-\bar{x})^k$. Similar to the Kahan Sum algorithm, this algorithm can also be easily adapted to the MapReduce environment, by letting mappers compute the partial values and a single reducer produce the final result.

%\input{cm_alg}

%$$n=n_1+n_2$$
%$$\delta=\mu_2-\mu_1$$
%$$\mu=\mu_1+n_2\frac{\delta}{n}$$
%$$M^p_g=M^p_{g_1}+M^p_{g_2}+\sum\limits_{k=1}^{p-2}{p \choose k}[(-\frac{n_2}{n})^k M^{p-k}_{g_1}+(\frac{n_1}{n})^k M^{p-k}_{g_2}]\delta + (\frac{n_1n_2}{n}\delta)^p[\frac{1}{n_2^{p-1}}-(\frac{-1}{n_1})^{p-1}]$$


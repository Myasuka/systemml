Most of descriptive statistics, except for order statistics, can be expressed in certain summation form, thus may appear simple to be implemented in MapReduce. However, straightforward implementations can sometimes lead to disasters in numerical accuracy, due to round-off and truncation errors associated with finite precision arithmetic. As the need for analyzing large volumes of data increases, \textit{numerical stability} becomes even more critical.
In the MapReduce environment, one needs effective algorithms that can both exploit massive data parallelism and produce robust results with growing data sizes. To the best of our knowledge, none of the existing large scale platforms, such as PIG~\cite{pig}, HIVE~\cite{hive} and JAQL~\cite{jaql}, pay any attention to numerical stability\footnote{Some of these systems support BigDecimal type, but it incurs heavy performance overhead, thus is not recommended for large scale data processing.}.
%Although some of these systems support BigDecimal types, using BigDecimal for numerical computation brings significant performance overhead, thus is not recommended for large scale data processing.}. 
Numerically unstable algorithms are still in use, even in well-known distributed database products~\cite{teradata}. In this section, we use summation and central moment as the driving examples to highlight the common pitfalls in implementing descriptive statistics, and describe how numerical stability is achieved in SystemML.

\begin{table}[t]
\centering
\begin{tabular}{|c|l|}
\hline
&Sum, Mean, Harmonic mean, Geometric mean, Mode,\\
 &  Min, Max, Range, Median, Quantiles, Inter-quartile mean, \\ 
Univariate & Variance, Standard deviation, Coefficient of variation, \\
& Central moment, Skewness, Kurtosis, Standard error of mean,\\
& Standard error of skewness, Standard error of kurtosis\\
\hline
Bivariate & Pearson's R, Chi-squared coefficient, Cramer's V\\
&  Eta, ANOVA F-measure, Spearman correlation\\
\hline
\end{tabular}
\Crunch
\caption{Descriptive statistics supported in SystemML}
\BigCrunch
\label{tab:stats}
\BigCrunch
\end{table}

%
%Numerical instability refers to inaccuracies in computation resulting from finite precision floating point arithmetic on digital computers with round-off and truncation errors. Quite often, multiple algebraically equivalent formulations of the same numerical calculation produce very different results -- some may propagate and magnify these approximation errors whereas others may be more robust or numerically stable. As the need for analyzing large volumes of data increases, the issue of numerical stability becomes much more critical.
%In the context of large distributed MapReduce-style environments, one needs effective algorithms that can both exploit massive data parallelism and produce robust results with the increasing number of input data items. 

%To the best of our knowledge, none of the existing large scale data processing platforms such as PIG~\cite{pig}, HIVE~\cite{hive} and JAQL~\cite{jaql} pay attention to these issues\footnote{Although some of these systems support BigDecimal types, using BigDecimal for numerical computation brings significant performance overhead, thus is not recommended for large scale data processing.}. In this section, we use summation and arbitrary order central moment as the driving examples, to highlight the common pitfalls of implementing descriptive statistics and describe how numerical stability is achieved in SystemML.

%Numerical instability is a problem caused by the finite precision arithmetic on numerical algorithms. If there were no round-off or truncation errors associated with finite precision arithmetic, numerical algorithms would always approach the right solution. Quite often, different algebraically equivalent ways of implementing a numerical calculation will result in very different result, with some propagating and magnifying approximation errors and others more robust -- also called numerically stable. The goal of most numerical analysis is to select these numerically stable algorithms. 

\textbf{Stable Summation.} Summation is a fundamental operation in many statistical functions, such as mean, variance, and norms. The simplest method is to perform {\em recurisve summation}, which initializes $sum=0$ and incrementally updates $sum$. It however suffers from numerical inaccuracies even on a single computer. For instance, with $2$-digit precision, naive summation of numbers $1.0, 0.04, 0.04, 0.04, 0.04, 0.04$ results in $1.0$, whereas the exact answer is $1.2$. This is because once the first element $1.0$ is added to $sum$, adding $0.04$ will have no effect on $sum$ due to round-off error. 
A simple alternative strategy is to first sort the data in increasing order, and subsequently perform the recursive summation. While it produces the accurate result for the above example, it is only applicable for non-negative numbers, and more importantly, it requires an expensive sort. There exist a number of other algorithms for stable summation~\cite{numStabBook}. One notable technique is proposed by Kahan~\cite{kahan1965further}. It is the recursive summation with a correction term to reduce the rounding errors. Algorithm~\ref{algo:kahan} shows the incremental update rule for Kahan algorithm.

%\textbf{Stable Summation.} Summation is a fundamental operation in many statistical functions, such as mean, variance, and norms. When performed using the simplest method, it suffers from numerical inaccuracies even on a single computer. Consider the summation of the following six numbers: $1.0, 0.04, 0.04, 0.04, 0.04, 0.04$. With 2 digit precision, the exact answer should be $sum=1.2$. However, the naive \textit{recursive summation} algorithm, which initializes $sum=0$ and keeps $sum=sum+x_i$, will results in $sum=1.0$. This is because once the first element $1.0$ is added to $sum$, adding $0.04$ will have no effect on $sum$ due to round-off error. %This naive algorithm is used in Pig, Hive and JAQL.

%Summations of numerical values are ubiquitous in statistical analysis. They appear in all kinds of statistical functions, such as mean, variance, norms and so on. As illustrated in the example below, even calculating the accurate summation of a set of numbers in a single machine is very tricky. Consider the summation of the following six numbers: $1.0, 0.04, 0.04, 0.04, 0.04, 0.04$. With 2 digit precision, the exact answer should be $sum=1.2$. However, the naive \textit{recursive summation} algorithm, which initializes $sum=0$ and keeps $sum=sum+x_i$, will results in $sum=1.0$. This is because once the first element $1.0$ is added to the sum, adding $0.04$ will have no effect on the sum due to round-off error.

%The core of the Kahan summation is an incremental update algorithm, shown in Algorithm~\ref{algo:kahan}, which aggregates two partial sums with their associated correction terms\footnote{For a single data item, the sum is its value and the correction term is 0.}. 

%This algorithm, when run on a single machine, has an error bound $|E_n|=|\hat{S}_n-S_n|\leq (2u+O(nu^2))\sum\limits_{i=1}^n|x_i|$, where $S_n$ is the true sum of $n$ numerical values $x_1, x_2,...,x_n$, $\hat{S}_n$ is the result produced by the summation algorithm, and $u$ is the \textit{unit roundoff}.  $u=\frac{1}{2}\beta^{1-t}$ for a floating point system with base $\beta$ and precision $t$. For IEEE double with $\beta=2$ and $t=53$, $u=2^{-53}\approx 10^{-16}$. When $nu\leq 1$, the error is independent of the number of data items.

%To produce accurate summation for large volumes of data on MapReduce, we need an algorithm that is easy to be parallelized and robust with increasing number of input data items. There have been well-studied summation algorithms in numerical analysis~\cite{numStabBook}. For non-negative numbers, recursively adding up the numbers by their ascending order will result in more accurate sum than the naive recursive summation. For example, this approach will result in the right summation $1.2$ for the above sequence of numbers. But this \textit{ordered recursive sum} requires first an expensive full sort on the set of data and then another scan of the data. Luckily, there is a numerically stable summation algorithm, called \textit{Kahan summation} that just requires one scan of the data. The \textit{Kahan summation} algorithm, shown in Algorithm~\ref{algo:kahan}, is a compensated summation method, which is recursive summation with a correction term to reduce the rounding errors. This algorithm, when run on a single machine, has an error bound $|E_n|=|\hat{S}_n-S_n|\leq (2u+O(nu^2))\sum\limits_{i=1}^n|x_i|$, where $S_n$ is the true sum of $n$ numerical values $x_1, x_2,...,x_n$, $\hat{S}_n$ is the result produced by the summation algorithm, and $u$ is the \textit{unit roundoff}.  $u=\frac{1}{2}\beta^{1-t}$ for a floating point system with base $\beta$ and precision $t$. For IEEE double with $\beta=2$ and $t=53$, $u=2^{-53}\approx 10^{-16}$. When $nu\leq 1$, the error is independent of the number of data items.

One can easily extend the Kahan algorithm to the MapReduce setting. 
%The Kahan summation algorithm can be easily adapted to the MapReduce environment. 
The resulting algorithm % shown in Algorithm~\ref{algo:mrkahan}, 
is a MapReduce job in which each mapper applies \textsc{KahanIncrement} and generates a partial sum with correction, and a single reducer produces the final sum. % by applying \textsc{KahanIncrement}. 
Through error analysis, we can derive that when each mapper processes $\leq 10^{16}$ data items, this algorithm is robust w.r.t. the total number of data items to be summed using IEEE doubles (proof omitted). 

%Through error analysis, we derive an error bound for this MapReduce Kahan summation algorithm: $E_n\leq [4u+4u^2+O(mu^2)+O(\frac{n}{m}u^2)+O(\frac{n}{m}u^3)+O(mu^3)+O(nu^4)]\sum\limits_{i=1}^n|x_i|$. (Proof is omitted in the interest of space.) Here, $E_n$ is the difference between the true sum and the produced sum for $x_1, x_2,...,x_n$, $u$ is the \textit{unit roundoff}\footnote{$u=\frac{1}{2}\beta^{1-t}$ for a floating point system with base $\beta$ and precision $t$.}, and $m$ is the number of mappers  ($m < n$). As long as $\frac{n}{m}u\leq 1$, the error is independent of number of input data items. For IEEE double, this means $\frac{n}{m}\leq 2^{53} \approx 10^{16}$. In other words, when each mapper process $\leq 10^{16}$ data items, the algorithm is robust with respect to the total number of data items to be summed.

%Following the error analysis for Kahan summation, we can easily derive the error bound for this MapReduce Kahan summation algorithm: $E_n\leq [4u+4u^2+O(mu^2)+O(\frac{n}{m}u^2)+O(\frac{n}{m}u^3)+O(mu^3)+O(nu^4)]\sum\limits_{i=1}^n|x_i|$. (Proof is omitted in the interest of space.) Here, $m$ is the number of mappers used in the MapReduce job ($m < n$). As long as $\frac{n}{m}u\leq 1$, the error is independent of number of input data items. For IEEE double, this means $\frac{n}{m}\leq 2^{53} \approx 10^{16}$. In other words, when each mapper process $\leq 10^{16}$ data items, the algorithm is robust with respect to the total number of data items to be summed.

%As a simple example, consider the sum of a sequence of numbers $1.0, 0.04, 0.04, 0.04, 0.04, 0.04$. With 2 digits precision, the correct answer should be $sum=1.2$. However, the naive \textit{recursive summation} algorithm, which initializes $sum=0$ and keeps $sum=sum+x_i$, will results in $sum=1.0$. This is because once the first element $1.0$ is added to the sum, adding $0.04$ will have no effect on the sum, due to round-off error. But if we reorder the sequence in ascending order $0.04, 0.04, 0.04, 0.04, 0.04, 1.0$, then the recursive summation algorithm will generate the correct result $sum=1.2$. 


%Table~\ref{tab:sum} lists these algorithms and their error bound analysis in the sequential environment. The \textit{recursive sum} is the naive algorithm described before. The \textit{pairwise sum} algorithm recursively performs pair-wise addition, reducing the array size by a factor of two in each recursion. The \textit{Kahan sum} algorithm is a compensated summation method, which is recursive summation with a correction term to reduce the rounding errors. It is described in Algorithm~\ref{algo:kahan}. In the error analysis shown in Table~\ref{tab:sum}, $S_n$ is the true sum of $n$ numerical values $x_1, x_2,...,x_n$, $\hat{S}_n$ is the result produced by a summation algorithm, $E_n=\hat{S}_n-S_n$ is the error of a summation algorithm, $u$ is the \textit{unit roundoff}, $u=\frac{1}{2}\beta^{1-t}$ for a floating point system with base $\beta$ and precision $t$. For IEEE double with $\beta=2$ and $t=53$, $u=2^{-53}\approx 10^{-16}$.

%The MapReduce algorithm is shown in Algorithm~\ref{algo:mrkahan}. Error bound is 
%$E_n\leq [4u+4u^2+O(mu^2)+O(\frac{n}{m}u^2)+O(\frac{n}{m}u^3)+O(mu^3)+O(nu^4)]\sum\limits_{i=1}^n|x_i|$. Here, $m$ is the number of mappers used in the MapReduce job ($m < n$). As long as $\frac{n}{m}u\leq 1$, the error is independent of input data size. For IEEE double, this means $\frac{n}{m}\leq 2^{53} \approx 10^{16}$.

\input{kahan_alg}

%\input{mr_kahan_alg}


%\begin{table}[t]
%\centering
%\begin{tabular}{|c|l|}
%\hline
%Algorithm & Error Bound: $|E_n|=|\hat{S}_n-S_n|$\\
%\hline
%Recursive Sum & $\leq (n-1)u\sum\limits_{i=1}^n|x_i|+O(u^2)$\\
%\hline
%Pairwise Sum & $\leq {\frac{u\log{n}}{1-u\log{n}}}\sum\limits_{i=1}^n|x_i|$ \\
%\hline
%Kahan Sum & $\leq (2u+O(nu^2))\sum\limits_{i=1}^n|x_i|$ \\
%\hline
%\end{tabular}
%\caption{Sequential summation algorithms and their error analysis. }
%\label{tab:sum}
%\end{table}

\input{highOrderStats}

We now summarize the lessons learned while implementing scalable and numerically stable descriptive statistics in SystemML.
\\

\noindent $\bullet$ \textbf{Many existing sequential techniques for numerical stability can be adapted to the distributed environment.} 

We successfully adapted the existing sequential stable algorithms for summation, mean, central moments and covariance to the MapReduce environment. Such adaptations are empirically shown to exhibit better numeric stability when compared to commonly used naive algorithms.
\\

\noindent $\bullet$ \textbf{Performance need not be sacrificed for accuracy.} 

While software packages like $BigDecimal$ can be used to improve the numerical accuracy of computations, they incur significant performance overhead -- we observed up to 5 orders of magnitude slowdown depending on the exact operation and precision used. Similarly, the {\em sorted sum} technique that is widely used in sequential environments is not able to achieve similar accuracy when adapted to a distributed environment. Furthermore, its performance is hindered by the fact that the entire data has be sorted up front. In contrast, our stable algorithms for summation, mean, covariance, and higher order statistics achieve good accuracy without sacrificing the runtime performance -- they make a single scan over the input data, and achieve comparable runtime performance with the unstable one-pass algorithms.
\\

%Although BigDecimal provides the flexibility to use any algorithms for accurate results, it incurs substantial performance overhead (up to 5 orders of magnitude slowdown depending on the precision observed in our study). Our stable summation, central moments and covariance algorithms only perform one pass over data, and achieve comparable performance to the corresponding unstable one-pass algorithms. The sorted summation algorithm, although a common practice in the sequential environment, is inferior to our stable summation method in terms of both performance and accuracy.

\noindent $\bullet$ \textbf{Shifting can be used for improved accuracy.} 

When all the values in the input data set are of large magnitude, it is useful to shift the elements by a constant (minimum value or approximate mean) prior to computing any statistics. This preprocessing technique helps in reducing the truncation errors, and often achieves better numerical accuracy. In the absence of such preprocessing, as shown in Sections~\ref{sec:univariate_exp}~\&~\ref{sec:bivariate_exp}, the accuracy of statistics degrades as the magnitude of input values increases (e.g., as the value range changes from R1 to R3).
\\

%Through the experiments in Section~\ref{sec:univariate_exp} and Section~\ref{sec:univariate_exp}, we observed that for a same delta, ranges with higher means suffer from degraded accuracies. However, by shifting all the data by an approximation of mean before attempting to compute descriptive statistics, significant gains of accuracy can be achieved.

%\noindent $\bullet$ \textbf{Divide-and-conquer improves the boundary of numerical stability for large data.} 
\noindent $\bullet$ \textbf{Divide-and-conquer design helps in scaling to larger data sets while achieving good numerical accuracy.} 

All the stable algorithms discussed in this paper operate in a divide-and-conquer paradigm, in which the partial results are computed in the mappers and combined later in the reducer. These algorithms partitions the work into smaller pieces, and hence they can scale to larger data sets while keeping the relative error bound independent of the input data size. For example, the sequential version of Kahan summation algorithm guarantees that the relative error bound is independent of the input data size as long as the {\em total number} of elements is in the order of $10^{16}$. In contrast, the MapReduce version of Kahan summation algorithm can provide similar guarantees as long as the data size processed by {\em each} mapper is in the order of $10^{16}$. In the words, the parallel version of the algorithm can scale to larger data sets without having a significant impact on the error bound. Here, we assume that the number of mappers is bounded by a constant which is significantly smaller than the input data size. While there is no formal proof of such a result for covariance and other higher-order statistics, we expect the distributed algorithms for these statistics to scale better than their sequential counterparts.
\\

%For the MapReduce Kahan summation algorithm, we have derived an error bound with higher boundary of numerical stability in Section~\ref{sec:sum}. More specifically, for sequential Kahan summation, the upper bound of data size that can be processed stably is in the order $10^{16}$. By applying the divide-and-conquer strategy, the MapReduce Kahan summation, has a much higher bound: as long as each mapper process less than the order of $10^{16}$ data entries. Although we don't have formal proof for the MapReduce central moments and covariance algorithms, we expect them to present similar trend.

\noindent $\bullet$ \textbf{Kahan technique is useful beyond a simple summation.} 

The strategy of keeping the correction term, as in Kahan summation algorithm, helps in alleviating the effect of truncation errors. Beyond the simple summation, we found this technique to be useful in computing stable values for other univariate and bivariate statistics, including mean, variance, covariance, Eta, ANOVA-F etc.


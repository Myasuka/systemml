
{\em Numerical stability} refers to the inaccuracies in computation resulting from finite precision floating point arithmetic on digital computers with round-off and truncation errors. Multiple algebraically equivalent formulations of the same numerical calculation often produce very different results. While some methods magnify these errors, others are more robust or stable. The exact nature and the magnitude of these errors depend on several different factors like the number of bits used to represent floating point numbers in the underlying architecture (commonly known as {\em precision}), the type of computation that is being performed, and also on the number of times a particular operation is performed. Such round-off and truncation errors typically grow with the input data size. Given the exponential growth in the amount of data that is being collected and processed in recent years, numerical stability becomes an important issue for many practical applications. 

One possible strategy to alleviate these errors is to use special software packages that can represent floating point values with {\em arbitrary} precision. For example, $BigDecimal$ in Java provides the capability to represent and operate on arbitrary-precision signed decimal numbers. Each $BigDecimal$ number is a Java object. While the operations like addition and subtraction on native data types (double, int) are performed on hardware, the operations on $BigDecimal$ are implemented in software. Furthermore, JVM has to explicitly manage the memory occupied by $BigDecimal$ objects. Therefore, these software packages often suffer from significant performance overhead. In our benchmark studies, we observed up to 2 orders of magnitude slowdown for addition, subtraction, and multiplication using $BigDecimal$ with precision $1000$ compared to the native $double$ data type; and up to 5 orders of magnitude slowdown for division.

In the rest of this section, we discuss methods adopted in SystemML to calculate descriptive statistics, which are both numerically stable and computationally efficient. We will also highlight the common pitfalls that must be avoided in practice. First, we discuss the fundamental operations {\em summation} and {\em mean}, and subsequently present the methods that we use for computing {\em higher-order statistics} and {\em covariance}.

%In the context of large distributed MapReduce-style environments, one needs effective algorithms that can both exploit massive data parallelism and produce robust results with the increasing number of input data items. 


%Most of descriptive statistics, except for order statistics, can be expressed in certain summation form, thus may appear simple to be implemented in MapReduce. However, straightforward implementations can sometimes lead to disasters in numerical accuracy, due to round-off and truncation errors associated with finite precision arithmetic. As the need for analyzing large volumes of data increases, \textit{numerical stability} becomes even more critical.

%In the MapReduce environment, one needs effective algorithms that can both exploit massive data parallelism and produce robust results with growing data sizes. Unfortunately, all the existing large scale platforms, such as  PIG~\cite{pig}, HIVE~\cite{hive} and JAQL~\cite{jaql}, are still using numerical unstable algorithms to compute basic statistics. For example, PIG as of version 0.9.1 only provides the very basic \textit{sum} and \textit{mean} functions, and neither of them are computed using numerically stable algorithms. The recent version of HIVE (0.7.1) supports more statistics including \textit{sum}, \textit{mean}, \textit{variance}, \textit{standard deviation}, \textit{covariance}, and \textit{corelation}. Luckily, the developers who implemented \textit{variance}, \textit{standard deviation}, \textit{covariance}, and \textit{corelation} are using stable algorithms similar to our approaches. However, \textit{sum} and \textit{mean} are still computed using numerically unstable algorithms. Some of these large scale systems support BigDecimal type, but it incurs heavy performance overhead, thus is not recommended for large scale data processing. In our experiments, we observed that simple addition, subtraction and multiplication using BigDecimal is 2 orders of magnitude slower than the equivalent operations using double. And division in BigDecimal can be 5 orders of magnitude slower than double with a precision 1000.  
%Although some of these systems support BigDecimal types, using BigDecimal for numerical computation brings significant performance overhead, thus is not recommended for large scale data processing.}. 
%Numerically unstable algorithms are still in use, even in well-known distributed database products~\cite{teradata}. 

%In this section, we use summation and central moment as the driving examples to highlight the common pitfalls in implementing descriptive statistics, and describe how numerical stability is achieved in SystemML.

%
%Numerical instability refers to inaccuracies in computation resulting from finite precision floating point arithmetic on digital computers with round-off and truncation errors. Quite often, multiple algebraically equivalent formulations of the same numerical calculation produce very different results -- some may propagate and magnify these approximation errors whereas others may be more robust or numerically stable. As the need for analyzing large volumes of data increases, the issue of numerical stability becomes much more critical.
%In the context of large distributed MapReduce-style environments, one needs effective algorithms that can both exploit massive data parallelism and produce robust results with the increasing number of input data items. 

%To the best of our knowledge, none of the existing large scale data processing platforms such as PIG~\cite{pig}, HIVE~\cite{hive} and JAQL~\cite{jaql} pay attention to these issues\footnote{Although some of these systems support BigDecimal types, using BigDecimal for numerical computation brings significant performance overhead, thus is not recommended for large scale data processing.}. In this section, we use summation and arbitrary order central moment as the driving examples, to highlight the common pitfalls of implementing descriptive statistics and describe how numerical stability is achieved in SystemML.

%Numerical instability is a problem caused by the finite precision arithmetic on numerical algorithms. If there were no round-off or truncation errors associated with finite precision arithmetic, numerical algorithms would always approach the right solution. Quite often, different algebraically equivalent ways of implementing a numerical calculation will result in very different result, with some propagating and magnifying approximation errors and others more robust -- also called numerically stable. The goal of most numerical analysis is to select these numerically stable algorithms. 

\subsection{Stable Summation}
\label{sec:sum}

Summation is a fundamental operation in many statistical functions, such as mean, variance, and norms. The simplest method is to perform {\em naive recursive summation}, which initializes $sum=0$ and incrementally updates $sum$. It however suffers from numerical inaccuracies even on a single computer. For instance, with $2$-digit precision, naive summation of numbers $1.0, 0.04, 0.04, 0.04, 0.04, 0.04$ results in $1.0$, whereas the exact answer is $1.2$. This is because once the first element $1.0$ is added to $sum$, adding $0.04$ will have no effect on $sum$ due to round-off error. %Both PIG and HIVE use this naive recursive algorithm to compute sum. 
A simple alternative strategy is to first sort the data in increasing order, and subsequently perform the naive recursive summation. While it produces the accurate result for the above example, it is only applicable for non-negative numbers, and more importantly, it requires an expensive sort. 

%There exist a number of other algorithms for stable summation~\cite{numStabBook}. One notable technique is proposed by Kahan~\cite{kahan1965further}. It is the recursive summation with a correction term to reduce the rounding errors. Algorithm~\ref{algo:kahan} shows the incremental update rule for Kahan algorithm. 

There exists a number of other methods for stable summation~\cite{numStabBook}. One notable technique is proposed by Kahan~\cite{kahan1965further}. It is a {\em compensated summation} technique -- see Algorithm~\ref{algo:kahan}. This method maintains a {\em correction} or {\em compensation} term to accumulate errors encountered in naive recursive summation. 

\input{kahan_alg}

Kahan and Knuth independently proved that Algorithm~\ref{algo:kahan} has the following relative error bound~\cite{numStabBook}: 
\begin{equation}
\begin{small}
\label{eq:relerror}
\frac{|E_n|}{|S_n|}=\frac{|\hat{S}_n-S_n|}{|S_n|}\leq (2u+O(nu^2))\condnumber,
\end{small}
\end{equation}
where $S_n=\sum\limits_{i=1}^n x_i$ denotes the true sum of a set of $n$ numbers $X=\{x_1, x_2,...,x_n\}$, and $\hat{S}_n$ is the sum produced by the summation algorithm, $u=\frac{1}{2}\beta^{1-t}$ is the {\em unit roundoff} for a floating point system with base $\beta$ and precision $t$. It denotes the upper bound on the relative error due to rounding. For IEEE 754 floating point standard with $\beta=2$ and $t=53$, $u = 2^{-53}\approx 10^{-16}$. Finally, $\condnumber$ is known as the {\em condition number} for the summation problem, and it is defined as the fraction $\frac{\sum\limits_{i=1}^n|x_i|}{|\sum\limits_{i=1}^n x_i|}$. The condition number measures the sensitivity of the problem to approximation errors, independent of the exact algorithm used. Higher the value of $\condnumber$, the higher will be the numerical inaccuracies and the relative error. It can be shown that for a random set of numbers with a nonzero mean, the condition number of summation asymptotically approaches to a finite constant as $n\to\infty$. Evidently, the condition number is equal to $1$ when all the input values are non-negative. It can be seen from Equation~\ref{eq:relerror} that when $nu\leq 1$, the bound on the relative error is independent of input size $n$. In the context of IEEE 754 standard, it means that when $n$ is in the order of $10^{16}$ the relative error can be bounded independent of the problem size. In comparison, the naive recursive summation has a relative error bound $\frac{|E_n|}{|S_n|}\leq (n-1)u\condnumber +\frac{O(u^2)}{|\sum\limits_{i=1}^n x_i|}$~\cite{numStabBook}, which clearly is a much larger upper bound when compared to Equation~\ref{eq:relerror}.

%It is proved in~\cite{numStabBook} that this algorithm has a relative error bound $\frac{|E_n|}{|S_n|}=\frac{|\hat{S}_n-S_n|}{|S_n|}\leq (2u+O(nu^2))\condnumber$, where $S_n=\sum\limits_{i=1}^n x_i$ is the true sum of a set of $n$ numerical values $X=\{x_1, x_2,...,x_n\}$, $\hat{S}_n$ is the result produced by the summation algorithm, $\condnumber =\frac{\sum\limits_{i=1}^n|x_i|}{|\sum\limits_{i=1}^n x_i|}$, and $u$ is the \textit{unit roundoff}. $u=\frac{1}{2}\beta^{1-t}$ for a floating point system with base $\beta$ and precision $t$. For IEEE double with $\beta=2$ and $t=53$, $u=2^{-53}\approx 10^{-16}$. $\condnumber $ is often called the \textit{condition number} of a summation problem. It measures the sensitivity of the sum for a given data set, independent of the algorithm used. It is proved that for random inputs with a nonzero mean the condition number asymptotically approaches to a finite constant as $n\to\infty$. Obviously, when inputs are all non-negative, the condition number is 1. Therefore, given a constant condition number, when $nu\leq 1$, the relative error is in the precision of $u$, which is independent of the number of data items $n$. For IEEE double, this means when $n\leq 10^{16}$, the relative error is independent of problem size. In comparison, the naive recursive summation has a relative error bound $\frac{|E_n|}{|S_n|}\leq (n-1)u\condnumber +O(u^2)$, which increases linearly with the problem size~\cite{numStabBook}.


%\textbf{Stable Summation.} Summation is a fundamental operation in many statistical functions, such as mean, variance, and norms. When performed using the simplest method, it suffers from numerical inaccuracies even on a single computer. Consider the summation of the following six numbers: $1.0, 0.04, 0.04, 0.04, 0.04, 0.04$. With 2 digit precision, the exact answer should be $sum=1.2$. However, the \textit{naive recursive summation} algorithm, which initializes $sum=0$ and keeps $sum=sum+x_i$, will results in $sum=1.0$. This is because once the first element $1.0$ is added to $sum$, adding $0.04$ will have no effect on $sum$ due to round-off error. %This naive algorithm is used in Pig, Hive and JAQL.

%Summations of numerical values are ubiquitous in statistical analysis. They appear in all kinds of statistical functions, such as mean, variance, norms and so on. As illustrated in the example below, even calculating the accurate summation of a set of numbers in a single machine is very tricky. Consider the summation of the following six numbers: $1.0, 0.04, 0.04, 0.04, 0.04, 0.04$. With 2 digit precision, the exact answer should be $sum=1.2$. However, the \textit{naive recursive summation} algorithm, which initializes $sum=0$ and keeps $sum=sum+x_i$, will results in $sum=1.0$. This is because once the first element $1.0$ is added to the sum, adding $0.04$ will have no effect on the sum due to round-off error.

%The core of the Kahan summation is an incremental update algorithm, shown in Algorithm~\ref{algo:kahan}, which aggregates two partial sums with their associated correction terms\footnote{For a single data item, the sum is its value and the correction term is 0.}. 

%This algorithm, when run on a single machine, has an error bound $|E_n|=|\hat{S}_n-S_n|\leq (2u+O(nu^2))\sum\limits_{i=1}^n|x_i|$, where $S_n$ is the true sum of $n$ numerical values $x_1, x_2,...,x_n$, $\hat{S}_n$ is the result produced by the summation algorithm, and $u$ is the \textit{unit roundoff}.  $u=\frac{1}{2}\beta^{1-t}$ for a floating point system with base $\beta$ and precision $t$. For IEEE double with $\beta=2$ and $t=53$, $u=2^{-53}\approx 10^{-16}$. When $nu\leq 1$, the error is independent of the number of data items.

%To produce accurate summation for large volumes of data on MapReduce, we need an algorithm that is easy to be parallelized and robust with increasing number of input data items. There have been well-studied summation algorithms in numerical analysis~\cite{numStabBook}. For non-negative numbers, recursively adding up the numbers by their ascending order will result in more accurate sum than the naive recursive summation. For example, this approach will result in the right summation $1.2$ for the above sequence of numbers. But this \textit{ordered recursive sum} requires first an expensive full sort on the set of data and then another scan of the data. Luckily, there is a numerically stable summation algorithm, called \textit{Kahan summation} that just requires one scan of the data. The \textit{Kahan summation} algorithm, shown in Algorithm~\ref{algo:kahan}, is a compensated summation method, which is recursive summation with a correction term to reduce the rounding errors. This algorithm, when run on a single machine, has an error bound $|E_n|=|\hat{S}_n-S_n|\leq (2u+O(nu^2))\sum\limits_{i=1}^n|x_i|$, where $S_n$ is the true sum of $n$ numerical values $x_1, x_2,...,x_n$, $\hat{S}_n$ is the result produced by the summation algorithm, and $u$ is the \textit{unit roundoff}.  $u=\frac{1}{2}\beta^{1-t}$ for a floating point system with base $\beta$ and precision $t$. For IEEE double with $\beta=2$ and $t=53$, $u=2^{-53}\approx 10^{-16}$. When $nu\leq 1$, the error is independent of the number of data items.

One can easily extend the Kahan algorithm to the MapReduce setting. 
%The Kahan summation algorithm can be easily adapted to the MapReduce environment. 
The resulting algorithm % shown in Algorithm~\ref{algo:mrkahan}, 
is a MapReduce job in which each mapper applies \textsc{KahanIncrement} and generates a partial sum with correction, and a single reducer produces the final sum. % by applying \textsc{KahanIncrement}. 
%Through error analysis, we can derive that when each mapper processes $\leq 10^{16}$ data items, this algorithm is robust w.r.t. the total number of data items to be summed using IEEE doubles (proof omitted). 

Through error analysis, we can derive the relative error bound for this MapReduce Kahan summation algorithm: $\frac{|E_n|}{|S_n|}\leq [4u+4u^2+O(mu^2)+O(\frac{n}{m}u^2)+O(mu^3)+O(\frac{n}{m}u^3)+O(nu^4)]\condnumber $ (see Appendix for proof). Here, $m$ is the number of mappers ($m$ is at most in 1000s). As long as $\frac{n}{m}u\leq 1$ (and $m \ll n$), the relative error is independent of the number of input data items. In the context of IEEE 754 standard, it can be shown that when $\frac{n}{m}$ is in the order of $2^{53} \approx 10^{16}$, the relative error can be bounded independent of $n$. In other words, as long as the number of elements processed by each mapper is in the order of $10^{16}$, the overall summation is robust with respect to the total number of data items to be summed. Therefore, by partitioning the work across multiple mappers, the MapReduce summation method is able to scale to larger data sets while keeping the upper bound on relative error independent of the input data size $n$.

%Following the error analysis for Kahan summation, we can easily derive the error bound for this MapReduce Kahan summation algorithm: $E_n\leq [4u+4u^2+O(mu^2)+O(\frac{n}{m}u^2)+O(\frac{n}{m}u^3)+O(mu^3)+O(nu^4)]\sum\limits_{i=1}^n|x_i|$. (Proof is omitted in the interest of space.) Here, $m$ is the number of mappers used in the MapReduce job ($m < n$). As long as $\frac{n}{m}u\leq 1$, the error is independent of number of input data items. For IEEE double, this means $\frac{n}{m}\leq 2^{53} \approx 10^{16}$. In other words, when each mapper process $\leq 10^{16}$ data items, the algorithm is robust with respect to the total number of data items to be summed.

%As a simple example, consider the sum of a sequence of numbers $1.0, 0.04, 0.04, 0.04, 0.04, 0.04$. With 2 digits precision, the correct answer should be $sum=1.2$. However, the naive \textit{recursive summation} algorithm, which initializes $sum=0$ and keeps $sum=sum+x_i$, will results in $sum=1.0$. This is because once the first element $1.0$ is added to the sum, adding $0.04$ will have no effect on the sum, due to round-off error. But if we reorder the sequence in ascending order $0.04, 0.04, 0.04, 0.04, 0.04, 1.0$, then the recursive summation algorithm will generate the correct result $sum=1.2$. 


%Table~\ref{tab:sum} lists these algorithms and their error bound analysis in the sequential environment. The \textit{recursive sum} is the naive algorithm described before. The \textit{pairwise sum} algorithm recursively performs pair-wise addition, reducing the array size by a factor of two in each recursion. The \textit{Kahan sum} algorithm is a compensated summation method, which is recursive summation with a correction term to reduce the rounding errors. It is described in Algorithm~\ref{algo:kahan}. In the error analysis shown in Table~\ref{tab:sum}, $S_n$ is the true sum of $n$ numerical values $x_1, x_2,...,x_n$, $\hat{S}_n$ is the result produced by a summation algorithm, $E_n=\hat{S}_n-S_n$ is the error of a summation algorithm, $u$ is the \textit{unit roundoff}, $u=\frac{1}{2}\beta^{1-t}$ for a floating point system with base $\beta$ and precision $t$. For IEEE double with $\beta=2$ and $t=53$, $u=2^{-53}\approx 10^{-16}$.

%The MapReduce algorithm is shown in Algorithm~\ref{algo:mrkahan}. Error bound is 
%$E_n\leq [4u+4u^2+O(mu^2)+O(\frac{n}{m}u^2)+O(\frac{n}{m}u^3)+O(mu^3)+O(nu^4)]\sum\limits_{i=1}^n|x_i|$. Here, $m$ is the number of mappers used in the MapReduce job ($m < n$). As long as $\frac{n}{m}u\leq 1$, the error is independent of input data size. For IEEE double, this means $\frac{n}{m}\leq 2^{53} \approx 10^{16}$.

%\input{mr_kahan_alg}


%\begin{table}[t]
%\centering
%\begin{tabular}{|c|l|}
%\hline
%Algorithm & Error Bound: $|E_n|=|\hat{S}_n-S_n|$\\
%\hline
%Recursive Sum & $\leq (n-1)u\sum\limits_{i=1}^n|x_i|+O(u^2)$\\
%\hline
%Pairwise Sum & $\leq {\frac{u\log{n}}{1-u\log{n}}}\sum\limits_{i=1}^n|x_i|$ \\
%\hline
%Kahan Sum & $\leq (2u+O(nu^2))\sum\limits_{i=1}^n|x_i|$ \\
%\hline
%\end{tabular}
%\caption{Sequential summation algorithms and their error analysis. }
%\label{tab:sum}
%\end{table}

\input{stablemean}

\input{highOrderStats}

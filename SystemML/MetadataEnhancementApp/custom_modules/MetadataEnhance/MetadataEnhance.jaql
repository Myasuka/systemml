
addClassPath("lib/metaDataParse.jar");
eval = javaudf("com.ibm.systemML.dataTransformation.metaDataParse");


// --------------------------------------------------------------------------------
// Read the input data in delimited format
// --------------------------------------------------------------------------------

inputData = fn(input, metadataMem, inputDelimiter) (

if (metadataMem == [])
(
   inputArray = if (isnull(inputDelimiter))
		  (
			read (del(location=input) )
		  )
		  else
		  (
			read (del(location=input, delimiter=inputDelimiter) )
		  ),
   numCols = inputArray[0] -> count(),
   attributeNames = range(1, numCols) -> transform serialize($),
   inputArray -> transform arrayToRecord(attributeNames, $)
)
else
(
   schemaString = metadataMem
      -> transform $.attributeName + ":" + $.attributeType
      -> strJoin(","),

   // schema of the input
   inputSchema = "schema {" + schemaString + "}",

   if (isnull(inputDelimiter))
		  (
			read (del(location=input, 
				schema = eval(inputSchema) ) )
		  )
		  else
		  (
			read (del(location=input, delimiter=inputDelimiter,
				schema = eval(inputSchema) ) )
		  )
)
);


//
// Generate metadata json file
// We do some pre-aggregation to avoid Java Heap size problem for distinct()
// in group by.
//

genMetadata = fn (inputData)
(
   inputData
      -> expand (fields($))
      -> group by d = $ 
            into { attributeName: d[0] 
                  ,num: count($) 
              }
      -> group by d = $.attributeName 
            into { attributeName: d
                  ,attributeCount: sum($[*].num)
                  ,attributeDistinctCount: count($[*].num) 
                 }
      -> group 
            into (cC = inputData->count(), cA = count($),
   	       $ -> transform each a {a.*, 
                                         caseCount: cC,
                                         countAttributes: cA
                                        }
                 ) 
      -> expand $
         // set attribute kind based on some heuristics
      -> transform if ($.attributeDistinctCount > 0.10 * $.caseCount)
                       {${*-}, attributeKind: "scale", attributeType: "string"} 
   		else                      
                       {${*-}, attributeKind: "nominal", attributeType: "string"}
      -> sort by [$.attributeName] 
      -> enumerate()
      -> transform {"attributeColumnId":$[0] + 1, $[1].*}
);


// --------------------------------------------------------------------------------
// Insert a heuristically derived value for any missing (null) attributeKind fields
// --------------------------------------------------------------------------------

metaEnhance = fn(inputData, metadataMem, outputMetadata) (

kindHeuristic = genMetadata (inputData),

join orig in metadataMem, preserve new in kindHeuristic 
   where orig.attributeName == new.attributeName
   into {
      new.attributeName,
      "attributeColumnId":(if (isnull(orig.attributeColumnId)) new.attributeColumnId else orig.attributeColumnId),
      "attributeKind":(if (isnull(orig.attributeKind)) new.attributeKind else orig.attributeKind),
      "attributeType":(if (isnull(orig.attributeType)) new.attributeType else orig.attributeType)
   }
   -> sort by [$.attributeColumnId]
   -> write(jsonText(outputMetadata))

);


//--------------------------------------------------------------------------------
// Recode Maps
//--------------------------------------------------------------------------------

jsonLines = fn(location)
    lines ( location,
        inoptions = { converter: "com.ibm.jaql.io.hadoop.converter.FromJsonTextConverter"},
        outoptions = { converter: "com.ibm.jaql.io.hadoop.converter.ToJsonTextConverter"});

genRcdMap = fn (inputData, attrs, outFile) (

   keep = (attrs -> filter $.attributeKind != "scale")[*].attributeName,
   inputData
   -> expand (fields($))
   -> filter $[0] in keep
   -> group by d = $ into d
   -> group by aN = $[0] as dVals into (
         x = dVals[*][1] -> filter not isnull $,	      
         attrKind = singleton(attrs -> filter $.attributeName == aN).attributeKind,
         if (attrKind != "ordinal") (
               x 
             ) else (
               x -> sort by [$ desc]
             )
             -> enumerate()
             -> transform {attributeName: aN, recodeId: $[0] + 1, catValue: $[1]} 
      )
   -> expand
   -> write(jsonLines(outFile))

);

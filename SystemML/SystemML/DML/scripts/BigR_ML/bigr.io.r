# bigr.io
#
# This file contains functions to handle I/O operations to read/write
# from different sources (e.g., BigSQL, delimited (text) files, etc.) 

# This method reads a big.frame from the corresponding data source. It returns
# a table expression as a JaQL query.
.bigr.read <- function(bf) {
    logSource <- "read"
    exp <- NULL
    bigr.info(logSource, "Reading...");
    if (bf@dataSource == bigr.env$TEXT_FILE) {
        bigr.info(logSource, "Reading a text file...")
        # Check that the file exists
        if (!.bigr.fileExists(bf@dataPath)) {
            bigr.err(logSource, "File '" %++% bf@dataPath %++% "' does not exist in HDFS.")
        }
        
        # If colnames are not empty, coltypes cannot be empty either, since the constructor
        # for bigr.frame automatically adjusts colnames/types
        if (length(bf@colnames) > 1) {
            bigr.info(logSource, "1. length(colnames) is greater than 1.")
            
            # If the user did not specify column names, but they were automatically generated
            # AND headers were specified, colnames should be picked up from headers
            if (bf@colnames[1] == bigr.env$DEFAULT_COLNAMES & bf@header) {
                bigr.info(logSource, "1.1 colnames was autogenerated, picking up colnames from the headers...")
                headers <- .bigr.getHeaderNames(bf)
                if (.bigr.validColnames(headers)) {
                    bf@colnames <- headers
                }
            }            
            exp <- .bigr.readTextToArray(bf)
        
        # Use the headers as colnames if no colnames/coltypes are specified
        } else if (bf@header) {
            bigr.info(logSource, "2. No colnames were specified. Header was true.")
            headers <- .bigr.getHeaderNames(bf)
            if (.bigr.validColnames(headers)) {
                bf@colnames <- headers
                
                # Since coltypes were not specified, default coltypes will be used 
                bf@coltypes <- rep(bigr.env$DEFAULT_COLTYPES, length(bf@colnames))
                
                # Invoke method to read a text file into a JaQL array
                exp <- .bigr.readTextToArray(bf)
                
            } else {
                bigr.err(logSource, "Invalid header names: \n" %++% headerStr)
            }
        
        # If delimiter is specified but no headers and no colnames/coltypes, get the number of columns
        # from the first line and then use default colnames and types
        } else if (!.bigr.isNullOrEmpty(bf@delimiter) & bf@delimiter != "\n") {
            bigr.info(logSource, "3. No colnames/coltypes, no headers, but delimiter specified.")
            headers <- .bigr.getHeaderNames(bf)
            
            if (!.bigr.isNullOrEmpty(headers)) {
                # Since colnames were not specified, default coltypes will be used 
                bf@colnames <- rep("V", length(headers)) %++% seq(1:length(headers))
                
                # Since coltypes were not specified, default coltypes will be used 
                bf@coltypes <- rep(bigr.env$DEFAULT_COLTYPES, length(headers))
                
                # Invoke method to read a text file into a JaQL array
                exp <- .bigr.readTextToArray(bf)
            } else {
                # If something went wrong with the headers, read as lines
                exp <- .bigr.readTextLines(bf)
            }
        # If no delimiter, no headers, and no column information, then read as lines
        } else {
            bigr.info(logSource, "4. No colnames/coltypes, no headers, no delimiter, or delimiter == '\n'. Reading lines...")
            exp <- .bigr.readTextLines(bf)
        }
    } else if (bf@dataSource == bigr.env$BIG_SQL) {
        readlist <- .bigr.readBigSQL(bf)     
        # Set the column name and column types
        exp <- readlist$tableExpression
        bf@colnames <- readlist$colnames
        bf@coltypes <- readlist$coltypes
    } else if (bf@dataSource == bigr.env$LINE_FILE) {
        exp <- .bigr.readTextLines(bf)
    } else if (bf@dataSource == bigr.env$TRANSFORM) {
        # No need to read when TRANSFORM
        exp <- ""
    } else {
        bigr.err(logSource, "Invalid data source: '" %++% bf@dataSource %++% "'")
    }
    bf@tableExpression <- exp
    return(bf)
}

# Returns the header names for a given bigr.frame by reading from  the file
.bigr.getHeaderNames <- function(bf) {
    logSource <- ".bigr.getHeaderNames"
    df <- .bigr.executeJaqlQuery(.bigr.readTextLines(bf) %++% " -> top 1", limit=FALSE)    
    if (is.null(df) | nrow(df) < 1 | .bigr.isNullOrEmpty(bf@delimiter)) {
        bigr.err(logSource, "Could not read headers. The dataset is empty / inaccessible, or delimiter was not set")
    } else {
        headerStr <- df[[1]]
        #headerStr <- gsub("\\s","", headerStr)
        headers <- strsplit(headerStr, bf@delimiter)[[1]]
        bigr.infoShow(logSource, headers)
        return(headers)
    }
}

# This method generates the JaQL query to read a character-delimited file from 
# HDFS as an array of arrays. If the read was not successful, it returns NULL
.bigr.readTextToArray <- function(bf) {
    logSource <- "readTextToArray"
    bigr.info(logSource, "Reading text file...");
    if (is.null(bf)) {
        bigr.err(logSource, "Cannot read from a NULL bigr.frame")
        return(NULL)
    }    
    bigr.info(logSource, "Generating reading expression for '" %++% bf@dataPath %++% "'...")
    
    bigr.info(logSource, "colnames: ")
    bigr.info(logSource, paste(bf@colnames, collapse=","))
    bigr.info(logSource, "coltypes:")
    bigr.info(logSource, paste(bf@coltypes, collapse=","))
    
    # Map R types to Jaql types
    jaqlTypes <- bf@coltypes
    dataNames <- names(bigr.env$DATA_TYPES)
    for (i in 1:length(bigr.env$DATA_TYPES)) {
        jaqlTypes <- replace(jaqlTypes, jaqlTypes == dataNames[i], bigr.env$DATA_TYPES[i])
    }
    
    # Choose between read or localRead accordingly
    readInstruction <- "read"
    if (bf@localProcessing == TRUE) {
        readInstruction <- "localRead"
    }
    
    jaqlExpression <- NULL
    # Handle NA string
    if (!.bigr.isNullOrEmpty(bf@na.string)) {
    
        # Build JaQL expression with all field as strings
        stringTypes <- rep("string", length(bf@coltypes))    
        jaqlExpression <- readInstruction %++% "(del(location='" %++% bf@dataPath %++%
            "', delimiter='" %++% bf@delimiter %++% "', inoptions={header:" %++% tolower(bf@header) %++% "},schema=schema [" %++% 
            paste(stringTypes %++% "?", collapse=", ") %++% "] ))"    
        
        # Recode NA's as null values
        jaqlExpression <- jaqlExpression %++% " -> transform replaceNAs($, '" %++% bf@na.string %++% "')"
        
        # Assign the corresponding datatypes after NA's have been recoded
        i <- 1
        dataTypeExpression <- ""
        while (i <= length(bf@coltypes)) {
            if (jaqlTypes[i] != "string") {
                column <- jaqlTypes[i] %++% "($[" %++% (i - 1) %++% "])"
            } else {
                column <- "($[" %++% (i - 1) %++% "])"
            }        
            dataTypeExpression <- dataTypeExpression %++% column
            
            # All but the last one have comma at the end
            if (i < length(bf@coltypes)) {
                dataTypeExpression <- dataTypeExpression %++% ", "
            }
            i <- i + 1
        }
        jaqlExpression <- jaqlExpression %++% " -> transform [" %++% dataTypeExpression %++% "]"       
    } else {
        jaqlExpression <- readInstruction %++% "(del(location='" %++% bf@dataPath %++%
            "', delimiter='" %++% bf@delimiter %++% "', inoptions={header:" %++% tolower(bf@header) %++% "},schema=schema [" %++% 
            paste(jaqlTypes %++% "?", collapse=", ") %++% "] ))"  
    }
    bigr.info(logSource, jaqlExpression)    
    return(jaqlExpression)
}

# This method generates the JaQL query to read a file from 
# HDFS as an array of lines. If the read was not successful, it returns NULL
.bigr.readTextLines <- function(bf) {
    logSource <- "readDel"
    if (is.null(bf)) {
        bigr.err(logSource, "Cannot read from a NULL bigr.frame")
    }    
    bigr.info(logSource, "Generating read expression for '" %++% bf@dataPath %++% "'...")
    
    readInstruction <- "read"
    if (bf@localProcessing) {
        readInstruction <- "localRead"
    }    
    jaqlExpression <- readInstruction %++% "(lines(location='" %++% bf@dataPath %++% "'))"
 
    bigr.info(logSource, jaqlExpression)
    return(jaqlExpression)
}

# Checks whether a file exists in HDFS
.bigr.fileExists <- function(filePath) {
    if (.bigr.isNullOrEmpty(filePath)) {
        return(FALSE)
    }
    result <- .bigr.executeJaqlQuery("ls('" %++% filePath %++% "')", limit=FALSE)
    if (.bigr.isNullOrEmpty(result)) {
        return(FALSE)
    }
    if (length(result) != 1) {
        return(FALSE)
    }
    if (is.na(result[1,1])) {
        return(FALSE)
    }
    return(TRUE)
}

# Read from BigSQL
.bigr.readBigSQL <- function(bf) {
    logSource = "readBigSQL"

    if (is.null(bf)) {
        bigr.err(logSource, "Cannot read from a NULL bigr.frame")
        stop()
    }

    # Determine the columns of the table, if it exists
    cols <- bigr.listColumns(bf@dataPath, expand=F)
    if (is.null(cols) | nrow(cols) == 0) {
        bigr.err(logSource, sprintf("Cannot find table '%s'.", bf@dataPath))
    }
    
    # Only select columns whose datatypes map to R
    colidx <- cols$type %in% bigr.env$DATA_TYPES
    if (length(colidx[colidx == FALSE]) > 0) {
        bigr.warn(logSource, "Cannot map all column datatypes to R. The following columns have been omitted: ")
        bigr.warn(logSource, paste(cols$name[!colidx], collapse=", "))
    }
    
    # Return the table expression and column metadata
    if (grepl("^sys", bf@dataPath)) {
        # For catalog tables (i.e. tables starting with "sys", build a slightly
        # different JaQL expression. We have no way of specifying "localProcessing"
        # for such tables, but there's no harm done as catalog tables are typically
        # smaller than regular tables.
        qry <- sprintf("(select * from %s) -> transform [ %s ]",
                       bf@dataPath, paste(c("$."), cols$name[colidx], sep="", collapse=", "))
    }
    else {
        qry <- sprintf("%s(hcatTable('%s')) -> transform [ %s ]",
                       ifelse(bf@localProcessing, "localRead", "read"),
                       bf@dataPath, paste(c("$."), cols$name[colidx], sep="", collapse=", "))
    }
    colnames <- cols$name[colidx]
    pos <- match(cols$type[colidx], bigr.env$DATA_TYPES)
    coltypes <- names(bigr.env$DATA_TYPES)[pos]
    
    return (list(tableExpression=qry, colnames=colnames, coltypes=coltypes))
}

# This method allows to write a dataset with the specified output parameters.
.bigr.write <- function(dataset) {
    logSource <- ".bigr.write"
    
    # Missing/invalid parameter validation
    if (missing(dataset)) {
        dataset <- NULL
    }
    
    # If the dataset is to be exported as a text file
    if (dataset@dataSource == bigr.env$TEXT_FILE) {
        .bigr.writeToTextFile(dataset)
    }

    # If the dataset is to be exported as a JSON file
    if (dataset@dataSource == bigr.env$JSON_FILE) {
        .bigr.writeToJSONFile(dataset)
    }
}

# This method exports a bigr.dataset to a JSON file in HDFS
.bigr.writeToJSONFile <- function(dataset) {
    #print(paste("class: ", class(dataset)))
    #print(paste("datapath: ", dataset@dataPath))
    
    bf = dataset
    
    datasetJaqlExpr = .bigr.getJaqlExpression(bf)
    json_data_loc = dataset@dataPath
    
    cnames = paste("['", paste(colnames(bf), collapse="', '"), "']", sep="")
    
    # The writing command, either write or localWrite
    writeCommand <- "localWrite"    
    if (dataset@localProcessing == FALSE) {
        writeCommand <- "write"
    }
    
    # Build the JaQL update to write to HDFS
    jaqlExpr <- datasetJaqlExpr %++% 
        " -> " %++% "transform arrayToRecord(" %++% cnames %++% ", $)" %++% 
        "-> " %++% writeCommand %++% "(jsonLines('" %++% dataset@dataPath %++% "'))"
    #print(paste("jaqlExpression: ", jaqlExpr))    
    
    # Return the result of the JaQL query
    if (length(.bigr.executeJaqlQuery(jaqlExpr, limit=FALSE)) > 0) {
        return(TRUE)
    } else {
        return(FALSE)
    }
}

# This method exports a bigr.dataset to a text file in HDFS
.bigr.writeToTextFile <- function(dataset) {    
    logSource = "writeDel"
    if (is.null(dataset)) {
        bigr.err(logSource, "Cannot write a NULL bigr.frame")    
    }
    if (class(dataset) != bigr.env$FRAME_CLASS_NAME & class(dataset) != bigr.env$VECTOR_CLASS_NAME) {
        bigr.err(logSource, "Writing to HDFS is only supported for bigr.frame's and bigr.vector's")    
    }
    
    # The JaQL statement with the data to be written
    datasetJaqlExpr <- NULL
    
    # Append the header if specified
    if (dataset@header == TRUE) {
        # Use colnames as headers in the case of bigr.frame's
        if (class(dataset) == bigr.env$FRAME_CLASS_NAME) {
            datasetJaqlExpr <- "union([[" %++% paste("'" %++% dataset@colnames %++% "'", collapse = ",") %++% "]]," %++% 
                .bigr.getJaqlExpression(dataset) %++% ")"
        # Use name as header in the case of bigr.vector's
        } else if (class(dataset) == bigr.env$VECTOR_CLASS_NAME) {
            datasetJaqlExpr <- "union([['" %++% dataset@name %++% "']]," %++% 
                .bigr.getJaqlExpression(dataset) %++% ")"
        }
    } else {
        datasetJaqlExpr <- .bigr.getJaqlExpression(dataset)
    }
    
    # The writing command, either write or localWrite
    writeCommand <- "localWrite"    
    if (dataset@localProcessing == FALSE) {
        writeCommand <- "write"
    }
    
    # Build the JaQL update to write to HDFS
    jaqlExpr <- datasetJaqlExpr %++% " -> " %++% writeCommand %++% "(del(location='" %++% 
        dataset@dataPath %++% "', delimiter='" %++% dataset@delimiter %++% "', quoted=false))"     
    
    # Return the result of the JaQL query
    if (length(.bigr.executeJaqlQuery(jaqlExpr, limit=FALSE)) > 0) {
        return(TRUE)
    } else {
        return(FALSE)
    }
}

# Write a bigr.frame to HDFS
.bigr.writeToBigSQL <- function(bf) {
    logSource = "writeBigSQL"
    if (is.null(bf)) {
        bigr.err(logSource, "Cannot read from a NULL bigr.frame")
        stop()
    }
    bigr.err(logSource, "Writing to BigSQL is not supported")
    return(NULL)
}

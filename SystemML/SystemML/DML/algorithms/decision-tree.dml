#-------------------------------------------------------------
# IBM Confidential
# OCO Source Materials
# (C) Copyright IBM Corp. 2010, 2015
# The source code for this program is not published or
# otherwise divested of its trade secrets, irrespective of
# what has been deposited with the U.S. Copyright Office.
#-------------------------------------------------------------

# Implements classification trees (with scale/continuous features)
#
# Example Usage:
#
# hadoop jar SystemML.jar -f decision-tree.dml -nvargs X=$INPUT_DIR/X Y=$INPUT_DIR/Y types=$INPUT_DIR/types model=$OUPUT_DIR/model bins=50 depth=10 num_leaf=1 num_samples=10 Log=$OUTPUT_DIR/Log fmt="csv"
#
# Args:
# X, Y: Input features and labels, respectively
# types: a matrix with 1 column denoting whether columns in X are continuous (id=1) or categorical (id=2)
# bins: Generates this many number of equiheight bins per scale feature to choose thresholds
# depth: Defines max depth of the learnt tree
# num_leaf: Defines the number of samples when splitting stops and a leaf node is added
# num_samples: Defines the number of samples at which point we switch to in-memory subtree building
# model: HDFS location where the learnt tree is stored
# Log: HDFS location where the log messages are stored
# fmt: Preferred format of the model file (default is ijv/"text")
#

order = externalFunction(Matrix[Double] A, Integer col, Boolean desc) return (Matrix[Double] B) 
	implemented in (classname="com.ibm.bi.dml.udf.lib.OrderWrapper",exectype="mem")
	
binning = externalFunction(Matrix[Double] A, Integer binsize, Integer numbins) return (Matrix[Double] B, Integer numbinsdef) 
	implemented in (classname="com.ibm.bi.dml.udf.lib.BinningWrapper",exectype="mem")
  
/*
 * Given a 1-column bit-vector returns the corresponding
 * 1-column vector that contains the values corresponding
 * to 1's in the input arg
 */	
gather = function( Matrix[Double] B ) return (Matrix[Double] V) {
	# bitvector into position vector, e.g., 1011 -> 1034
    Bv = seq(1,nrow(B),1) * B

	# gather positions into condensed vector, e.g., 1034 -> 134
    V = removeEmpty(target=Bv, margin="rows")     
}

/*
 * Function to generate (row selecting) permutation matrix
 * Given a list of row ids, the matrix returned by the following 
 * function when pre-multiplied to the data matrix will return 
 * only the rows whose row ids were provided, in order of
 * the ids' appearance
 */
expand = function( Matrix[Double] V ) return (Matrix[Double] M) {
    M = table( seq(1, nrow(V), 1), V );
}

/*
 * Computes label for classification trees
 * Determines the mode of y
 */ 		    
findLabelFromVector = function(Matrix[Double] y, Integer num_classes) return (Double label){
	aggr_labels = t(aggregate(target=y, groups=y, fn="count", ngroups=num_classes))
	label = findLabel(aggr_labels)
}

/*
 * Accepts a row vector of label counts 
 * and returns the label corresponding
 * to the maximum count
 */
findLabel = function(Matrix[Double] aggr_labels) return (Double label){
	label = as.scalar(rowIndexMax(aggr_labels))
}

/*
 * Returns the indicator col determining the left samples
 * defined by splitting on this (scale) feature col with this threshold
 */
computeLeftForScale = function(Matrix[Double] feature_vals, Double threshold) return(Matrix[Double] left){
      left = ppred(feature_vals, threshold, "<")
}

/*
 * Returns the indicator col determining the left samples
 * defined by splitting on this (categorical) feature col with
 * this subset of vals (provided as input in bit vector representation)
 */
computeLeftForCategorical = function(Matrix[Double] feature_vals, Matrix[Double] vals) return(Matrix[Double] left){	  
	  P = table(seq(1, nrow(feature_vals)), feature_vals, nrow(feature_vals), nrow(vals))
	  left = P %*% vals
}

/*
 * Takes counts of labels and returns (Gini) impurity
 */
computeImpurity = function(Matrix[Double] label_counts) 
		  return (Double impurity)
{
  tot_count = sum(label_counts)  
  tot_count = max(tot_count, 1.0)
  label_proportions = label_counts/tot_count
  impurity = 1 - sum(label_proportions^2)
}

vComputeImpurity = function(Matrix[Double] label_counts) 
		  return (Matrix[Double] impurity)
{
  tot_count = rowSums(label_counts)
  tot_count = max(tot_count, 1.0)
  label_proportions = label_counts/tot_count
  impurity = 1 - rowSums(label_proportions^2)
}

/*
 * Takes label counts arriving at the parent node
 * and label counts going to the left branch
 * to compute the gain (calls computeImpurity)
 */
computeGainFromLabelCounts = function(Matrix[Double] label_counts_all,
	      	       	              Matrix[Double] label_counts_left)
	      		     return (Double gain){
  tot_count_all = sum(label_counts_all)
  impurity_all = computeImpurity(label_counts_all)

  tot_count_left = sum(label_counts_left)
  impurity_left = computeImpurity(label_counts_left)
  
  label_counts_right = label_counts_all - label_counts_left
  tot_count_right = sum(label_counts_right)
  impurity_right = computeImpurity(label_counts_right)

  gain = computeGainFromImpurities(impurity_all, 
         	     		   impurity_left,
	 	     		   impurity_right,
		     		   tot_count_all,
		     		   tot_count_left,
		     		   tot_count_right)
}	       

/*
 * Takes impurities and counts at parent, left and right child nodes
 * Computes information gain
 */ 
computeGainFromImpurities = function(Double impurity_parent, 
	      	       	    	     Double impurity_left, 
		       		     Double impurity_right, 
		       		     Integer count_parent, 
		       		     Integer count_left, 
		       		     Integer count_right)
	      		    return (Double gain){
	gain = count_parent*impurity_parent - count_left*impurity_left - count_right*impurity_right			
}

/*
 * Determines the best threshold to split
 * Take the col of feature values, the labels and
 * the label counts arriving at the parent as input args
 * Returns the best threshold, the gain when splitting at
 * that threshold, and if the split at that threshold 
 * produces a left/right child that should be a label node
 */
scaleSplit = function(Matrix[Double] feature_vals,
	              	  Matrix[Double] y,
		      		  Matrix[Double] label_counts_at_parent,
		      		  Integer num_classes)
	     	 return(Double best_threshold_for_feature,
	     	    	Double best_gain_for_feature,
		    		Integer is_left_child_pure_4_best,
		    		Integer is_right_child_pure_4_best,
		    		Double make_leaf){
      sorted_feature_vals = order(feature_vals, 1, FALSE)
		
      /*
       * print("feature=" + i + " sorted feature values:")
       * for(j in 1:nrow(sorted_feature_vals))
       *	print(" " + j + " -> " + castAsScalar(sorted_feature_vals[j,1]))
       */
		 
      best_threshold_pos_for_feature = 0
      best_gain_for_feature = 0.0
      is_left_child_pure_4_best = 0
      is_right_child_pure_4_best = 0
      make_leaf = 0.0
      for(j in 1:(nrow(sorted_feature_vals)-1)){
	      threshold = castAsScalar((sorted_feature_vals[j,1] + sorted_feature_vals[j+1,1])/2)
			
	      left = computeLeftForScale(feature_vals, threshold)
	      yl = removeEmpty(target=y*left, margin="rows")
	      
	      if(sum(yl) > 0)
                label_counts_left = aggregate(target=yl, groups=yl, fn="count", ngroups=num_classes)
          else label_counts_left = matrix(0, rows=num_classes, cols=1)
          
          gain = computeGainFromLabelCounts(label_counts_at_parent, label_counts_left)
		
	      plc = as.integer(sum(label_counts_left) == max(label_counts_left))
	      
	      prc = as.integer(sum(label_counts_at_parent - label_counts_left) == max(label_counts_at_parent - label_counts_left))

	      #print("feature=" + i + " threshold=" + threshold + " gain=" + gain + " nl=" + nl + " nr=" + nr)
			
	      if(best_threshold_pos_for_feature == 0 | gain > best_gain_for_feature){
		      best_gain_for_feature = gain
		      best_threshold_pos_for_feature = j
		      is_left_child_pure_4_best = plc
		      is_right_child_pure_4_best = prc
		      
		      label_counts_right = label_counts_at_parent - label_counts_left
		      nl = sum(yl)
              nr = sum(label_counts_right)
              if(nl == 0 | nr == 0){
                    aggr_labels =
                    	as.integer(nl == 0) * t(label_counts_right)
                    	+ as.integer(nr == 0) * t(label_counts_left)
                    	
                    make_leaf = findLabel(aggr_labels) 
              }else make_leaf = 0.0
	      }
      }
		
      best_threshold_for_feature = castAsScalar((sorted_feature_vals[best_threshold_pos_for_feature,1] + sorted_feature_vals[best_threshold_pos_for_feature + 1, 1])/2)
}

/*
 * Determines the best subset of values from the domain
 * of the feature to split on so we get the highest gain
 * Takes as arg a domain X num_classes matrix of label counts
 * Returns the bit vector corresponding to the best subset of 
 * values, the gain at that split, and whether the split
 * leads to left/right child that should be label nodes
 */
categoricalSplit = function(Matrix[Double] label_dist) 
				   return (Matrix[Double] ret, 
				   		   Double curr_gain, 
				   		   Integer pure_left_child, 
				   		   Integer pure_right_child,
				   		   Double make_leaf)
{
  num_classes = ncol(label_dist)
	domain_size = nrow(label_dist)
	
	label_counts_all = colSums(label_dist)

	ret = matrix(0, rows=domain_size, cols=1)

	curr_gain = 0.0
	vals_included = 0

	curr_label_counts_left = matrix(0, rows=1, cols=num_classes)

  continue = TRUE;
	while( continue ) {
     I = ppred(ret[1:domain_size,1],0,"==");
     label_counts_left = label_dist + curr_label_counts_left
     tot_count_all = sum(label_counts_all); 
     impurity_all = computeImpurity(label_counts_all) 
     tot_count_left = rowSums(label_counts_left)
     impurity_left = vComputeImpurity(label_counts_left)
     label_counts_right = -label_counts_left + label_counts_all
     tot_count_right = rowSums(label_counts_right)
     impurity_right = vComputeImpurity(label_counts_right)  
     gain = tot_count_all*impurity_all - tot_count_left*impurity_left - tot_count_right*impurity_right

     G = I * gain;  
     best_gain = max(G);
     best_val_to_include = as.integer(as.scalar(rowIndexMax(t(G))));
	
	   if(sum(label_dist[best_val_to_include,]) > 0 & as.scalar(ret[best_val_to_include,1]) == 0 & best_gain >= curr_gain){
	      curr_gain = best_gain
	      curr_label_counts_left = curr_label_counts_left + label_dist[best_val_to_include,]
	      ret[best_val_to_include,1] = 1
	      vals_included = vals_included + 1
	   }
    else 
        continue = FALSE;
	}

	pure_left_child = as.integer(sum(curr_label_counts_left) == max(curr_label_counts_left))
	pure_right_child = as.integer(sum(label_counts_all - curr_label_counts_left) == max(label_counts_all - curr_label_counts_left))
	
  make_leaf = 0.0
  if(sum(curr_label_counts_left) == 0 | sum(label_counts_all - curr_label_counts_left) == 0){
     aggr_labels = colSums(label_dist)
     make_leaf = findLabel(aggr_labels)
  }
}

/*
 * Finds a the best split for a node
 * Meant for use when a 'small' number of samples reach this node
 */
findSplit = function(Matrix[Double] X, 
	    	     	 Matrix[Double] y, 
		     		 Integer num_classes, 
		     		 Matrix[Double] kinds, 
		     		 Matrix[Double] domain_sizes) 
	    	return (Integer feature, 
	    	    	Double threshold, 
		    		Matrix[Double] left_vals,
		    		Matrix[Double] left_rows,
		    		Double make_leaf){
	n = nrow(X)

	label_counts_all = aggregate(target=y, groups=y, fn="count", ngroups=num_classes)
	
	best_gain = 0.0
	best_feature = -1
	best_threshold = 0.0
	best_left_vals = matrix(0, rows=1, cols=1)
	best_make_leaf = 0.0
    for(i in 1:ncol(X)){
		feature_vals = X[,i]

		if(as.scalar(kinds[i,1]) == 1){
		      [best_threshold_for_feature, best_gain_for_feature, plc, prc, make_leaf] = scaleSplit(feature_vals, y, label_counts_all, num_classes)

		      #print("feature=" + i + " best_threshold=" + best_threshold_for_feature + " best_gain=" + best_gain_for_feature)

		      if(best_feature == -1 | best_gain_for_feature > best_gain){
		            best_gain = best_gain_for_feature
			    	best_feature = i
			    	best_threshold = best_threshold_for_feature
			    	best_make_leaf = make_leaf
		      }
		}else{
		      label_dist = table(feature_vals, y, as.scalar(domain_sizes[1,i]), num_classes)
		      [left_vals_bit_vector, best_gain_for_feature, plc, prc, make_leaf] = categoricalSplit(label_dist)
		      
		      if(best_feature == -1 | best_gain_for_feature > best_gain){
		            best_gain = best_gain_for_feature
			        best_feature = i
			    	best_left_vals = left_vals_bit_vector
			     	best_make_leaf = make_leaf
		      }
		}

	}
	
	make_leaf = best_make_leaf

	feature = best_feature
	threshold = 0.0
	left_vals = matrix(0, rows=1, cols=1)
	if(as.scalar(kinds[feature,1]) == 1){
	      threshold = best_threshold
	      left_rows = computeLeftForScale(X[,feature], threshold)
	}else{
	      left_vals = best_left_vals
	      left_rows = computeLeftForCategorical(X[,feature], best_left_vals)
	}

	#print("best_feature=" + feature + " best_threshold=" + threshold + " best_gain=" + best_gain)
}

/*
 * Builds a small subtree when computation has reached
 * lower levels of the decision tree
 * Only meant for nodes that have a small number of samples
 * reaching it
 * Is a recursive function, but meant to run in memory
 */
buildNode = function(Matrix[Double] X, 
		     Matrix[Double] y, 
		     Matrix[Double] model,
		     Integer pos, 
		     Integer node_id, 
		     Integer max_node_id, 
		     Integer leaf_count,
		     Integer num_classes,
		     Matrix[Double] kinds,
		     Matrix[Double] domain_sizes) 
		    return (Matrix[Double] model){
	if(nrow(X) > leaf_count & 2*node_id <= max_node_id & 2*node_id + 1 <= max_node_id & min(y) != max(y)){
		[feature, threshold, left_vals_bit_vector_representation, left_rows, make_leaf] = findSplit(X, y, num_classes, kinds, domain_sizes)

		if(make_leaf > 0){
              model[1,pos] = -1
              model[2,pos] = make_leaf
        }else{
			  model[1, pos] = feature
			  model[2, pos] = kinds[feature,1]

		      if(as.scalar(kinds[feature,1]) == 1) model[3, pos] = threshold
			  else{
		            domain_size = as.scalar(domain_sizes[1,feature])

	      	  	    left_vals_size = sum(left_vals_bit_vector_representation)
	      	  	    model[3, pos] = left_vals_size

		      	    left_vals = gather(left_vals_bit_vector_representation)
                    model[4:(3+nrow(left_vals)), pos] = left_vals
		      }	

			  /*
		 	   * print("X:")
	 	 	   * for(i in 1:nrow(X))
	 	       *	for(j in 1:ncol(X))
	 	       *		print(" " + i + " " + j + " -> " + castAsScalar(X[i,j]))
		       */
		 
		      /*
		       * print("y:")
	 	       * for(i in 1:nrow(y))
	 	       *	for(j in 1:ncol(y))
	 	       * 		print(" " + i + " " + j + " -> " + castAsScalar(y[i,j]))
		       */
		 
			  num_left = sum(left_rows)
			  num_right = nrow(X) - num_left
		
		      permut_mat = matrix(0, rows=nrow(X), cols=nrow(X))
		      
		      permut_mat_4_left_rows = gather(left_rows)
		      permut_mat_4_left_rows = expand(permut_mat_4_left_rows)
		      permut_mat[1:num_left, 1:ncol(permut_mat_4_left_rows)] = permut_mat_4_left_rows
		      
		      right_rows = 1 - left_rows
		      permut_mat_4_right_rows = gather(right_rows)
		      permut_mat_4_right_rows = expand(permut_mat_4_right_rows)
		      permut_mat[(num_left+1):(num_left+nrow(permut_mat_4_right_rows)), 1:ncol(permut_mat_4_right_rows)] = permut_mat_4_right_rows
		      
			  reordered_X = permut_mat %*% X
			  reordered_y = permut_mat %*% y
		
			  left_X = reordered_X[1:num_left,]
			  left_y = reordered_y[1:num_left,]
		
			  /*
		 	   * print("left_X:")
	 	 	   * for(i in 1:nrow(left_X))
	 	 	   * 	for(j in 1:ncol(left_X))
	 	 	   *		print(" " + i + " " + j + " -> " + castAsScalar(left_X[i,j]))
	 	 	   */
	 	 
	 		  /* 		
	 	 	   * print("left_y:")
	 	 	   * for(i in 1:nrow(left_y))
	 	 	   * 	for(j in 1:ncol(left_y))
	 	 	   * 		print(" " + i + " " + j + " -> " + castAsScalar(left_y[i,j]))
	 	 	   */ 
	 	 
		      right_X = reordered_X[(num_left+1):nrow(X),]
			  right_y = reordered_y[(num_left+1):nrow(y),]
		
		      /*
		 	   * print("right_X:")
	 	 	   * for(i in 1:nrow(right_X))
	 	 	   *	for(j in 1:ncol(right_X))
	 	 	   *		print(" " + i + " " + j + " -> " + castAsScalar(right_X[i,j]))
	 	 	   */
	 	 
	 		  /* 		
	 	 	   * print("right_y:")
	 	 	   * for(i in 1:nrow(right_y))
	 	 	   * for(j in 1:ncol(right_y))
	 	 	   * 		print(" " + i + " " + j + " -> " + castAsScalar(right_y[i,j]))
	 	 	   */ 

			  model = buildNode(left_X, left_y, model, 2*pos, 2*node_id, max_node_id, leaf_count, num_classes, kinds, domain_sizes)
			  model = buildNode(right_X, right_y, model, 2*pos + 1, 2*node_id + 1, max_node_id, leaf_count, num_classes, kinds, domain_sizes)
		  }
	}else{
		label = findLabelFromVector(y, num_classes)
				
		model[1,pos] = -1
		model[2,pos] = label
	} 
}

/*
 * Merges a subtree into the master model
 * Is a recursive function, but meant to run in memory
 */
recursiveCopy = function(Matrix[Double] local_model, 
	      		 		 Integer local_pos, 
			 		     Matrix[Double] global_model, 
			 			 Integer global_pos)
		return (Matrix[Double] global_model){
	feature = as.scalar(local_model[1, local_pos])
	global_model[, global_pos] = local_model[, local_pos]

	if(feature > 0){
		global_model = recursiveCopy(local_model, 2*local_pos, global_model, 2*global_pos)
		global_model = recursiveCopy(local_model, 2*local_pos + 1, global_model, 2*global_pos + 1)
	}
	
	/*
	 * print("(inside function) global_model (" + global_pos + "):")
	 *	for(i in 1:nrow(global_model))
	 *		for(j in 1:ncol(global_model))
	 *	 		print(" " + i + " " + j + " -> " + castAsScalar(global_model[i,j]))
	 */
}

/*
 * MAIN BEGINS HERE
 */

cmdLine_fmt = ifdef($fmt, "text")
cmdLine_bins = ifdef($bins, 50)
cmdLine_depth = ifdef($depth, 10)
cmdLine_num_leaf = ifdef($num_leaf, 1)
cmdLine_num_samples = ifdef($num_samples, 10)

X = read($X)

numRows = nrow(X)
if(numRows < 2)
	stop("Stopping due to invalid inputs: Not possible to learn a classifier without at least 2 rows")

y = read($Y) #y must be recoded
kinds = read($types) #put 2 for categorical, 1 for scale (like bivar-stats)

num_bins = cmdLine_bins
if(num_bins <= 1)
	stop("Stopping due to invalid argument: bins must be greater than 1")

bin_size = max(as.integer(nrow(X)/num_bins),1);

/*
 * Generating useful thresholds for each continuous feature
 * count_thresholds[1,feature_id] = the number of thresholds 
 * computed for feature_id
 * thresholds[i, feature_id] = the ith threshold for feature_id
 * note that, only i = 1 ... count_thresholds[1,feature_id]
 * are valid
 */
count_thresholds = matrix(0, rows=1, cols=ncol(X))
thresholds = matrix(0, rows=num_bins+1, cols=ncol(X))
parfor(i8 in 1:ncol(X)) {
   if(as.scalar(kinds[i8,1]) == 1){
      [col] = order(X[,i8], 1, FALSE);
      [col_bins, num_bins_defined] = binning(col, bin_size, num_bins);
      count_thresholds[,i8] = num_bins_defined;
      thresholds[,i8] = col_bins;	
   }
}

#compute max domain size for categorical
maxs = colMaxs(X);
max_domain_size = max( ppred(kinds, 2, "==") * t(maxs) );

# compute begin/end domain offsets via prefix sums 
cum_maxs = cumsum( t(append(as.matrix(1),maxs*t(ppred(kinds,2,"==")))) );
domain_offsets = append(cum_maxs[1:ncol(X),1], cum_maxs[2:(ncol(X)+1),1]-1);

min_y = min(y)
if(min_y < 1)
	stop("Stopping due to invalid argument: Label vector (Y) must be recoded")
num_classes = as.integer(max(y))
if(num_classes == 1)
	stop("Stopping due to invalid argument: Maximum label value is 1, need more than one class to learn a multi-class classifier")	
mod1 = y %% 1
mod1_should_be_nrow = sum(abs(ppred(mod1, 0, "==")))
if(mod1_should_be_nrow != nrow(y))
	stop("Stopping due to invalid argument: Please ensure that Y contains (positive) integral labels")

#exposed to user
depth = cmdLine_depth - 1 #user counts depth from 1, but we count from 0
if(depth <= 0)
	stop("Stopping due to invalid argument: depth must be positive")

leaf_count = cmdLine_num_leaf
if(leaf_count < 1)
	stop("Stopping due to invalid argument: num_leaf must be at least 1")

#internal
small_sample_count = cmdLine_num_samples
#small_sample_count = 2 #set this to >1
if(small_sample_count < 0)
	stop("Stopping due to invalid argument: num_samples must be non-negative")
	
max_node_id = as.integer(2^(depth + 1) - 1)

#first row contains feature id
#second row contains threshold
model = matrix(0, rows=(3+max_domain_size), cols=max_node_id)

q = matrix(0, rows=max_node_id, cols=1)
leaf_q = matrix(0, rows=max_node_id, cols=1)
if(2 <= max_node_id & 3 <= max_node_id & nrow(X) > leaf_count)
	q[1,1] = 1
else leaf_q[1,1] = 1

while(sum(q) != 0 | sum(leaf_q) != 0){
	/*
	 * str = "q:"
	 * for(i in 1:nrow(q))
	 *	if(castAsScalar(q[i,1]) > 0)
	 *		str = append(str, " " + castAsScalar(q[i,1]))
	 * print(str)
	 */

	/*
	 * str = "leaf_q:"
	 * for(i in 1:nrow(leaf_q))
	 *	if(castAsScalar(leaf_q[i,1]) > 0)
	 *		str = append(str, " " + castAsScalar(leaf_q[i,1]))
	 * print(str)
	 */

	/*
	 * Maps nodes in q, leaf_q to which cols store 
	 * the corresponding 0/1 reached vector in reached
	 * position[node,1] is reached col for node
	 */
	position = matrix(0, rows=nrow(q), cols=1)
	num_q = 1
	for(i3 in 1:nrow(q)){
		#print(" in q: " + castAsScalar(q[i3,1]))
		if(castAsScalar(q[i3,1]) > 0){
			position[castAsScalar(q[i3,1]), 1] = i3
			num_q = num_q + 1
		}
	}
	num_q = num_q - 1
	cnt = num_q	
		
	for(i in 1:nrow(leaf_q)){
		#print(" in leaf_q: " + castAsScalar(leaf_q[i,1]))
		if(castAsScalar(leaf_q[i,1]) > 0){
			position[castAsScalar(leaf_q[i,1]),1] = num_q + i
			cnt = cnt + 1
		}
	}
	
	/*
	 * print("position:")
	 * for(i in 1:nrow(position))
	 *   if(castAsScalar(position[i,1]) > 0)
	 *		print(" " + i + " -> " + castAsScalar(position[i,1]))
	 */

	/* 
	 * Maps which samples reach which node
	 * reached[i,j] = 1 if sample i reaches the jth node in q
	 * else reached[i,j] = 0
	 */
	reached = matrix(0, rows=nrow(X), cols=cnt)
	parfor(i4 in 1:nrow(X)){
		row = X[i4,]
		curr_node = 1
		while(as.scalar(model[1,curr_node]) > 0){
			#print("curr_node=" + curr_node)
			curr_node_feature = as.scalar(model[1,curr_node])
			curr_node_kind = as.scalar(model[2,curr_node])
			row_value_for_curr_node_feature = as.scalar(row[1,curr_node_feature])
			
			if(curr_node_kind == 1)
				r_branch = 1.0 - as.integer(row_value_for_curr_node_feature < as.scalar(model[3,curr_node]))
			else
				r_branch = 1.0 - sum(ppred(model[4:nrow(model),curr_node], row_value_for_curr_node_feature, "=="))
			
			curr_node = 2*curr_node + r_branch
		}
		
		#print("row " + i4 + " reached node " + curr_node)
		if(as.scalar(position[curr_node,1]) != 0)
			reached[i4,as.scalar(position[curr_node,1])] = 1
	}

	/*
	 * str="reached:"
	 * for(i in 1:nrow(reached))
	 *	for(j in 1:ncol(reached))
	 *		str = append(str, i + " " + j + " -> " + castAsScalar(reached[i,j]))
	 * print(str) 
	 */
	 	 
	parfor(i7 in 1:nrow(leaf_q), check=0){
	#for(i7 in 1:nrow(leaf_q)){
		node = castAsScalar(leaf_q[i7,1])
			
		#print("generating leaf node=" + node 
		#	 + " (" + castAsScalar(position[node,1]) + ")"
		#	 )
			
		if(node > 0){
			reach = reached[,castAsScalar(position[node,1])]
			cnt_reach = sum(reach)
			y_reaching_this_node = removeEmpty(target=reach*y, margin="rows")
			mode_y = findLabelFromVector(y_reaching_this_node, num_classes)
			
			#print("for node=" + node + " cnt=" + cnt_reach + " mode_y=" + mode_y)
			
			label = 0.0
			if(cnt_reach > 0) 
				label = mode_y
			
			# add leaf node
			model[1,node] = -1
			model[2,node] = label
		}
	}
	 
	/*
	 * Separate q into two sets: big_q and small_q
	 * Sub-trees for small_q with smaller nodes can be learnt using 1 parfor
	 * Nodes in big_q need more work
	 */
	num_reached = colSums(reached)
	
	/*
	 * print("reached:")
	 * for(i in 1:ncol(num_reached))
	 *	print(" " + castAsScalar(num_reached[1,i]) + " samples reached at node=" + castAsScalar(q[i,1]))
	 */
	 
	num_big = 0.0
	num_small = 0.0
	if(num_q > 0){
		is_large_node = ppred(num_reached[,1:num_q], small_sample_count, ">")
		num_big = sum(is_large_node)
		num_small = num_q - num_big
		#print("num_big=" + num_big + " num_small=" + num_small)
	}
	curr_big = 1
	
	/*
	 * use parfors to compute sufficient statistics
	 * to split multiple nodes
	 */
	if(num_big > 0){
		big_q = matrix(0, rows=num_big, cols=1)
		for(i in 1:nrow(q))
			if(castAsScalar(q[i,1]) > 0)
				if(castAsScalar(num_reached[1,i]) > small_sample_count){
				        node_id = q[i,1]
					big_q[curr_big, 1] =  node_id
					curr_big = curr_big + 1
				}
	
		#print big_q
		/*
		 * str = "big_q:"
		 * for(i in 1:nrow(big_q))
		 *	str = append(str, " " + castAsScalar(big_q[i,1]))
		 * print(str)
		 */

		count_all = matrix(0, rows=num_big, cols=num_classes)
		parfor(i9 in 1:num_big){
		      big_node_id = castAsScalar(big_q[i9,1])
		      position_of_big_node = castAsScalar(position[big_node_id,1])
		      reach_col = reached[,position_of_big_node]
		
		      count_reached = sum(reach_col)
		      y_reach_dot = removeEmpty(target=y*reach_col, margin="rows")
		      
		      count_all_vector = t(aggregate(target=y_reach_dot, groups=y_reach_dot, fn="count", ngroups=num_classes))

		      count_all[i9,] = count_all_vector
		}
		
		gain_matrix = matrix(0, rows=num_big, cols=ncol(X))
		threshold_matrix = matrix(0, rows=num_big, cols=ncol(X))
		left_vals_matrix = matrix(0, rows=num_big, cols=max(domain_offsets))
		pure_left_child = matrix(0, rows=num_big, cols=ncol(X))
		pure_right_child = matrix(0, rows=num_big, cols=ncol(X))
		make_leaf = matrix(0, rows=num_big, cols=ncol(X))

		parfor(i10 in 1:ncol(X), check=0){
			X_col = X[,i10]
			kind_1 = as.scalar(kinds[i10,1])

			if(kind_1 == 1){
				#parfor(i10_node in 1:num_big, check=0){
				for(i10_node in 1:num_big){
				      big_node_ID = castAsScalar(big_q[i10_node,1])
				      position_of_big_node_ID = castAsScalar(position[big_node_ID,1])
				      reach_col_big = reached[,position_of_big_node_ID]
				      all_counts = count_all[i10_node,]
				
				      best_gain_for_this_feature = -1.0
				      best_threshold_for_this_feature = 0.0
				      pure_left_child_for_scale = 0
				      pure_right_child_for_scale = 0
				      make_leaf_for_scale = 0.0
				      for(i12 in 1:castAsScalar(count_thresholds[1,i10])){
				      	    threshold = as.scalar(thresholds[i12,i10])
					    	left_X_col = computeLeftForScale(X_col, threshold)
				      	    y_reach_dot_prod = removeEmpty(target=y*reach_col_big*left_X_col, margin="rows")
					    
					    	if(sum(y_reach_dot_prod) > 0)
					        	  left_counts = t(aggregate(target=y_reach_dot_prod, groups=y_reach_dot_prod, fn="count", ngroups=num_classes))
					    	else left_counts = matrix(0, rows=1, cols=num_classes)

				        	gain = computeGainFromLabelCounts(all_counts, left_counts)
					    
					    	if(best_gain_for_this_feature == -1 | gain > best_gain_for_this_feature){
					          	  best_gain_for_this_feature = gain
						  		  best_threshold_for_this_feature = threshold
						  
						  		  right_counts = all_counts - left_counts	
						  
						  		  nl = sum(left_counts)
                                  nr = sum(right_counts)
                                                  
						  		  if(nl == 0 | nr == 0){
						  		  		aggr_labels = 
						  		  			as.integer(nl == 0) * right_counts
						  		  			+ as.integer(nr == 0) * left_counts
						  		  
                                  	    make_leaf_for_scale = findLabel(aggr_labels)
                                        
                                  }else make_leaf_for_scale = 0.0
						  
						  		  pure_left_child_for_scale = as.integer(sum(left_counts) == max(left_counts))
							
						  		  pure_right_child_for_scale = as.integer(sum(all_counts - left_counts) == max(all_counts - left_counts))
					    	}
				      }
				      
				      gain_matrix[i10_node, i10] = best_gain_for_this_feature
				      threshold_matrix[i10_node, i10] = best_threshold_for_this_feature
				      pure_left_child[i10_node, i10] = pure_left_child_for_scale
				      pure_right_child[i10_node, i10] = pure_right_child_for_scale
				      make_leaf[i10_node, i10] = make_leaf_for_scale
				}
			}else{
				for(i10_node in 1:num_big){
				      big_node_ID = castAsScalar(big_q[i10_node,1])
				      position_of_big_node_ID = castAsScalar(position[big_node_ID,1])
				      reach_col_big = reached[,position_of_big_node_ID]

				      y_reach_dot_prod = removeEmpty(target=append(X_col*reach_col_big, y*reach_col_big), margin="rows")

				      label_dist = table(y_reach_dot_prod[,1], y_reach_dot_prod[,2], as.scalar(maxs[1,i10]), num_classes)

				      [left_vals_bit_vector, best_gain_categorical, plc, prc, make_leaf_for_cat] = categoricalSplit(label_dist)

				      gain_matrix[i10_node,i10] = best_gain_categorical
				      
				      feature_beg_pos = as.scalar(domain_offsets[i10,1])
				      #feature_end_pos = as.scalar(domain_offsets[i10,2])
				      left_vals_matrix[i10_node,feature_beg_pos:(feature_beg_pos + nrow(left_vals_bit_vector) - 1)] = t(left_vals_bit_vector)

				      pure_left_child[i10_node, i10] = plc
				      pure_right_child[i10_node, i10] = prc
				      make_leaf[i10_node, i10] = make_leaf_for_cat
				}
			}
		}

		best_gains = rowIndexMax(gain_matrix)
			
		/*	
		 * print("best_gains:")
		 * for(i in 1:nrow(best_gains))
		 *	print(" node=" + castAsScalar(big_q[i,1]) + " -> " + castAsScalar(best_gains[i,1]))		
		 */
		 	
		for(i in 1:num_big){
			big_node = castAsScalar(big_q[i,1])
			
			best_gain_feature = as.scalar(best_gains[i,1])
			
			ml = as.scalar(make_leaf[i,best_gain_feature])
			
			if(ml == 0){
				  model[1,big_node] = best_gain_feature

				  kind = as.scalar(kinds[best_gain_feature, 1])
				  model[2,big_node] = kind

				  if(kind == 1)
      			        model[3,big_node] = threshold_matrix[i,best_gain_feature]
				  else{
   			      		beg_for_best_feature = as.scalar(domain_offsets[best_gain_feature, 1])
			      		end_for_best_feature = as.scalar(domain_offsets[best_gain_feature, 2])

			      		left_vals_bit_vector_representation = t(left_vals_matrix[i,beg_for_best_feature:end_for_best_feature])

			      		left_vals_size = sum(left_vals_bit_vector_representation)
			      		model[3,big_node] = left_vals_size
			      
			      		left_vals = gather(left_vals_bit_vector_representation)
			      		model[4:(3+nrow(left_vals)), big_node] = left_vals
				  }
			}else{
            	  model[1,big_node] = -1
                  model[2,big_node] = ml
            }
      }
	}
		
	/*
	 * one parfor to learn lots of small sub-trees
	 */
	if(num_small > 0){
		small_q = matrix(0, rows=num_small, cols=1)
		small_q_cnts = matrix(0, rows=num_small, cols=1)
		
		curr_small = 1
		for(i in 1:nrow(q))
			if(castAsScalar(q[i,1]) > 0)
				if(castAsScalar(num_reached[1,i]) <= small_sample_count){
					small_q[curr_small, 1] = q[i,1]
					small_q_cnts[curr_small, 1] = num_reached[1,i]
					curr_small = curr_small + 1
				}
			
		cumulative_small_q_cnts = matrix(0, rows=num_small, cols=1)
		cumulative_small_q_cnts[1,1] = 0
		for(i in 2:nrow(small_q_cnts))
			cumulative_small_q_cnts[i,1] = cumulative_small_q_cnts[i-1,1] + small_q_cnts[i-1,1]	
			
		#print small_q	
		/*
		 * str = "small_q:"
		 * for(i in 1:nrow(small_q))
		 *	str = append(str," "+castAsScalar(small_q[i,1])+"("+castAsScalar(small_q_cnts[i,1])+")("+castAsScalar(cumulative_small_q_cnts[i,1])+")")
		 * print(str)
		 */
		
		select_mat = matrix(0, rows=ncol(reached), cols=num_small)
		for(i in 1:nrow(small_q))
			select_mat[castAsScalar(position[castAsScalar(small_q[i,1]),1]),i] = 1
		
		small_reached = reached %*% select_mat
		
		/*
		 * print("small_reached:")
		 * for(i in 1:nrow(small_reached))
		 *	for(j in 1:ncol(small_reached))
		 *		print(" " + i + " " + j + " -> " + castAsScalar(small_reached[i,j]))
		 */
		 
		permut_mat = matrix(0, rows=sum(small_q_cnts), cols=nrow(X))
		parfor(i5 in 1:ncol(small_reached), check=0){
			offset = castAsScalar(cumulative_small_q_cnts[i5,1])
			
			small_reached_col = small_reached[,i5]
			row_ids = gather(small_reached_col)
			permut_block = expand(row_ids)
			permut_mat[(offset+1):(offset+nrow(permut_block)), 1:ncol(permut_block)] = permut_block
		}
		
		/*
		 * print("permut_mat:")
		 * for(i in 1:nrow(permut_mat))
		 *	for(j in 1:ncol(permut_mat))
		 *		print(" " + i + " " + j + " -> " + castAsScalar(permut_mat[i,j]))
		 */
		
		reordered_X = permut_mat %*% X
		reordered_y = permut_mat %*% y
		
		/*	
		 * print("reordered_X:")
		 * for(i in 1:nrow(reordered_X))
		 *	for(j in 1:ncol(reordered_X))
		 *		print(" " + i + " " + j + " -> " + castAsScalar(reordered_X[i,j]))
		 */
		
		/*
		 * print("reordered_y:")
		 * for(i in 1:nrow(reordered_y))
		 *	for(j in 1:ncol(reordered_y))
		 *		print(" " + i + " " + j + " -> " + castAsScalar(reordered_y[i,j])) 
		 */
		 
		local_models = matrix(0, rows=nrow(small_q), cols=(nrow(model)*ncol(model)))
		parfor(i6 in 1:nrow(small_q)){
			beg = 1 + castAsScalar(cumulative_small_q_cnts[i6,1])
			end = beg + castAsScalar(small_q_cnts[i6,1]) - 1
			
			#print("beg=" + beg + " end=" + end)
			
			if(end - beg > -1){
				local_X = reordered_X[beg:end,]
				local_y = reordered_y[beg:end,]
		
				node1 = castAsScalar(small_q[i6,1])
			
				/*
			 	 * print("local_X " + node1 + ":")
		 	      	 *  for(i in 1:nrow(local_X))
		 	 	 *	for(j in 1:ncol(local_X))
		 	 	 *		print(" " + i + " " + j + " -> " + castAsScalar(local_X[i,j]))
			 	 */
			
				/*
			 	 * print("local_y " + node1 + ":")
		 	 	 * for(i in 1:nrow(local_y))
		 	 	 *	for(j in 1:ncol(local_y))
		 	 	 *		print(" " + i + " " + j + " -> " + castAsScalar(local_y[i,j]))
		 	 	 */
		 	 	
				/*
			 	 * learn sub-tree from local_X and local_y
			 	 */
				
				local_node = as.integer(as.scalar(small_q[i6,1]))
				
				local_model = matrix(0, rows=nrow(model), cols=ncol(model))
				local_model1 = buildNode(local_X, local_y, local_model, 1, local_node, max_node_id, leaf_count, num_classes, kinds, maxs)
				
				local_models[i6,] = matrix(local_model1, rows=1, cols=nrow(local_model1)*ncol(local_model1))
			}else{
				local_model = matrix(0, rows=nrow(model), cols=ncol(model))
				local_model[1,1] = -1.0

				local_models[i6,] = matrix(local_model, rows=1, cols=nrow(local_model)*ncol(local_model))
			}
		}
		
		for(i in 1:nrow(small_q)){
			local_model = matrix(local_models[i,], rows=nrow(model), cols=ncol(model))

			local_node = 1
			global_node = as.integer(castAsScalar(small_q[i,1]))
			
			model = recursiveCopy(local_model, local_node, model, global_node)
		}
		
		/*
		 * print("(in main) model:")
		 * for(i in 1:nrow(model))
		 *	for(j in 1:ncol(model))
		 *		print(" " + i + " " + j + " -> " + castAsScalar(model[i,j]))
		 */
	}

	new_q = matrix(0, rows=nrow(q), cols=1)
	new_leaf_q = matrix(0, rows=nrow(q), cols=1)
	if(num_big > 0){
		curr = 1
		curr_leaf = 1
		for(i in 1:nrow(big_q)){
			par_node = castAsScalar(big_q[i,1])
			
			if(par_node > 0 & as.scalar(model[1,par_node]) != -1){
				l_child = 2*par_node
				if(2*l_child <= max_node_id & 2*l_child + 1 <= max_node_id & as.scalar(pure_left_child[i,as.scalar(model[1,par_node])]) == 0){
					new_q[curr,1] = l_child
					curr = curr + 1
				}else{
					new_leaf_q[curr_leaf,1] = l_child
					curr_leaf = curr_leaf + 1
				}
				
				r_child = 2*par_node + 1
				if(2*r_child <= max_node_id & 2*r_child + 1 <= max_node_id & as.scalar(pure_right_child[i,as.scalar(model[1,par_node])]) == 0){
					new_q[curr,1] = r_child
					curr = curr + 1
				}else{
					new_leaf_q[curr_leaf,1] = r_child
					curr_leaf = curr_leaf + 1
				}
			}
		}
		
		/*	
		 * str = "new_q:"
		 * for(i in 1:nrow(new_q))
		 *	if(castAsScalar(new_q[i,1]) > 0)
		 *		str = append(str, " " + castAsScalar(new_q[i,1]))
		 * print(str)
		 */
		 
		/*		
		 * str = "new_leaf_q:"
		 * for(i in 1:nrow(new_leaf_q))
		 *	if(castAsScalar(new_leaf_q[i,1]) > 0)
		 *		str = append(str, " " + castAsScalar(new_leaf_q[i,1]))
		 * print(str)
		 */
	}
	
	q = new_q
	leaf_q = new_leaf_q
	
	/*	
	 * print("model:")
	 * for(i in 1:nrow(model))
	 *	for(j in 1:ncol(model))
	 * 		print(" " + i + " " + j + " -> " + castAsScalar(model[i,j]))
	 */
}

write(model, $model, format=cmdLine_fmt)

pred = matrix(0, rows=nrow(X), cols=1)

parfor(j1 in 1:nrow(X)){
	row_j1 = X[j1,]
    curr_node_j1 = 1
    while(as.scalar(model[1,curr_node_j1]) > 0){
		curr_node_j1_feature = as.scalar(model[1,curr_node_j1])
	    curr_node_j1_kind = as.scalar(model[2,curr_node_j1])
	    row_value_for_curr_node_j1_feature = as.scalar(row_j1[1,curr_node_j1_feature])
	    
	    if(curr_node_j1_kind == 1)
	    	right_branch = 1.0 - as.integer(row_value_for_curr_node_j1_feature < as.scalar(model[3,curr_node_j1]))
	    else
	    	right_branch = 1.0 - sum(ppred(model[4:nrow(model),curr_node_j1], row_value_for_curr_node_j1_feature, "=="))
	    	
	    curr_node_j1 = 2*curr_node_j1 + right_branch
	}
		
    pred[j1,1] = model[2,curr_node_j1]
}

num_correct = sum(ppred(pred, y, "=="))
acc = 100*num_correct/nrow(X)

acc_str = "Training Accuracy (%): " + acc
print(acc_str)
write(acc_str, $Log)

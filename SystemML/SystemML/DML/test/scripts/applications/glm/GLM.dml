#-------------------------------------------------------------
# IBM Confidential
# OCO Source Materials
# (C) Copyright IBM Corp. 2010, 2014
# The source code for this program is not published or
# otherwise divested of its trade secrets, irrespective of
# what has been deposited with the U.S. Copyright Office.
#-------------------------------------------------------------

setwd ("DML/test/scripts/applications/glm");
source ("CGSteihaug.dml");

# THIS SCRIPT SOLVES GLM REGRESSION USING NEWTON/FISHER SCORING WITH TRUST REGIONS
#
# INPUT PARAMETERS:
# ---------------------------------------------------------------------------------------------
# NAME  TYPE   DEFAULT  MEANING
# ---------------------------------------------------------------------------------------------
# X     String  ---     Location to read the matrix X of feature vectors
# Y     String  ---     Location to read response matrix Y with either 1 or 2 columns:
#                       if dfam = 2, Y is 1-column Bernoulli or 2-column Binomial (#pos, #neg)
# B     String  ---     Location to store estimated regression parameters (the betas)
# dfam  Int     1       Distribution family code: 1 = Power, 2 = Binomial
# vpow  Double  0.0     Power for Variance defined as (mean)^power (ignored if dfam != 1):
#                       0.0 = Gaussian, 1.0 = Poisson, 2.0 = Gamma, 3.0 = Inverse Gaussian
# link  Int     0       Link function code: 0 = canonical (depends on distribution),
#                       1 = Power, 2 = Logit, 3 = Probit, 4 = Cloglog, 5 = Cauchit
# lpow  Double  1.0     Power for Link function defined as (mean)^power (ignored if link != 1):
#                       -2.0 = 1/mu^2, -1.0 = reciprocal, 0.0 = log, 0.5 = sqrt, 1.0 = identity
# yneg  Double  0.0     Response value for Bernoulli "No" label, usually 0.0 or -1.0
# icpt  Int     0       Intercept presence, X columns shifting and rescaling:
#                       0 = no intercept, no shifting, no rescaling;
#                       1 = add intercept, but neither shift nor rescale X;
#                       2 = add intercept, shift & rescale X columns to mean = 0, variance = 1
# reg   Double  0.0     Regularization parameter (lambda) for L2 regularization
# tol   Double 0.000001 Tolerance (epsilon)
# disp  Double  0.0     (Over-)dispersion value, or 0.0 to estimate it from data
# moi   Int     200     Maximum number of outer (Newton / Fisher Scoring) iterations
# mii   Int     0       Maximum number of inner (Conjugate Gradient) iterations, 0 = no maximum
# ---------------------------------------------------------------------------------------------
# OUTPUT: Matrix beta, whose size depends on icpt:
#     icpt=0: ncol(X) x 1;  icpt=1: (ncol(X) + 1) x 1;  icpt=2: (ncol(X) + 1) x 2
#
# Example with distribution = "Binomial.logit":
# hadoop jar SystemML.jar -f GLM_HOME/GLM.dml -nvargs dfam=2 link=2 yneg=-1.0 icpt=2 reg=0.001
#     tol=0.00000001 disp=1.0 moi=100 mii=10 X=INPUT_DIR/X Y=INPUT_DIR/Y B=OUTPUT_DIR/betas
#
#
# SUPPORTED GLM DISTRIBUTION FAMILIES AND LINKS:
# ----------------------------------------------
# INPUT PARAMETERS:    MEANING:           Cano-
# dfam vpow link lpow  Distribution.link  nical?
# ----------------------------------------------
#  1   0.0   1  -1.0   Gaussian.inverse
#  1   0.0   1   0.0   Gaussian.log
#  1   0.0   1   1.0   Gaussian.id         Yes
#  1   1.0   1   0.0   Poisson.log         Yes
#  1   1.0   1   0.5   Poisson.sqrt
#  1   1.0   1   1.0   Poisson.id
#  1   2.0   1  -1.0   Gamma.inverse       Yes
#  1   2.0   1   0.0   Gamma.log
#  1   2.0   1   1.0   Gamma.id
#  1   3.0   1  -2.0   InvGaussian.1/mu^2  Yes
#  1   3.0   1  -1.0   InvGaussian.inverse
#  1   3.0   1   0.0   InvGaussian.log
#  1   3.0   1   1.0   InvGaussian.id
#  1    *    1    *    AnyVariance.AnyLink
# ----------------------------------------------
#  2    *    1   0.0   Binomial.log
#  2    *    1   0.5   Binomial.sqrt
#  2    *    2    *    Binomial.logit      Yes
#  2    *    3    *    Binomial.probit
#  2    *    4    *    Binomial.cloglog
#  2    *    5    *    Binomial.cauchit
# ----------------------------------------------


# Default values for input parameters
/*
$dfam = 1;
$vpow = 0.0;
$link = 0;
$lpow = 1.0;
$yneg = 0.0;
$icpt = 0;
$reg  = 0.0;
$tol  = 0.000001;
$disp = 0.0;
$moi  = 200;
$mii  = 0;
*/

print("BEGIN GLM SCRIPT");
print("Reading X...");
X = read ($X);
print("Reading Y...");
Y = read ($Y);
intercept_status = $icpt;

num_records  = nrow (X);
num_features = ncol (X);
zeros_r = matrix (0, rows = num_records, cols = 1);
ones_r = 1 + zeros_r;

# Introduce the intercept, shift and rescale the columns of X if needed

shift_X_cols = matrix (0, rows = 1, cols = num_features);
scale_X_cols = matrix (1, rows = 1, cols = num_features);
scale_lambda = matrix (1, rows = 1, cols = num_features);

if (intercept_status == 1 | intercept_status == 2) {  # add the intercept column
    if (intercept_status == 2) { # shift/rescale X columns to mean 0, variance 1
        shift_X_cols = - colSums(X) / num_records;
        X = X + ones_r %*% shift_X_cols;
        scale_X_cols = sqrt ((num_records - 1) / colSums (X ^ 2));
        X = X * (ones_r %*% scale_X_cols);
#scale_lambda = colSums (X ^ 2) / (num_records - 1); # ALTERNATIVE: rescale regularization template
    }
    X = append (X, ones_r);
    num_features = ncol (X);
    zero_cell = matrix (0, rows = 1, cols = 1);
    shift_X_cols = append (shift_X_cols, zero_cell);
    scale_X_cols = append (scale_X_cols, zero_cell + 1);
    scale_lambda = append (scale_lambda, zero_cell);
}

# Initialize other input-dependent parameters

distribution_type = $dfam;
variance_as_power_of_the_mean = 0.0 + $vpow;
bernoulli_No_label = 0.0 + $yneg;
link_type = $link;
link_as_power_of_the_mean = 0.0 + $lpow;
overdispersion = 0.0 + $disp;
lambda = t(scale_lambda) * $reg;
eps = 0.0 + $tol;
max_iteration_IRLS = $moi;
max_iteration_CG = $mii;
if (max_iteration_CG == 0) {
    max_iteration_CG = num_features;
}

# In Bernoulli case, convert one-column "Y" into two-column

if (distribution_type == 2 & ncol(Y) == 1)
{
    if (bernoulli_No_label == 1.0) {
        print ("GLM Input Error: the NO-label for Bernoulli distribution cannot be identical to the YES-label, which is 1.0.");
    } else {
        Y = append (Y - bernoulli_No_label, 1.0 - Y);
        Y = Y / (1.0 - bernoulli_No_label);
}   }

# Set up the canonical link, if requested [Then we have: Var(mu) * (d link / d mu) = const]

if (link_type == 0)
{
    if (distribution_type == 1) {
        link_type = 1;
        link_as_power_of_the_mean = 1.0 - variance_as_power_of_the_mean;
    } else { if (distribution_type == 2) {
            link_type = 2;
}   }   }

# For power distributions and/or links, we use two constants,
# "variance as power of the mean" and "link_as_power_of_the_mean",
# to specify the variance and the link as arbitrary powers of the
# mean.  However, the variance-powers of 1.0 (Poisson family) and
# 2.0 (Gamma family) have to be treated as special cases, because
# these values integrate into logarithms.  The link-power of 0.0
# is also special as it represents the logarithm link.

num_response_columns = ncol (Y);
is_supported = check_if_supported (num_response_columns, distribution_type, variance_as_power_of_the_mean, link_type, link_as_power_of_the_mean);
if (is_supported == 1)
{

#####   INITIALIZE THE BETAS   #####

[beta, saturated_log_l, isNaN] = 
    glm_initialize (X, Y, distribution_type, variance_as_power_of_the_mean, link_type, link_as_power_of_the_mean, intercept_status);
if (isNaN == 0)
{

#####  START OF THE MAIN PART  #####

trust_delta = 0.5 * sqrt (num_features) / max (sqrt (rowSums (X ^ 2)));
max_trust_delta = trust_delta * 10000.0;
g = matrix (0.0, rows = num_features, cols = 1);
A = matrix (0.0, rows = num_features, cols = num_features);
log_l = 0.0;
deviance_nodisp = 0.0;
new_deviance_nodisp = 0.0;
isNaN_log_l = 2;
newbeta = beta;
accept_new_beta = 1;
reached_trust_boundary = 0;
neg_log_l_change_predicted = 0.0;
converged_IRLS = 0;
i_IRLS = 0;

print ("BEGIN IRLS ITERATIONS...");

all_linear_terms = X %*% newbeta;

[new_log_l, isNaN_new_log_l] = glm_log_likelihood_part
    (all_linear_terms, Y, distribution_type, variance_as_power_of_the_mean, link_type, link_as_power_of_the_mean);

if (isNaN_new_log_l == 0) {
    new_deviance_nodisp = 2.0 * (saturated_log_l - new_log_l);
    new_log_l = new_log_l - 0.5 * sum (lambda * newbeta ^ 2);
}
                             

while (converged_IRLS == 0)
{
    accept_new_beta = 1;
    
    if (i_IRLS > 0)
    {
        if (isNaN_log_l == 0) {
            accept_new_beta = 0;
        }

# Decide whether to accept a new iteration point and update the trust region
# See Alg. 4.1 on p. 69 of "Numerical Optimization" 2nd ed. by Nocedal and Wright

        rho = (- new_log_l + log_l) / neg_log_l_change_predicted;
        if (rho < 0.25 | isNaN_new_log_l == 1) {
            trust_delta = 0.25 * trust_delta;
        }
        if (rho > 0.75 & isNaN_new_log_l == 0 & reached_trust_boundary == 1) {
            trust_delta = 2 * trust_delta;
            if (trust_delta > max_trust_delta) {
                trust_delta = max_trust_delta;
            }
        }
        if (rho > 0.1 & isNaN_new_log_l == 0) {
            accept_new_beta = 1;
        }
    }

    if (accept_new_beta == 1)
    {
        beta = newbeta;  log_l = new_log_l;  deviance_nodisp = new_deviance_nodisp;  isNaN_log_l = isNaN_new_log_l;
        
        [g_Y, w] = glm_dist (all_linear_terms, Y, distribution_type, variance_as_power_of_the_mean, link_type, link_as_power_of_the_mean);
        
        # We introduced these variables to avoid roundoff errors:
        #     g_Y = y_residual / (y_var * link_grad);
        #     w   = 1.0 / (y_var * link_grad * link_grad);
                      
/*                      
norm_y_residual = sqrt (sum (y_residual * y_residual));
norm_y_var = sqrt (sum (y_var * y_var));
norm_link_grad = sqrt (sum (link_grad * link_grad));
min_linear_term = min (all_linear_terms);
max_linear_term = max (all_linear_terms);
print ("||y_residual|| = " + norm_y_residual + ";  ||y_var|| = " + norm_y_var + ";  ||link_grad|| = " + norm_link_grad + ";  max_linear_term = " + max_linear_term);
print ("min (y_var) = " + min (y_var) + ";  min (abs (link_grad)) = " + min (abs (link_grad)));                      
*/
                      
        g = - t(X) %*% g_Y;
#       A = t(X) %*% diag (w) %*% X;
    }
    
    [z, neg_log_l_change_predicted, reached_trust_boundary] = 
        get_CG_Steihaug_point_Xw (X, w, g, beta, lambda, trust_delta, max_iteration_CG);

#   [z, neg_log_l_change_predicted, reached_trust_boundary] = 
#       get_CG_Steihaug_point_A (A, g, beta, lambda, trust_delta, max_iteration_CG);

    newbeta = beta + z;
    
    all_linear_terms = X %*% newbeta;
    
    [new_log_l, isNaN_new_log_l] = glm_log_likelihood_part
        (all_linear_terms, Y, distribution_type, variance_as_power_of_the_mean, link_type, link_as_power_of_the_mean);

    if (isNaN_new_log_l == 0) {
        new_deviance_nodisp = 2.0 * (saturated_log_l - new_log_l);
        new_log_l = new_log_l - 0.5 * sum (lambda * newbeta ^ 2);
    }
        
    log_l_change = new_log_l - log_l;               # R's criterion for termination: |dev - devold|/(|dev| + 0.1) < eps

    if (reached_trust_boundary == 0 & isNaN_new_log_l == 0 & 
        (2.0 * abs (log_l_change) < eps * (deviance_nodisp + 0.1) | abs (log_l_change) < (abs (log_l) + abs (new_log_l)) * 0.00000000000001) )  
    {
        converged_IRLS = 1;
    }
    rho = - log_l_change / neg_log_l_change_predicted;
    z_norm = sqrt (sum (z * z));
    
    [z_norm_m, z_norm_e] = round_to_print (z_norm);
    [trust_delta_m, trust_delta_e] = round_to_print (trust_delta);
    [rho_m, rho_e] = round_to_print (rho);
    [new_log_l_m, new_log_l_e] = round_to_print (new_log_l);
    [log_l_change_m, log_l_change_e] = round_to_print (log_l_change);
    g_norm = sqrt (sum (g * g));
    [g_norm_m, g_norm_e] = round_to_print (g_norm);

    i_IRLS = i_IRLS + 1;
    print ("Iter #" + i_IRLS + " completed"
        + ", ||z|| = " + z_norm_m + "E" + z_norm_e
        + ", trust_delta = " + trust_delta_m + "E" + trust_delta_e
        + ", reached = " + reached_trust_boundary
        + ", ||g|| = " + g_norm_m + "E" + g_norm_e
        + ", new_log_l = " + new_log_l_m + "E" + new_log_l_e
        + ", log_l_change = " + log_l_change_m + "E" + log_l_change_e
        + ", rho = " + rho_m + "E" + rho_e);
    if (i_IRLS == max_iteration_IRLS) {
        converged_IRLS = 2;
    }
}

beta = newbeta;
log_l = new_log_l;
deviance_nodisp = new_deviance_nodisp;

if (converged_IRLS == 1) {
    print ("Converged in " + i_IRLS + " steps.");
} else {
    print ("Did not converge.");
}

beta_out = beta;
if (intercept_status == 2) {
    beta_out = t(scale_X_cols) * beta_out;
    beta_out [num_features, 1] = beta_out [num_features, 1] + shift_X_cols %*% beta_out;
    beta_out = append (beta_out, beta);
}

print ("beta[1] = " + castAsScalar(beta_out [1, 1]));
print ("beta[2] = " + castAsScalar(beta_out [2, 1]));
print ("beta[3] = " + castAsScalar(beta_out [3, 1]));

write (beta_out, $B, format="text");



#####  OVER-DISPERSION PART  #####

all_linear_terms = X %*% beta;
[g_Y, w] = glm_dist (all_linear_terms, Y, distribution_type, variance_as_power_of_the_mean, link_type, link_as_power_of_the_mean);
    
pearson_residual_sq = g_Y ^ 2 / w;
pearson_residual_sq = replace (target = pearson_residual_sq, pattern = 0.0/0.0, replacement = 0);
# pearson_residual_sq = (y_residual ^ 2) / y_var;

if (overdispersion <= 0.0)
{
    overdispersion = sum (pearson_residual_sq) / (num_records - num_features);
    print ("Estimated overdispersion = " + overdispersion);
}

deviance = deviance_nodisp / overdispersion;
print ("Deviance:  no overdispersion = " + deviance_nodisp + ",  with overdispersion = " + deviance);


#####  END OF THE MAIN PART  #####

} else { print ("Response vector is out of range.  Terminating the DML."); }
} else { print ("Distribution/Link not supported.  Terminating the DML."); }


check_if_supported = 
    function (int ncol_y, int dist_type, double var_power, int link_type, double link_power)
    return   (int is_supported)
{
    is_supported = 0;
    if (ncol_y == 1 & dist_type == 1 & link_type == 1)
    { # POWER DISTRIBUTION
        is_supported = 1;
        if (var_power == 0.0 & link_power == -1.0) {print ("Gaussian.inverse");      } else {
        if (var_power == 0.0 & link_power ==  0.0) {print ("Gaussian.log");          } else {
        if (var_power == 0.0 & link_power ==  1.0) {print ("Gaussian.id");           } else {
        if (var_power == 0.0                     ) {print ("Gaussian.power_nonlog"); } else {
        if (var_power == 1.0 & link_power ==  0.0) {print ("Poisson.log");           } else {
        if (var_power == 1.0 & link_power ==  0.5) {print ("Poisson.sqrt");          } else {
        if (var_power == 1.0 & link_power ==  1.0) {print ("Poisson.id");            } else {
        if (var_power == 1.0                     ) {print ("Poisson.power_nonlog");  } else {
        if (var_power == 2.0 & link_power == -1.0) {print ("Gamma.inverse");         } else {
        if (var_power == 2.0 & link_power ==  0.0) {print ("Gamma.log");             } else {
        if (var_power == 2.0 & link_power ==  1.0) {print ("Gamma.id");              } else {
        if (var_power == 2.0                     ) {print ("Gamma.power_nonlog");    } else {
        if (var_power == 3.0 & link_power == -2.0) {print ("InvGaussian.1/mu^2");    } else {
        if (var_power == 3.0 & link_power == -1.0) {print ("InvGaussian.inverse");   } else {
        if (var_power == 3.0 & link_power ==  0.0) {print ("InvGaussian.log");       } else {
        if (var_power == 3.0 & link_power ==  1.0) {print ("InvGaussian.id");        } else {
        if (var_power == 3.0                     ) {print ("InvGaussian.power_nonlog");}else{
        if (                   link_power ==  0.0) {print ("PowerDist.log");         } else {
                                                    print ("PowerDist.power_nonlog");  # TAKING THIS INSTRUCTION INTO {...} GIVES AN ERROR!
    }   }}}}} }}}}} }}}}} }}}
    if (ncol_y == 1 & dist_type == 2)
    {
        print ("Error: Bernoulli response matrix has not been converted into two-column format.");
    }
    if (ncol_y == 2 & dist_type == 2 & link_type >= 1 & link_type <= 5)
    { # BINOMIAL/BERNOULLI DISTRIBUTION
        is_supported = 1;
        if (link_type == 1 & link_power == 0.0) {print ("Binomial.log");             } else {
        if (link_type == 1)                     {print ("Binomial.power_nonlog");    } else {
        if (link_type == 2)                     {print ("Binomial.logit");           } else {
        if (link_type == 3)                     {print ("Binomial.probit");          } else {
        if (link_type == 4)                     {print ("Binomial.cloglog");         } else {
        if (link_type == 5)                     {print ("Binomial.cauchit");         }
    }   }}}}}
    if (is_supported == 0) {
        print ("Response matrix with " + ncol_y + " columns, distribution family (" + dist_type + ", " + var_power
             + ") and link family (" + link_type + ", " + link_power + ") are NOT supported together.");
    }
}

glm_initialize = function (Matrix[double] X, Matrix[double] Y,
                 int dist_type, double var_power, int link_type, double link_power, int icept_status)
return (Matrix[double] beta, double saturated_log_l, int isNaN)
{
    saturated_log_l = 0.0;
    isNaN = 0;
    y_corr = Y [, 1];
    if (dist_type == 2) {
        n_corr = rowSums (Y);
        is_n_zero = ppred (n_corr, 0.0, "==");
        y_corr = Y [, 1] / (n_corr + is_n_zero) + (0.5 - Y [, 1]) * is_n_zero;    
    }
    linear_terms = y_corr;
    if (dist_type == 1 & link_type == 1) { # POWER DISTRIBUTION
        if          (link_power ==  0.0) {
            if (sum (ppred (y_corr, 0.0, "<")) == 0) {
                is_zero_y_corr = ppred (y_corr, 0.0, "==");
                linear_terms = log (y_corr + is_zero_y_corr) - is_zero_y_corr / (1.0 - is_zero_y_corr);
            } else { isNaN = 1; }
        } else { if (link_power ==  1.0) {
            linear_terms = y_corr;
        } else { if (link_power == -1.0) {
            linear_terms = 1.0 / y_corr;
        } else { if (link_power ==  0.5) {
            if (sum (ppred (y_corr, 0.0, "<")) == 0) {
                linear_terms = sqrt (y_corr);
            } else { isNaN = 1; }
        } else { if (link_power >   0.0) {
            if (sum (ppred (y_corr, 0.0, "<")) == 0) {
                is_zero_y_corr = ppred (y_corr, 0.0, "==");
                linear_terms = (y_corr + is_zero_y_corr) ^ link_power - is_zero_y_corr;
            } else { isNaN = 1; }
        } else {
            if (sum (ppred (y_corr, 0.0, "<=")) == 0) {
                linear_terms = y_corr ^ link_power;
            } else { isNaN = 1; }
        }}}}}
    }
    if (dist_type == 2 & link_type >= 1 & link_type <= 5)
    { # BINOMIAL/BERNOULLI DISTRIBUTION
        if          (link_type == 1 & link_power == 0.0)  { # Binomial.log
            if (sum (ppred (y_corr, 0.0, "<")) == 0) {
                is_zero_y_corr = ppred (y_corr, 0.0, "==");
                linear_terms = log (y_corr + is_zero_y_corr) - is_zero_y_corr / (1.0 - is_zero_y_corr);
            } else { isNaN = 1; }
        } else { if (link_type == 1 & link_power >  0.0)  { # Binomial.power_nonlog pos
            if (sum (ppred (y_corr, 0.0, "<")) == 0) {
                is_zero_y_corr = ppred (y_corr, 0.0, "==");
                linear_terms = (y_corr + is_zero_y_corr) ^ link_power - is_zero_y_corr;
            } else { isNaN = 1; }
        } else { if (link_type == 1)                      { # Binomial.power_nonlog neg
            if (sum (ppred (y_corr, 0.0, "<=")) == 0) {
                linear_terms = y_corr ^ link_power;
            } else { isNaN = 1; }
        } else { 
            is_zero_y_corr = ppred (y_corr, 0.0, "<=");
            is_one_y_corr  = ppred (y_corr, 1.0, ">=");
            y_corr = y_corr * (1.0 - is_zero_y_corr) * (1.0 - is_one_y_corr) + 0.5 * (is_zero_y_corr + is_one_y_corr);
            if (link_type == 2)                           { # Binomial.logit
                linear_terms = log (y_corr / (1.0 - y_corr)) 
                    + is_one_y_corr / (1.0 - is_one_y_corr) - is_zero_y_corr / (1.0 - is_zero_y_corr);
            } else { if (link_type == 3)                  { # Binomial.probit
                y_below_half = y_corr + (1.0 - 2.0 * y_corr) * ppred (y_corr, 0.5, ">");
                t = sqrt (- 2.0 * log (y_below_half));
                approx_inv_Gauss_CDF = - t + (2.515517 + t * (0.802853 + t * 0.010328)) / (1.0 + t * (1.432788 + t * (0.189269 + t * 0.001308)));
                linear_terms = approx_inv_Gauss_CDF * (1.0 - 2.0 * ppred (y_corr, 0.5, ">"))
                    + is_one_y_corr / (1.0 - is_one_y_corr) - is_zero_y_corr / (1.0 - is_zero_y_corr);
            } else { if (link_type == 4)                  { # Binomial.cloglog
                linear_terms = log (- log (1.0 - y_corr))
                    - log (- log (0.5)) * (is_zero_y_corr + is_one_y_corr)
                    + is_one_y_corr / (1.0 - is_one_y_corr) - is_zero_y_corr / (1.0 - is_zero_y_corr);
            } else { if (link_type == 5)                  { # Binomial.cauchit
                linear_terms = tan ((y_corr - 0.5) * 3.1415926535897932384626433832795)
                    + is_one_y_corr / (1.0 - is_one_y_corr) - is_zero_y_corr / (1.0 - is_zero_y_corr);
        }}  }}}}}
    }
    
    if (isNaN == 0) {
        [saturated_log_l, isNaN] = 
            glm_log_likelihood_part (linear_terms, Y, dist_type, var_power, link_type, link_power);
    }
    
    beta = matrix (0.0, rows = ncol(X), cols = 1);
    if (icept_status == 1 | icept_status == 2) {
        beta [ncol(X), 1] = 1.0;
    } else {
        beta = straightenX (X, 0.000001);   #  0.000000001);   #  avg(X %*% beta) = 1
    }

    if (link_type == 1 & link_power == 0.0) {
        beta = beta * log (0.5);
    } else { if (link_type == 1) {
        beta = beta * (0.5 ^ link_power);
    } else {
        beta = beta * 0.5;
    }}
    
#       if (shrink_linear_terms_spread == 1) {
#           linear_terms = X %*% beta;
#           meanLF = sum (linear_terms) / nrow (linear_terms);
#           [beta, sigmaLF] = scaleWeights (X, beta, meanLF, 0.0);
#       }

}


glm_dist = function (Matrix[double] linear_terms, Matrix[double] Y,
                     int dist_type, double var_power, int link_type, double link_power)
    return (Matrix[double] g_Y, Matrix[double] w)
    # ORIGINALLY we returned more meaningful vectors, namely:
    # Matrix[double] y_residual    : y - y_mean, i.e. y observed - y predicted
    # Matrix[double] link_gradient : derivative of the link function
    # Matrix[double] var_function  : variance without dispersion, i.e. the V(mu) function
    # BUT, this caused roundoff errors, so we had to compute "directly useful" vectors
    # and skip over the "meaningful intermediaries".  Now we output these two variables:
    #     g_Y = y_residual / (var_function * link_gradient);
    #     w   = 1.0 / (var_function * link_gradient ^ 2);
{
    num_records = nrow (linear_terms);
    zeros_r = matrix (0.0, rows = num_records, cols = 1);
    ones_r = 1 + zeros_r;
    g_Y  = zeros_r;
    w  = zeros_r;

    # Some constants

    one_over_sqrt_two_pi = 0.39894228040143267793994605993438;
    ones_2 = matrix (1.0, rows = 1, cols = 2);
    p_one_m_one = ones_2;
    p_one_m_one [1, 2] = -1.0;
    m_one_p_one = ones_2;
    m_one_p_one [1, 1] = -1.0;
    zero_one = ones_2;
    zero_one [1, 1] = 0.0;
    one_zero = ones_2;
    one_zero [1, 2] = 0.0;
    flip_pos = matrix (0, rows = 2, cols = 2);
    flip_neg = flip_pos;
    flip_pos [1, 2] = 1;
    flip_pos [2, 1] = 1;
    flip_neg [1, 2] = -1;
    flip_neg [2, 1] = 1;
    
    if (dist_type == 1 & link_type == 1) { # POWER DISTRIBUTION
        y_mean = zeros_r;
        if          (link_power ==  0.0) {
            y_mean = exp (linear_terms);
            y_mean_pow = y_mean ^ (1 - var_power);
            w   = y_mean_pow * y_mean;
            g_Y = y_mean_pow * (Y - y_mean);
        } else { if (link_power ==  1.0) {
            y_mean = linear_terms;
            w   = y_mean ^ (- var_power);
            g_Y = w * (Y - y_mean);
        } else {
            y_mean = linear_terms ^ (1.0 / link_power);
            c1  = (1 - var_power) / link_power - 1;
            c2  = (2 - var_power) / link_power - 2;
            g_Y = (linear_terms ^ c1) * (Y - y_mean) / link_power;
            w   = (linear_terms ^ c2) / (link_power ^ 2);
    }   }}
    if (dist_type == 2 & link_type >= 1 & link_type <= 5)
    { # BINOMIAL/BERNOULLI DISTRIBUTION
        if (link_type == 1) { # BINOMIAL.POWER LINKS
            if (link_power == 0.0)  { # Binomial.log
                vec1 = 1 / (exp (- linear_terms) - 1);
                g_Y = Y [, 1] - Y [, 2] * vec1;
                w   = rowSums (Y) * vec1;
            } else {                  # Binomial.nonlog
                vec1 = zeros_r;
                if (link_power == 0.5)  {
                    vec1 = 1 / (1 - linear_terms ^ 2);
                } else { if (sum (ppred (linear_terms, 0.0, "<")) == 0) {
                    vec1 = linear_terms ^ (- 2 + 1 / link_power) / (1 - linear_terms ^ (1 / link_power));
                } else {isNaN = 1;}}
                # We want a "zero-protected" version of
                #     vec2 = Y [, 1] / linear_terms;
                is_y_0 = ppred (Y [, 1], 0.0, "==");
                vec2 = (Y [, 1] + is_y_0) / (linear_terms * (1 - is_y_0) + is_y_0) - is_y_0;
                g_Y =  (vec2 - Y [, 2] * vec1 * linear_terms) / link_power;
                w   =  rowSums (Y) * vec1 / link_power ^ 2;
            }
        } else {
            is_LT_pos_infinite = ppred (linear_terms,  1.0/0.0, "==");
            is_LT_neg_infinite = ppred (linear_terms, -1.0/0.0, "==");
            is_LT_infinite = is_LT_pos_infinite %*% one_zero + is_LT_neg_infinite %*% zero_one;
            finite_linear_terms = replace (target =        linear_terms, pattern =  1.0/0.0, replacement = 0);
            finite_linear_terms = replace (target = finite_linear_terms, pattern = -1.0/0.0, replacement = 0);
            if (link_type == 2)                           { # Binomial.logit
                Y_prob = exp (finite_linear_terms) %*% one_zero + ones_r %*% zero_one;
                Y_prob = Y_prob / (rowSums (Y_prob) %*% ones_2);
                Y_prob = Y_prob * ((1.0 - rowSums (is_LT_infinite)) %*% ones_2) + is_LT_infinite;
                g_Y = rowSums (Y * (Y_prob %*% flip_neg));           ### = y_residual;
                w   = rowSums (Y * (Y_prob %*% flip_pos) * Y_prob);  ### = y_variance;
            } else { if (link_type == 3)                  { # Binomial.probit
                is_lt_pos = ppred (linear_terms, 0.0, ">=");
                t_gp = 1.0 / (1.0 + abs (finite_linear_terms) * 0.231641888);  # 0.231641888 = 0.3275911 / sqrt (2.0)
                pt_gp = t_gp * ( 0.254829592 
                      + t_gp * (-0.284496736 # "Handbook of Mathematical Functions", ed. by M. Abramowitz and I.A. Stegun,
                      + t_gp * ( 1.421413741 # U.S. Nat-l Bureau of Standards, 10th print (Dec 1972), Sec. 7.1.26, p. 299
                      + t_gp * (-1.453152027 
                      + t_gp *   1.061405429))));
                the_gauss_exp = exp (- (linear_terms ^ 2) / 2.0);
                vec1 = 0.25 * pt_gp * (2 - the_gauss_exp * pt_gp);
                vec2 = Y [, 1] - rowSums (Y) * is_lt_pos + the_gauss_exp * pt_gp * rowSums (Y) * (is_lt_pos - 0.5);
                w   = the_gauss_exp * (one_over_sqrt_two_pi ^ 2) * rowSums (Y) / vec1;
                g_Y = one_over_sqrt_two_pi * vec2 / vec1;
            } else { if (link_type == 4)                  { # Binomial.cloglog
                the_exp = exp (linear_terms)
                the_exp_exp = exp (- the_exp);
                is_too_small = ppred (10000000 + the_exp, 10000000, "==");
                the_exp_ratio = (1 - is_too_small) * (1 - the_exp_exp) / (the_exp + is_too_small) + is_too_small * (1 - the_exp / 2);
                g_Y =  (rowSums (Y) * the_exp_exp - Y [, 2]) / the_exp_ratio;
                w   =  the_exp_exp * the_exp * rowSums (Y) / the_exp_ratio;
            } else { if (link_type == 5)                  { # Binomial.cauchit
                Y_prob = 0.5 + (atan (finite_linear_terms) %*% p_one_m_one) / 3.1415926535897932384626433832795;
                Y_prob = Y_prob * ((1.0 - rowSums (is_LT_infinite)) %*% ones_2) + is_LT_infinite;
                y_residual = Y [, 1] * Y_prob [, 2] - Y [, 2] * Y_prob [, 1];
                var_function = rowSums (Y) * Y_prob [, 1] * Y_prob [, 2];
                link_gradient_normalized = (1 + linear_terms ^ 2) * 3.1415926535897932384626433832795;
                g_Y =  rowSums (Y) * y_residual / (var_function * link_gradient_normalized);
                w   = (rowSums (Y) ^ 2) / (var_function * link_gradient_normalized ^ 2);
            }}}}   
        }
    }
}


glm_log_likelihood_part = function (Matrix[double] linear_terms, Matrix[double] Y,
        int dist_type, double var_power, int link_type, double link_power)
    return (double log_l, int isNaN)
{
    isNaN = 0;
    log_l = 0.0;
    num_records = nrow (Y);
    zeros_r = matrix (0.0, rows = num_records, cols = 1);
    
    if (dist_type == 1 & link_type == 1)
    { # POWER DISTRIBUTION
        b_cumulant = zeros_r;
        natural_parameters = zeros_r;
        is_natural_parameter_log_zero = zeros_r;
        if          (var_power == 1.0 & link_power == 0.0)  { # Poisson.log
            b_cumulant = exp (linear_terms);
            is_natural_parameter_log_zero = ppred (linear_terms, -1.0/0.0, "==");
            natural_parameters = replace (target = linear_terms, pattern = -1.0/0.0, replacement = 0);
        } else { if (var_power == 1.0 & link_power == 1.0)  { # Poisson.id
            if (sum (ppred (linear_terms, 0.0, "<")) == 0)  {
                b_cumulant = linear_terms;
                is_natural_parameter_log_zero = ppred (linear_terms, 0.0, "==");
                natural_parameters = log (linear_terms + is_natural_parameter_log_zero);
            } else {isNaN = 1;}
        } else { if (var_power == 1.0 & link_power == 0.5)  { # Poisson.sqrt
            if (sum (ppred (linear_terms, 0.0, "<")) == 0)  {
                b_cumulant = linear_terms ^ 2;
                is_natural_parameter_log_zero = ppred (linear_terms, 0.0, "==");
                natural_parameters = 2.0 * log (linear_terms + is_natural_parameter_log_zero);
            } else {isNaN = 1;}
        } else { if (var_power == 1.0 & link_power  > 0.0)  { # Poisson.power_nonlog, pos
            if (sum (ppred (linear_terms, 0.0, "<")) == 0)  {
                is_natural_parameter_log_zero = ppred (linear_terms, 0.0, "==");
                b_cumulant = (linear_terms + is_natural_parameter_log_zero) ^ (1.0 / link_power) - is_natural_parameter_log_zero;
                natural_parameters = log (linear_terms + is_natural_parameter_log_zero) / link_power;
            } else {isNaN = 1;}
        } else { if (var_power == 1.0)                      { # Poisson.power_nonlog, neg
            if (sum (ppred (linear_terms, 0.0, "<=")) == 0) {
                b_cumulant = linear_terms ^ (1.0 / link_power);
                natural_parameters = log (linear_terms) / link_power;
            } else {isNaN = 1;}
        } else { if (var_power == 2.0 & link_power == -1.0) { # Gamma.inverse
            if (sum (ppred (linear_terms, 0.0, "<=")) == 0) {
                b_cumulant = - log (linear_terms);
                natural_parameters = - linear_terms;
            } else {isNaN = 1;}
        } else { if (var_power == 2.0 & link_power ==  1.0) { # Gamma.id
            if (sum (ppred (linear_terms, 0.0, "<=")) == 0) {
                b_cumulant = log (linear_terms);
                natural_parameters = - 1.0 / linear_terms;
            } else {isNaN = 1;}
        } else { if (var_power == 2.0 & link_power ==  0.0) { # Gamma.log
            b_cumulant = linear_terms;
            natural_parameters = - exp (- linear_terms);
        } else { if (var_power == 2.0)                      { # Gamma.power_nonlog
            if (sum (ppred (linear_terms, 0.0, "<=")) == 0) {
                b_cumulant = log (linear_terms) / link_power;
                natural_parameters = - linear_terms ^ (- 1.0 / link_power);
            } else {isNaN = 1;}
        } else { if                    (link_power ==  0.0) { # PowerDist.log
            natural_parameters = exp (linear_terms * (1.0 - var_power)) / (1.0 - var_power);
            b_cumulant = exp (linear_terms * (2.0 - var_power)) / (2.0 - var_power);
        } else {                                              # PowerDist.power_nonlog
            if          (-2 * link_power == 1.0 - var_power) {
                natural_parameters = 1.0 / (linear_terms ^ 2) / (1.0 - var_power);
            } else { if (-1 * link_power == 1.0 - var_power) {
                natural_parameters = 1.0 / linear_terms / (1.0 - var_power);
            } else { if (     link_power == 1.0 - var_power) {
                natural_parameters = linear_terms / (1.0 - var_power);
            } else { if ( 2 * link_power == 1.0 - var_power) {
                natural_parameters = linear_terms ^ 2 / (1.0 - var_power);
            } else {
                if (sum (ppred (linear_terms, 0.0, "<=")) == 0) {
                    power = (1.0 - var_power) / link_power;
                    natural_parameters = (linear_terms ^ power) / (1.0 - var_power);
                } else {isNaN = 1;}
            }}}}
            if          (-2 * link_power == 2.0 - var_power) {
                b_cumulant = 1.0 / (linear_terms ^ 2) / (2.0 - var_power);
            } else { if (-1 * link_power == 2.0 - var_power) {
                b_cumulant = 1.0 / linear_terms / (2.0 - var_power);
            } else { if (     link_power == 2.0 - var_power) {
                b_cumulant = linear_terms / (2.0 - var_power);
            } else { if ( 2 * link_power == 2.0 - var_power) {
                b_cumulant = linear_terms ^ 2 / (2.0 - var_power);
            } else {
                if (sum (ppred (linear_terms, 0.0, "<=")) == 0) {
                    power = (2.0 - var_power) / link_power;
                    b_cumulant = (linear_terms ^ power) / (2.0 - var_power);
                } else {isNaN = 1;}
            }}}}
        }}}}} }}}}}
        if (sum (is_natural_parameter_log_zero * abs (Y)) > 0.0) {
            log_l = -1.0 / 0.0;
            isNaN = 1;
        }
        if (isNaN == 0)
        {
            log_l = sum (Y * natural_parameters - b_cumulant);
            if (log_l != log_l | (log_l == log_l + 1.0 & log_l == log_l * 2.0)) {
                log_l = -1.0 / 0.0;
                isNaN = 1;
    }   }   }
    
    if (dist_type == 2 & link_type >= 1 & link_type <= 5)
    { # BINOMIAL/BERNOULLI DISTRIBUTION
    
        [Y_prob, isNaN] = binomial_probability_two_column (linear_terms, link_type, link_power);
        
        if (isNaN == 0) {            
            does_prob_contradict = ppred (Y_prob, 0.0, "<=");
            if (sum (does_prob_contradict * abs (Y)) == 0.0) {
                log_l = sum (Y * log (Y_prob * (1 - does_prob_contradict) + does_prob_contradict));
                if (log_l != log_l | (log_l == log_l + 1.0 & log_l == log_l * 2.0)) {
                    isNaN = 1;
                }
            } else {
                log_l = -1.0 / 0.0;
                isNaN = 1;
    }   }   }
    
    if (isNaN == 1) {
        log_l = - 1.0 / 0.0; 
    }
}



binomial_probability_two_column =
    function (Matrix[double] linear_terms, int link_type, double link_power)
    return   (Matrix[double] Y_prob, int isNaN)
{
    isNaN = 0;
    num_records = nrow (linear_terms);

    # Define some auxiliary matrices

    ones_2 = matrix (1.0, rows = 1, cols = 2);
    p_one_m_one = ones_2;
    p_one_m_one [1, 2] = -1.0;
    m_one_p_one = ones_2;
    m_one_p_one [1, 1] = -1.0;
    zero_one = ones_2;
    zero_one [1, 1] = 0.0;
    one_zero = ones_2;
    one_zero [1, 2] = 0.0;

    zeros_r = matrix (0.0, rows = num_records, cols = 1);
    ones_r = 1.0 + zeros_r;

    # Begin the function body

    Y_prob = zeros_r %*% ones_2;
    if (link_type == 1) { # Binomial.power
        if          (link_power == 0.0) { # Binomial.log
            Y_prob = exp (linear_terms) %*% p_one_m_one + ones_r %*% zero_one;    
        } else { if (link_power == 0.5) { # Binomial.sqrt
            Y_prob = (linear_terms ^ 2) %*% p_one_m_one + ones_r %*% zero_one;    
        } else {                          # Binomial.power_nonlog
            if (sum (ppred (linear_terms, 0.0, "<")) == 0) {
                Y_prob = (linear_terms ^ (1.0 / link_power)) %*% p_one_m_one + ones_r %*% zero_one;    
            } else {isNaN = 1;}
        }}
    } else {              # Binomial.non_power
        is_LT_pos_infinite = ppred (linear_terms,  1.0/0.0, "==");
        is_LT_neg_infinite = ppred (linear_terms, -1.0/0.0, "==");
        is_LT_infinite = is_LT_pos_infinite %*% one_zero + is_LT_neg_infinite %*% zero_one;
        finite_linear_terms = replace (target =        linear_terms, pattern =  1.0/0.0, replacement = 0);
        finite_linear_terms = replace (target = finite_linear_terms, pattern = -1.0/0.0, replacement = 0);
        if (link_type == 2)             { # Binomial.logit
            Y_prob = exp (finite_linear_terms) %*% one_zero + ones_r %*% zero_one;
            Y_prob = Y_prob / (rowSums (Y_prob) %*% ones_2);
        } else { if (link_type == 3)    { # Binomial.probit
            lt_pos_neg = ppred (finite_linear_terms, 0.0, ">=") %*% p_one_m_one + ones_r %*% zero_one;
            t_gp = 1.0 / (1.0 + abs (finite_linear_terms) * 0.231641888);  # 0.231641888 = 0.3275911 / sqrt (2.0)
            pt_gp = t_gp * ( 0.254829592 
                  + t_gp * (-0.284496736 # "Handbook of Mathematical Functions", ed. by M. Abramowitz and I.A. Stegun,
                  + t_gp * ( 1.421413741 # U.S. Nat-l Bureau of Standards, 10th print (Dec 1972), Sec. 7.1.26, p. 299
                  + t_gp * (-1.453152027 
                  + t_gp *   1.061405429))));
            the_gauss_exp = exp (- (finite_linear_terms ^ 2) / 2.0);
            Y_prob = lt_pos_neg + ((the_gauss_exp * pt_gp) %*% ones_2) * (0.5 - lt_pos_neg);
        } else { if (link_type == 4)    { # Binomial.cloglog
            the_exp = exp (finite_linear_terms);
            the_exp_exp = exp (- the_exp);
            is_too_small = ppred (10000000 + the_exp, 10000000, "==");
            Y_prob [, 1] = (1 - is_too_small) * (1 - the_exp_exp) + is_too_small * the_exp * (1 - the_exp / 2);
            Y_prob [, 2] = the_exp_exp;
        } else { if (link_type == 5)    { # Binomial.cauchit
            Y_prob = 0.5 + (atan (finite_linear_terms) %*% p_one_m_one) / 3.1415926535897932384626433832795;
        } else {
            isNaN = 1;
        }}}}
        Y_prob = Y_prob * ((1.0 - rowSums (is_LT_infinite)) %*% ones_2) + is_LT_infinite;
}   }            



# Computes vector w such that  ||X %*% w - 1|| -> MIN  given  avg(X %*% w) = 1
# We find z_LS such that ||X %*% z_LS - 1|| -> MIN unconditionally, then scale
# it to compute  w = c * z_LS  such that  sum(X %*% w) = nrow(X).
straightenX =
    function (Matrix[double] X, double eps)
    return   (Matrix[double] w)
{
    eps_LS = eps * nrow(X);
    lambda_LS = 0.000001 * sum(X ^ 2) / ncol(X);
    w_X = t(colSums(X));

    # BEGIN LEAST SQUARES
    
    r_LS = - w_X;
    z_LS = matrix (0.0, rows = ncol(X), cols = 1);
    p_LS = - r_LS;
    norm_r2_LS = sum (r_LS ^ 2);
    i_LS = 0;
    while (i_LS < 50 & i_LS < ncol(X) & norm_r2_LS >= eps_LS)
    {
        temp_LS = X %*% p_LS;
        q_LS = (t(X) %*% temp_LS) + lambda_LS * p_LS;
        alpha_LS = norm_r2_LS / sum (p_LS * q_LS);
        z_LS = z_LS + alpha_LS * p_LS;
        old_norm_r2_LS = norm_r2_LS;
        r_LS = r_LS + alpha_LS * q_LS;
        norm_r2_LS = sum (r_LS ^ 2);
        p_LS = -r_LS + (norm_r2_LS / old_norm_r2_LS) * p_LS;
        i_LS = i_LS + 1;
    }
    
    # END LEAST SQUARES
    
    w = (nrow(X) / sum (w_X * z_LS)) * z_LS;
}


        
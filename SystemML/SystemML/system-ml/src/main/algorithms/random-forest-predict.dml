#-------------------------------------------------------------
# IBM Confidential
# OCO Source Materials
# (C) Copyright IBM Corp. 2010, 2015
# The source code for this program is not published or
# otherwise divested of its trade secrets, irrespective of
# what has been deposited with the U.S. Copyright Office.
#-------------------------------------------------------------
#  
# THIS SCRIPT COMPUTES LABEL PREDICTIONS MEANT FOR USE WITH A DECISION TREE MODEL ON A HELD OUT TEST SET.
#
# INPUT         PARAMETERS:
# ---------------------------------------------------------------------------------------------
# NAME          TYPE     DEFAULT      MEANING
# ---------------------------------------------------------------------------------------------
# X             String   ---          Location to read test feature matrix X; note that X needs to be both recoded and dummy coded 
# Y	 		    String   " "		  Location to read true label matrix Y if requested; note that Y needs to be both recoded and dummy coded
# R   	  		String   " "	      Location to read the matrix R which for each feature in X contains the following information 
#										- R[,1]: column ids
#										- R[,2]: start indices 
#										- R[,3]: end indices
#									  If R is not provided by default all variables are assumed to be scale
# M             String 	 ---	   	  Location to read matrix M containing the learnt tree i the following format
#								 		- M[1,j]: id of node j (in a complete binary tree)
#										- M[2,j]: tree id 
#	 									- M[3,j]: Offset (no. of columns) to left child of j if j is an internal node, otherwise 0
#	 									- M[4,j]: Feature index of the feature that node j looks at if j is an internal node, otherwise 0
#	 									- M[5,j]: Type of the feature that node j looks at if j is an internal node: 1 for scale and 2 for categorical features, 
#		     									  otherwise the label that leaf node j is supposed to predict
#	 									- M[6,j]: If j is an internal node: 1 if the feature chosen for j is scale, otherwise the size of the subset of values 
#			 									  stored in rows 7,8,... if j is categorical 
#						 						  If j is a leaf node: number of misclassified samples reaching at node j 
#	 									- M[7:,j]: If j is an internal node: Threshold the example's feature value is compared to is stored at M[7,j] 
#							   					   if the feature chosen for j is scale, otherwise if the feature chosen for j is categorical rows 7,8,... 
#												   depict the value subset chosen for j
#	          									   If j is a leaf node 1 if j is impure and the number of samples at j > threshold, otherwise 0
# C 			String   " "		  Location to read the counts matrix containing the number of times samples are chosen in each tree of the random forest
# P				String   ---		  Location to store the label predictions for X
# A     		String   " "          Location to store the test accuracy (%) for the prediction if requested
# OOB 			String   " "		  If C is provided location to store the Out-Of-Bag (OOB) error of learned model 
# CM     		String   " "		  Location to store the confusion matrix if requested 
# fmt     	    String   "text"       The output format of the output, such as "text" or "csv"
# ---------------------------------------------------------------------------------------------
# OUTPUT: 
#	1- Matrix Y containing the predicted labels for X 
#   2- Test accuracy if requested
#   3- Confusion matrix C if requested
# -------------------------------------------------------------------------------------------
# HOW TO INVOKE THIS SCRIPT - EXAMPLE:
# hadoop jar SystemML.jar -f decision-tree-predict.dml -nvargs X=INPUT_DIR/X Y=INPUT_DIR/Y R=INPUT_DIR/R M=INPUT_DIR/M P=OUTPUT_DIR/P
#														accuracy=OUTPUT_DIR/accuracy confusion=OUTPUT_DIR/confusion fmt=csv

fileX = $X;
fileM = $M;
fileP = $P;
fileY = ifdef ($Y, " ");
fileR = ifdef ($R, " ");
fileC = ifdef ($C, " ");
fileOOB = ifdef ($OOB, " ");
fileCM = ifdef ($CM, " ");
fileA = ifdef ($A, " ");
fmtO = ifdef ($fmt, "text");
X_test = read (fileX);
M = read (fileM);

num_records = nrow (X_test);
Y_predicted = matrix (0, rows = num_records, cols = 1);
num_trees  = max (M[2,]);
num_labels = max (M[5,]);
num_nodes_per_tree = aggregate (target = t (M[2,]), groups = t (M[2,]), fn = "count");
num_nodes_per_tree_cum = cumsum (num_nodes_per_tree);

R_cat = matrix (0, rows = 1, cols = 1);
R_scale = matrix (0, rows = 1, cols = 1);

if (fileR != " ") {
	R = read (fileR);
	dummy_coded = ppred (R[,2], R[,3], "!=");
	R_scale = removeEmpty (target = R[,2] * (1 - dummy_coded), margin = "rows");
	R_cat = removeEmpty (target = R[,2:3] * dummy_coded, margin = "rows");
} else { # only scale features available
	R_scale = seq (1, ncol (X_test));
}

if (fileC != " ") {
	C = read (fileC);
	label_counts_oob = matrix (0, rows = num_records, cols = num_labels);
}

label_counts = matrix (0, rows = num_records, cols = num_labels); 
for (j in 1:num_trees, check = 0) {
	
	start_ind = 1;
	if (j > 1) {
		start_ind = start_ind + as.scalar (num_nodes_per_tree_cum[j - 1,]);
	}
	offset = as.scalar (num_nodes_per_tree_cum[j,]);
	cur_M = M[,start_ind:offset];
		
	for (i in 1:num_records, check = 0) {
		cur_sample = X_test[i,];
		cur_node = 1;
		label_found = FALSE;
		while (!label_found) {
			cur_feature = as.scalar (cur_M[4,cur_node]);
			type_label = as.scalar (cur_M[5,cur_node]);
			if (cur_feature == 0) { # leaf node
				label_found = TRUE;
				label_counts[i,type_label] = label_counts[i,type_label] + 1;
				if (fileC != " ") {
					label_counts_oob[i,type_label] = label_counts_oob[i,type_label] + 1;
				}
			} else {
				# determine type: 1 for scale, 2 for categorical 
				if (type_label == 1) { # scale feature
					cur_start_ind = as.scalar (R_scale[cur_feature,]);
					cur_value = as.scalar (cur_sample[,cur_start_ind]);
					cur_split = as.scalar (M[7,cur_node]);
					if (cur_value < cur_split) { # go to left branch
						cur_node = cur_node + as.scalar (M[3,cur_node]);
					} else { # go to right branch
						cur_node = cur_node + as.scalar (M[3,cur_node]) + 1;
					}
				} else if (type_label == 2) { # categorical feature				
					cur_start_ind = as.scalar (R_cat[cur_feature,1]);
					cur_end_ind = as.scalar (R_cat[cur_feature,2]);					
					cur_value = as.scalar (rowIndexMax(cur_sample[,cur_start_ind:cur_end_ind])); # as.scalar (cur_sample[,cur_feature]);
					cur_offset = as.scalar (M[6,cur_node]);
					value_found = sum (ppred (M[7:(7 + cur_offset - 1),cur_node], cur_value, "=="));
					print ("find " + cur_value);
					success = print_result (M[7:(7 + cur_offset - 1),cur_node]);
					if (value_found) { # go to left branch
						cur_node = cur_node + as.scalar (M[3,cur_node]);
					} else { # go to right branch
						cur_node = cur_node + as.scalar (M[3,cur_node]) + 1;
					}
}}}}}

Y_predicted = t (rowIndexMax (label_counts));
write (Y_predicted, fileP, format = fmtO);

if (fileY != " ") {
	Y_test = read (fileY);
	num_classes = ncol (Y_test);
	Y_test = rowSums (Y_test * t (seq (1, num_classes)));
	result = ppred (Y_test, Y_predicted, "==");
	result = sum (result);
	accuracy = result / num_records * 100;
	acc_str = "Accuracy (%): " + accuracy;
	if (fileA != " ") {
		write (acc_str, fileA, format = fmtO);
	} else {
		print (acc_str);
	}
	if (fileC != " ") {
		Y_predicted_oob = t (rowIndexMax (label_counts_oob));
		result = ppred (Y_test, Y_predicted_oob, "==");
		oob_error = (1 - (result / num_records)) * 100;
		oob_str = "Out-Of-Bag error (%): " + oob_error;
		if (fileOOB != " ") {
			write (oob_str, fileOOB, format = fmtO);
		} else {
			print (oob_str);
		}
	}
	if (fileCM != " ") {
		confusion_mat = table(Y_predicted, Y_test, num_classes, num_classes)
        write(confusion_mat, fileCM, format = fmtO)
	}
}

if (fileC != " ") {
	Y_predicted_oob = t (rowIndexMax (label_counts_oob));
	
}
#-------------------------------------------------------------
# IBM Confidential
# OCO Source Materials
# (C) Copyright IBM Corp. 2010, 2015
# The source code for this program is not published or
# otherwise divested of its trade secrets, irrespective of
# what has been deposited with the U.S. Copyright Office.
#-------------------------------------------------------------
#  
# THIS SCRIPT IMPLEMENTS CLASSIFICATION TREES (WITH SCALE FEATURES ONLY!)
#
# INPUT         PARAMETERS:
# ---------------------------------------------------------------------------------------------
# NAME          TYPE     DEFAULT      MEANING
# ---------------------------------------------------------------------------------------------
# X             String   ---          Location to read feature matrix X; note that X needs to be recoded and dummy coded 
# Y 			String   ---		  Location to read label matrix Y
# I             String   " "          If X contains categorical features, location to read the matrix containing the start and end indices 
#									  of columns corresponding to each feature (both scale and categorical)
#								   		- I[,1]: start indices
#								   		- I[,2]: end indices
#									  If I is not provided all features are assumed to be scale
# T             String   " "          If X contains categorical features, location to read the row matrix T denoting the types of the features in X: 
#									  1 for scale, 2 for categorical 
#									  If T is not provided all features are assumed to be scale
# bins          Int 	 20			  Number of equiheight bins per scale feature to choose thresholds
# depth         Int 	 25			  Maximum depth of the learnt tree
# num_leaf      Int      10           Number of samples when splitting stops and a leaf node is added
# num_samples   Int 	 3000		  Number of samples at which point we switch to in-memory subtree building
# impurity      String   "entropy"    Impurity measure: entropy (the default) or Gini 
# M             String 	 ---	   	  Location to write matrix M containing the learnt tree
# fmt     	    String   "text"       The output format of the model (matrix M), such as "text" or "csv"
# ---------------------------------------------------------------------------------------------
# OUTPUT: 
# Matrix M where each column corresponds to a node in the learnt tree and each row contains the following information:
#	 M[1,j]: id of node j (in a complete binary tree)
#	 M[2,j]: Offset (no. of columns) to left child of j 
#	 M[3,j]: Feature index of the feature that node j looks at if j is an internal node, otherwise 0
#	 M[4,j]: Type of the feature that node j looks at if j is an internal node: 1 for scale and 2 for categorical features, 
#		     otherwise the label that leaf node j is supposed to predict
#	 M[5,j]: Only applicable for internal nodes. Threshold the example's feature value is compared to if the feature 
#	     	 chosen for j is scale or the size of the subset of values stored in rows 6,7,... if the feature chosen for j is categorical 
#	 M[6:,j]: Only applicable if j is an internal node and the feature chosen for j is categorical: rows 6,7,... depict the value subset chosen for j   
# -------------------------------------------------------------------------------------------
# HOW TO INVOKE THIS SCRIPT - EXAMPLE:
# hadoop jar SystemML.jar -f decision-tree-scale.dml -nvargs X=INPUT_DIR/X Y=INPUT_DIR/Y I=INPUT_DIR/I T=INPUT_DIR/T M=OUTPUT_DIR/M
#     				                 					     bins=20 depth=20 num_leaf=1 num_samples=3000 fmtO=csv

# udf for binning
binning = externalFunction(Matrix[Double] A, Integer binsize, Integer numbins) return (Matrix[Double] B, Integer numbinsdef) 
	implemented in (classname="com.ibm.bi.dml.udf.lib.BinningWrapper",exectype="mem")

	
fileI = ifdef($I, " ");
fileT = ifdef ($T, " ");	
num_bins = ifdef($bins, 20); 
depth = ifdef($depth, 25); 
num_leaf = ifdef($leaf, 10); 
threshold = ifdef ($num_samples, 3000); 
impurity = ifdef($impurity, "entropy");
fmtO = ifdef($fmt, "text");

X = read($X)
Y = read($Y)
if (fileI != " ") {
	I = read (fileI);
}
if (fileT != " ") {
	T = read (fileT);
	# check types
	type_not_supported = max (ppred (T, 1, "!="));
	if (type_not_supported) {
		stop ("Currently only scale features are supported!");
	}
}

num_rows = nrow (X);
num_features = ncol (X);

Y_bin = table (seq (1, num_rows), Y); # Y in binary representation
num_classes = ncol (Y_bin);


# --- binning ---
bin_size = max (as.integer (nrow(X) / num_bins), 1);
count_thresholds = matrix (0, rows = 1, cols = num_features)
thresholds = matrix (0, rows = num_bins + 1, cols = num_features)
print ("Computing binning...");
parfor(i1 in 1:num_features) { # log = DEBUG
      col = order (target = X[,i1], by = 1, decreasing = FALSE);
      [col_bins, num_bins_defined] = binning (col, bin_size, num_bins);
      count_thresholds[,i1] = num_bins_defined;
      thresholds[,i1] = col_bins;	
}
log_str = "...binning finished " + (t_binning_end - t_binning_start) + " (ms)";
print (log_str);


# --- preprocessing --- 
min_num_bins = min (count_thresholds);
max_num_bins = max (count_thresholds);
total_num_bins = sum (count_thresholds);
cum_count_thresholds = t (cumsum (t (count_thresholds)));
log_str = "min no. bins: " + (min_num_bins + 1) + " max no. bins: " + (max_num_bins + 1) + " total no. bins: " + (total_num_bins + num_features);
print (log_str);

X_ext_left = matrix (0, rows = num_rows, cols = total_num_bins);
print ("Computing preprocessed dataset...");
parfor (i2 in 1:num_features, check = 0) { #, log = DEBUG, opt = CONSTRAINED, mode = REMOTE_MR) {
# for (i2 in 1:num_features, check = 0) {
	Xi2 = X[,i2];
	count_threshold = as.scalar (count_thresholds[,i2]);
	offset_feature = 1;
	if (i2 > 1) {
		offset_feature = offset_feature + as.integer (as.scalar (cum_count_thresholds[, (i2 - 1)]));
	}
	# print ("offset_feature " + offset_feature + " count_threshold " + count_threshold);
	ti2 = t(thresholds[1:count_threshold, i2]);
	X_ext_left[,offset_feature:(offset_feature + count_threshold - 1)] = outer(Xi2, ti2, "<");
}
print ("...preprocessing finished " + (t_pre_end - t_pre_start) + " (ms)");


# --- initialization ---
L = matrix (1, rows = num_rows, cols = 1); # last visited

# model
# leaf nodes
NC_large = matrix (0, rows = 2, cols = 1);
NC_small = matrix (0, rows = 2, cols = 1);

# internal nodes
Q_large = matrix (1, rows = 1, cols = 1); # ids of LARGE internal nodes 
Q_small = matrix (0, rows = 1, cols = 1); # ids of SMALL internal nodes

F_large = matrix (0, rows = 1, cols = 1); # features
F_small = matrix (0, rows = 1, cols = 1); # features
S_large = matrix (0, rows = 1, cols = 1); # split points
S_small = matrix (0, rows = 1, cols = 1); # split points

# initialization
cur_nodes_large = matrix (1, rows = 1, cols = 1);
# I_nodes = matrix (1, rows = 1, cols = 1);
# L_nodes = matrix (0, rows = 1, cols = 1);
cur_nodes_small_nonzero = matrix (0, rows = 2, cols = 2); # first row: ids of SMALL internal nodes, second row: no. of samples for these nodes
# Ix_left = matrix (0, rows = 1, cols = 1);

num_cur_nodes_large = 1;
num_cur_nodes_small = 0;
level = 0;
while ((num_cur_nodes_large + num_cur_nodes_small) > 0 & level < depth) {
	
	level = level + 1;
	log_str = " --- start level " + level + " --- ";
	print (log_str);
	
	if (num_cur_nodes_large > 0) { # large nodes to process
		cur_Q_large = matrix (0, rows = 1, cols = 2 * num_cur_nodes_large);
		cur_NC_large = matrix (0, rows = 2, cols = 2 * num_cur_nodes_large);
		cur_F_large = matrix (0, rows = 1, cols = num_cur_nodes_large);
		cur_S_large = cur_F_large;
		cur_nodes_small = cur_NC_large; 
	}
		
	# loop over large nodes
	parfor (i6 in 1:num_cur_nodes_large, check = 0) { # log = DEBUG
	# for (i6 in 1:num_cur_nodes_large) {
				
		cur_node = as.scalar (cur_nodes_large[,i6]);	
		# print (" Expanding LARGE node: " + cur_node);
			
		# --- find best split ---
		# samples that reach cur_node 
		Ix = ppred (L[,1], cur_node, "==");		
			
		cur_Y_bin = Y_bin * Ix;
		label_counts_overall = colSums (cur_Y_bin);
		label_sum_overall = sum (label_counts_overall);
		label_dist_overall = label_counts_overall / label_sum_overall;
		label_dist_zero = ppred (label_dist_overall, 0, "==");

		cur_entropy = - sum (label_dist_overall * log (label_dist_overall + label_dist_zero) / log (2)); # entropy before
			
		# main operation	
		label_counts_left = t (t (cur_Y_bin) %*% X_ext_left);  
		
		# compute left and right label distribution
		label_sum_left = rowSums (label_counts_left);
		label_dist_left = label_counts_left / label_sum_left;
		label_dist_left = replace (target = label_dist_left, pattern = "NaN", replacement = 1);
		label_dist_left = replace (target = label_dist_left, pattern = 0, replacement = 1);
		log_label_dist_left = log (label_dist_left);
		entropy_left = - rowSums (label_dist_left * log_label_dist_left);
		#
		label_counts_right = - label_counts_left + label_counts_overall;
		label_sum_right = rowSums (label_counts_right);
		label_dist_right = label_counts_right / label_sum_right;
		label_dist_right = replace (target = label_dist_right, pattern = "NaN", replacement = 1);
		label_dist_right = replace (target = label_dist_right, pattern = 0, replacement = 1);
		log_label_dist_right = log (label_dist_right);
		entropy_right = - rowSums (label_dist_right * log_label_dist_right);			
			
		I_gain = cur_entropy - ( ( label_sum_left / label_sum_overall ) * entropy_left + ( label_sum_right / label_sum_overall ) * entropy_right);
			
		Ix_label_sum_left_zero = ppred (label_sum_left, 0, "==");
		Ix_label_sum_right_zero = ppred (label_sum_right, 0, "==");
		Ix_label_sum_zero = Ix_label_sum_left_zero * Ix_label_sum_right_zero;
		I_gain = I_gain * (1 - Ix_label_sum_zero);		

			
		# determine best feature to split on and the split value
		max_I_gain_ind = as.scalar (rowIndexMax (t (I_gain)));
		p = ppred (cum_count_thresholds, max_I_gain_ind, "<");
		sum_cum_count_thresholds = sum (p);
		feature = sum_cum_count_thresholds + 1;
		split = max_I_gain_ind;
		if (feature != 1) {
			split = split - as.scalar(cum_count_thresholds[,(feature - 1)]);
		}
			
		# --- update model ---
		cur_F_large[1,i6] = feature;
		cur_S_large[1,i6] = thresholds[split, feature];
				
		left_child = 2 * (cur_node - 1) + 1 + 1;
		right_child = 2 * (cur_node - 1) + 2 + 1;
					
		# samples going to the left subtree
		Ix_left = X_ext_left[, max_I_gain_ind];
		Ix_left = Ix * Ix_left;
					
		Ix_right = Ix * (1 - Ix_left);
				
		L[,1] = L[,1] * (1 - Ix_left) + (Ix_left * left_child);
		L[,1] = L[,1] * (1 - Ix_right) + (Ix_right * right_child);				
			
		left_child_size = sum (Ix_left);
		right_child_size = sum (Ix_right);
			
		# print ("parent " + cur_node + ": size(" + left_child + ") --> " + left_child_size + " size(" + right_child + ") --> " + right_child_size);
				
		# check if left or right child is a leaf
		left_pure = FALSE;
		right_pure = FALSE;
		cur_entropy_left = as.scalar(entropy_left[max_I_gain_ind,]);
		cur_entropy_right = as.scalar(entropy_right[max_I_gain_ind,]);	
				
		if ( (left_child_size <= num_leaf | cur_entropy_left == 0 | (level == depth)) & (right_child_size <= num_leaf | cur_entropy_right == 0 | (level == depth)) ) { # both left and right nodes are leaf
				
			cur_label_counts_left = label_counts_left[max_I_gain_ind,];
			cur_NC_large[1,(2 * (i6 - 1) + 1)] = left_child; 
			cur_NC_large[2,(2 * (i6 - 1) + 1)] = as.scalar( rowIndexMax (cur_label_counts_left)); # leaf class label
			left_pure = TRUE;
			
			cur_label_counts_right = label_counts_overall - cur_label_counts_left;
			cur_NC_large[1,(2 * i6)] = right_child; 
			cur_NC_large[2,(2 * i6)] = as.scalar( rowIndexMax (cur_label_counts_right)); # leaf class label
			right_pure = TRUE;
					
		} else if (left_child_size <= num_leaf | cur_entropy_left == 0 | (level == depth)) {
				
			cur_label_counts_left = label_counts_left[max_I_gain_ind,];
			cur_NC_large[1,(2 * (i6 - 1) + 1)] = left_child; 
			cur_NC_large[2,(2 * (i6 - 1) + 1)] = as.scalar( rowIndexMax (cur_label_counts_left)); # leaf class label				
			left_pure = TRUE;	
			
		} else if (right_child_size <= num_leaf | cur_entropy_right == 0 | (level == depth)) {
	
			cur_label_counts_right = label_counts_right[max_I_gain_ind,];
			cur_NC_large[1,(2 * i6)] = right_child; 
			cur_NC_large[2,(2 * i6)] = as.scalar( rowIndexMax (cur_label_counts_right)); # leaf class label
			right_pure = TRUE;
	
		}		
									
		# add nodes to Q
		if (!left_pure) {
			if (left_child_size > threshold) {
				cur_Q_large[1,(2 * (i6 - 1)+ 1)] = left_child; 
			} else {
				cur_nodes_small[1,(2 * (i6 - 1)+ 1)] = left_child;
				cur_nodes_small[2,(2 * (i6 - 1)+ 1)] = left_child_size;
			}
		}
		if (!right_pure) {
			if (right_child_size > threshold) {
				cur_Q_large[1,(2 * i6)] = right_child;
			} else{
				cur_nodes_small[1,(2 * i6)] = right_child;
				cur_nodes_small[2,(2 * i6)] = right_child_size;					
			}	
		}	
	}
		
	# prepare model for LARGE nodes
	if (num_cur_nodes_large > 0) {
		cur_Q_large = removeEmpty (target = cur_Q_large, margin = "cols");
		if (as.scalar (cur_Q_large[1,1]) != 0) Q_large = append (Q_large, cur_Q_large);
		cur_NC_large = removeEmpty (target = cur_NC_large, margin = "cols");
		if (as.scalar (cur_NC_large[1,1]) != 0) NC_large = append (NC_large, cur_NC_large);
		F_large = append (F_large, cur_F_large);
		S_large = append (S_large, cur_S_large);
	
		num_cur_nodes_large_pre = 2 * num_cur_nodes_large;
		if (as.scalar (cur_Q_large[1,1]) == 0) {
			num_cur_nodes_large = 0;
		} else {
			cur_nodes_large = cur_Q_large;
			num_cur_nodes_large = ncol (cur_Q_large);
		}	
	}
		
	# -----------------------------------------------------	
	if (num_cur_nodes_small > 0) { # small nodes to process		
		reserve_len = sum (2 ^ (ceil (log (cur_nodes_small_nonzero[2,]))));
		cur_Q_small =  matrix (0, rows = 1, cols = reserve_len);
		cur_NC_small = matrix (0, rows = 2, cols = reserve_len);
		cur_F_small = cur_Q_small;
		cur_S_small = cur_Q_small;
	}
	
	# process small nodes
	parfor (i7 in 1:num_cur_nodes_small, check = 0) { # log = DEBUG
		  
		cur_node_small = as.scalar (cur_nodes_small_nonzero[1,i7]);
			
		print ("Expanding SMALL node " + cur_node_small);
						
		# build dataset for SMALL node
		Ix = ppred (L[,1], cur_node_small, "==");		
		X_ext_left_small = removeEmpty (target = X_ext_left * Ix, margin = "rows");
		L_small = removeEmpty (target = L * Ix, margin = "rows");
		Y_bin_small = removeEmpty (target = Y_bin * Ix, margin = "rows");
			
		# compute offset
		offsets = cumsum (t (2 ^ ceil (log (cur_nodes_small_nonzero[2,]))));
		start_ind = 1;
		if (i7 > 1) {
			start_ind = start_ind + as.scalar (offsets[(i7 - 1),]);
		}
		end_ind = as.scalar (offsets[i7,]);
			
		Q = matrix (cur_node_small, rows = 1, cols = 1); 
		NC = matrix (0, rows = 2, cols = 1);
		F = matrix (0, rows = 1, cols = 1);
		S = matrix (0, rows = 1, cols = 1); 

		cur_nodes_ = matrix (cur_node_small, rows = 1, cols = 1);

		num_cur_nodes = 1;
		level_ = 0;
		while (num_cur_nodes > 0 & level_ < depth) {
	
			level_ = level_ + 1;
			# log_str = "    === start level " + level_ + " for SMALL nodes === ";
			# print (log_str);
	
			cur_Q = matrix (0, rows = 1, cols = 2 * num_cur_nodes);

			cur_NC = matrix (0, rows = 2, cols = 2 * num_cur_nodes);
			cur_F = matrix (0, rows = 1, cols = num_cur_nodes);
			cur_S = cur_F;
		
			# loop over large nodes
			parfor (i8 in 1:num_cur_nodes, check = 0) { # log = DEBUG, opt = CONSTRAINED, mode = LOCAL) { 
			# for (i8 in 1:num_cur_nodes) {
			
				cur_node = as.scalar (cur_nodes_[,i8]);	
				# print ("    Expanding SMALL node: " + cur_node);
			
				/*
				print ("cur_nodes_");
				[success] = print_result2 (cur_nodes_);
				*/
			
				# --- find best split ---
				# samples that reach cur_node 
				Ix = ppred (L_small[,1], cur_node, "==");		
			
				cur_Y_bin = Y_bin_small * Ix;
				label_counts_overall = colSums (cur_Y_bin);
				label_sum_overall = sum (label_counts_overall);
				label_dist_overall = label_counts_overall / label_sum_overall;

				label_dist_zero = ppred (label_dist_overall, 0, "==");

				cur_entropy = - sum (label_dist_overall * log (label_dist_overall + label_dist_zero) / log (2)); # entropy before
			
				label_counts_left = t (t (cur_Y_bin) %*% X_ext_left_small);  
			
				# compute left and right label distribution
				label_sum_left = rowSums (label_counts_left);
				label_dist_left = label_counts_left / label_sum_left;
				label_dist_left = replace (target = label_dist_left, pattern = "NaN", replacement = 1);
				label_dist_left = replace (target = label_dist_left, pattern = 0, replacement = 1);
				log_label_dist_left = log (label_dist_left);
				entropy_left = - rowSums (label_dist_left * log_label_dist_left);
				#
				label_counts_right = - label_counts_left + label_counts_overall;
				label_sum_right = rowSums (label_counts_right);
				label_dist_right = label_counts_right / label_sum_right;
				label_dist_right = replace (target = label_dist_right, pattern = "NaN", replacement = 1);
				label_dist_right = replace (target = label_dist_right, pattern = 0, replacement = 1);
				log_label_dist_right = log (label_dist_right);
				entropy_right = - rowSums (label_dist_right * log_label_dist_right);			
			
				I_gain = cur_entropy - ( ( label_sum_left / label_sum_overall ) * entropy_left + ( label_sum_right / label_sum_overall ) * entropy_right);
			
				Ix_label_sum_left_zero = ppred (label_sum_left, 0, "==");
				Ix_label_sum_right_zero = ppred (label_sum_right, 0, "==");
				Ix_label_sum_zero = Ix_label_sum_left_zero * Ix_label_sum_right_zero;
				I_gain = I_gain * (1 - Ix_label_sum_zero);
				

				# determine best feature to split on and the split value
				max_I_gain_ind = as.scalar (rowIndexMax (t (I_gain)));
				p = ppred (cum_count_thresholds, max_I_gain_ind, "<");
				sum_cum_count_thresholds = sum (p);
				feature = sum_cum_count_thresholds + 1;
				split = max_I_gain_ind;
				if (feature != 1) {
					split = split - as.scalar(cum_count_thresholds[,(feature - 1)]);
				}
			
			
				# --- update model ---
				cur_F[1,i8] = feature; 
				cur_S[1,i8] = thresholds[split, feature];
			
				left_child = 2 * (cur_node - 1) + 1 + 1;
				right_child = 2 * (cur_node - 1) + 2 + 1;
				
		
				# samples going to the left subtree
				Ix_left = X_ext_left_small[, max_I_gain_ind];
				Ix_left = Ix * Ix_left;
			
				Ix_right = Ix * (1 - Ix_left);
			
				
				L_small[,1] = L_small[,1] * (1 - Ix_left) + (Ix_left * left_child);
				L_small[,1] = L_small[,1] * (1 - Ix_right) + (Ix_right * right_child);				
		
				left_child_size = sum (Ix_left);
				right_child_size = sum (Ix_right);
			
				# print ("    parent " + cur_node + ": size(" + left_child + ") --> " + left_child_size + " size(" + right_child + ") --> " + right_child_size);
				
				# check if left or right child is a leaf
				left_pure = FALSE;
				right_pure = FALSE;
				cur_entropy_left = as.scalar(entropy_left[max_I_gain_ind,]);
				cur_entropy_right = as.scalar(entropy_right[max_I_gain_ind,]);	
			
		
				if ( (left_child_size <= num_leaf | cur_entropy_left == 0 | (level_ == depth)) & (right_child_size <= num_leaf | cur_entropy_right == 0 | (level_ == depth)) ) { # both left and right nodes are leaf
				
					cur_label_counts_left = label_counts_left[max_I_gain_ind,];
					cur_NC[1,(2 * (i8 - 1) + 1)] = left_child; 
					cur_NC[2,(2 * (i8 - 1) + 1)] = as.scalar( rowIndexMax (cur_label_counts_left)); # leaf class label
					left_pure = TRUE;
			
					cur_label_counts_right = label_counts_overall - cur_label_counts_left;
					cur_NC[1,(2 * i8)] = right_child; 
					cur_NC[2,(2 * i8)] = as.scalar( rowIndexMax (cur_label_counts_right)); # leaf class label
					right_pure = TRUE;
				
				} else if (left_child_size <= num_leaf | cur_entropy_left == 0 | (level_ == depth)) {
				
					cur_label_counts_left = label_counts_left[max_I_gain_ind,];
					cur_NC[1,(2 * (i8 - 1) + 1)] = left_child; 
					cur_NC[2,(2 * (i8 - 1) + 1)] = as.scalar( rowIndexMax (cur_label_counts_left)); # leaf class label				
					left_pure = TRUE;	
		
				} else if (right_child_size <= num_leaf | cur_entropy_right == 0 | (level_ == depth)) {
	
					cur_label_counts_right = label_counts_right[max_I_gain_ind,];
					cur_NC[1,(2 * i8)] = right_child; 
					cur_NC[2,(2 * i8)] = as.scalar( rowIndexMax (cur_label_counts_right)); # leaf class label
					right_pure = TRUE;
	
				}		
									
				# add nodes to Q
				if (!left_pure) {
					cur_Q[1,(2 * (i8 - 1)+ 1)] = left_child;
				}
				if (!right_pure) {
					cur_Q[1,(2 * i8)] = right_child;
				}	
			}
		
			cur_Q = removeEmpty (target = cur_Q, margin = "cols");
			if (as.scalar (cur_Q[1,1]) != 0) Q = append (Q, cur_Q);
			cur_NC = removeEmpty (target = cur_NC, margin = "cols");
			if (as.scalar (cur_NC[1,1]) != 0) NC = append (NC, cur_NC);
			
			F = append (F, cur_F);
			S = append (S, cur_S);
		
			num_cur_nodes_pre = 2 * num_cur_nodes;
			if (as.scalar (cur_Q[1,1]) == 0) {
			num_cur_nodes = 0;
			} else {
				cur_nodes_ = cur_Q;
				num_cur_nodes = ncol (cur_Q);
			}
		
			# log_str = "    === end level " + level_ + " for SMALL nodes, remaining no. of SMALL nodes to expand " + num_cur_nodes + "  " + (t_end_small - t_start_small) + " (ms) --- ";
			# print (log_str);

		}
	
		cur_Q_small[,start_ind:(start_ind + ncol (Q) - 1)] = Q;
		cur_NC_small[,start_ind:(start_ind + ncol (NC) - 1 - 1)] = NC[,2:ncol (NC)];
		cur_F_small[,start_ind:(start_ind + ncol (F) - 1 - 1)] = F[,2:ncol (F)];
		cur_S_small[,start_ind:(start_ind + ncol (S) - 1 - 1)] = S[,2:ncol (S)];
			
		#-------------------------------------------------------------------------------
	}	
		
	# prepare model for SMALL nodes	
	if (num_cur_nodes_small > 0) {	# small nodes already processed
		
		cur_Q_small = removeEmpty (target = cur_Q_small, margin = "cols");
		# if (as.scalar (cur_Q_small[1,1]) != 0) 
		Q_small = append (Q_small, cur_Q_small);
		cur_NC_small = removeEmpty (target = cur_NC_small, margin = "cols");
		# if (as.scalar (cur_NC_small[1,1]) != 0) 
		NC_small = append (NC_small, cur_NC_small);
	
		cur_F_small = removeEmpty (target = cur_F_small, margin = "cols"); # 
		F_small = append (F_small, cur_F_small);
		cur_S_small = removeEmpty (target = cur_S_small, margin = "cols"); #		
		S_small = append (S_small, cur_S_small)
		
		num_cur_nodes_small = 0; # reset
	
	} else {
	
		cur_nodes_small_nonzero = removeEmpty (target = cur_nodes_small, margin = "cols");
		if (as.scalar (cur_nodes_small_nonzero[1,1]) != 0) { # if small nodes exist
			num_cur_nodes_small = ncol (cur_nodes_small_nonzero);
		}		
	}

	log_str = " --- end level " + level + ", remaining no. of LARGE nodes to expand " + num_cur_nodes_large + "  remaining no. of SMALL nodes to expand " + num_cur_nodes_small);
	print (log_str);

}

# prepare model
if (as.scalar (Q_large[1,1]) == 0 & ncol (Q_large) > 1) {
	Q_large = Q_large[,2:ncol (Q_large)];
}
if (as.scalar (NC_large[1,1]) == 0 & ncol (NC_large) > 1) {
	NC_large = NC_large[,2:ncol (NC_large)];
}
if (as.scalar (S_large[1,1]) == 0 & ncol (S_large) > 1) {
	S_large = S_large[,2:ncol (S_large)];
}
if (as.scalar (F_large[1,1]) == 0 & ncol (F_large) > 1) {
	F_large = F_large[,2:ncol (F_large)];
}
#--------------------------
if (as.scalar (Q_small[1,1]) == 0 & ncol (Q_small) > 1) {
	Q_small = Q_small[,2:ncol (Q_small)];
}
if (as.scalar (NC_small[1,1]) == 0 & ncol (NC_small) > 1) {
	NC_small = NC_small[,2:ncol (NC_small)];
}
if (as.scalar (S_small[1,1]) == 0 & ncol (S_small) > 1) {
	S_small = S_small[,2:ncol (S_small)];
}
if (as.scalar (F_small[1,1]) == 0 & ncol (F_small) > 1) {
	F_small = F_small[,2:ncol (F_small)];
}

num_large_internal = ncol (Q_large);

M1_large = matrix (0, rows = 5, cols = num_large_internal);
M1_large[1,] = Q_large;
M1_large[3,] = F_large;
M1_large[4,] = matrix (1, rows = 1, cols = num_large_internal);
M1_large[5,] = S_large;

num_small_internal = ncol (Q_small);
M1_small = matrix (0, rows = 5, cols = num_small_internal);
M1_small[1,] = Q_small;
M1_small[3,] = F_small;
M1_small[4,] = matrix (1, rows = 1, cols = num_small_internal);
M1_small[5,] = S_small;

num_large_leaf = ncol (NC_large);
M2_large = matrix (0, rows = 5, cols = num_large_leaf);
M2_large[1,] = NC_large[1,];
M2_large[4,] = NC_large[2,];

num_small_leaf = ncol (NC_small);
M2_small = matrix (0, rows = 5, cols = num_small_leaf);
M2_small[1,] = NC_small[1,];
M2_small[4,] = NC_small[2,];

M = append (append (M1_large, M1_small), append (M2_large, M2_small));
M = t (order (target = t (M), by = 1));

parfor (it in 1:ncol (Q_large), check = 0) {
# for (it in 1:ncol (Q_large), check = 0) {
	a1 = as.scalar (Q_large[1,it]);
	pos_a1 = sum (ppred (M[1,], a1, "<="));
	a2 = 2 * a1;
	pos_a2 = sum (ppred (M[1,], a2, "<="));
	M[2,pos_a1] = pos_a2 - pos_a1;
}


parfor (it2 in 1:ncol (Q_small), check = 0) {
# for (it2 in 1:ncol (Q_small), check = 0) {
	b1 = as.scalar (Q_small[1,it2]);
	pos_b1 = sum (ppred (M[1,], b1, "<="));
	b2 = 2 * b1;
	pos_b2 = sum (ppred (M[1,], b2, "<="));
	M[2,pos_b1] = pos_b2 - pos_b1;
}

write (M, fileM, fmtO);



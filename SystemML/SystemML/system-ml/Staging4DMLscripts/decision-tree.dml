#-------------------------------------------------------------
# IBM Confidential
# OCO Source Materials
# (C) Copyright IBM Corp. 2010, 2015
# The source code for this program is not published or
# otherwise divested of its trade secrets, irrespective of
# what has been deposited with the U.S. Copyright Office.
#-------------------------------------------------------------
#
# Implements classification trees (with scale features)
#
# Example Usage:
#
# hadoop jar SystemML.jar -f decision-tree.dml -nvargs X=$INPUT_DIR/X Y=$INPUT_DIR/Y model=$OUPUT_DIR/model bins=50 depth=10 num_leaf=1 fmt="csv"
#
# Args:
# X, Y: Input features and labels, respectively
# bins: Generates this many number of equiheight bins per scale feature to choose thresholds
# depth: Defines max depth of the learnt tree
# num_leaf: Defines the number of samples when splitting stops and a leaf node is added
# model: HDFS location where the learnt tree is stored; includes three row matrices where each entry corresponds to a node in a full binary tree: 
# N: contains leaf labels if the corresponding node is a leaf node and 0 otherwise
# F: contains which features are used for splitting for internal nodes and 0 otherwise
# S: contains the split value corresponding to features in F 
# fmt: Preferred format of the model files (default is ijv/"text")

# udf for binning
binning = externalFunction(Matrix[Double] A, Integer binsize, Integer numbins) return (Matrix[Double] B, Integer numbinsdef) 
	implemented in (classname="com.ibm.bi.dml.udf.lib.BinningWrapper",exectype="mem")

time = externalFunction(Integer i) return (Double B) implemented in
  	(classname="com.ibm.bi.dml.udf.lib.TimeWrapper", exectype="mem");
	
	
fmt = ifdef($fmt, "text");
num_bins = ifdef($bins, 20); 
depth = ifdef($depth, 25); 
num_leaf = ifdef($leaf, 10); 
threshold = ifdef ($thr, 3000); # 6000

# X = read($X)
# Y = read($Y)

# random data
# X = rand (rows = 20, cols = 5, min = 0, max = 10);
# Y = floor (rand (rows = 20, cols = 1, min = 1, max = 3.999999999999)); # 3.99999999999999

# iris
# M = read ("/temp/irisM");
# X = M[,1:4];
# Y = M[,5];

# MNIST small
# X = read ("/user/biadmin/sen/product_testing_new/train_data.mtx");
# Y = read ("/user/biadmin/sen/product_testing_new/train_label.mtx");

# MNIST 
# X = read ("/user/biadmin/fmakari/mnist.data");
# Y = read ("/user/biadmin/fmakari/mnist.label");

# MNIST scaled each entry divided by 255
X = read ("/user/biadmin/fmakari/mnist_train.data.mtx");
Y = read ("/user/biadmin/fmakari/mnist_train.labels.mtx");

# X = X[1:30000,];
# Y = Y[1:30000,];

num_rows = nrow (X);
num_features = ncol (X);

Y_bin = table (seq (1, num_rows), Y); # Y in binary representation
num_classes = ncol (Y_bin);

/*
print ("X");
[success] = print_result2 (X);
print ("Y");
[success] = print_result2 (Y);
*/

# --- binning ---
t_binning_start = time (1);
bin_size = max (as.integer (nrow(X) / num_bins), 1);
count_thresholds = matrix (0, rows = 1, cols = num_features)
thresholds = matrix (0, rows = num_bins + 1, cols = num_features)
print ("Computing binning...");
parfor(i1 in 1:num_features, log = DEBUG) {
      col = order (target = X[,i1], by = 1, decreasing = FALSE);
      [col_bins, num_bins_defined] = binning (col, bin_size, num_bins);
      count_thresholds[,i1] = num_bins_defined;
      thresholds[,i1] = col_bins;	
}
t_binning_end = time (1);
log_str = "...binning finished " + (t_binning_end - t_binning_start) + " (ms)";
print (log_str);

/*
print ("threshold");
[success] = print_result2 (thresholds);	
print ("count_threshold");
[success] = print_result2 (count_thresholds);
*/

# --- preprocessing --- 
t_pre_start = time (1);
min_num_bins = min (count_thresholds);
max_num_bins = max (count_thresholds);
total_num_bins = sum (count_thresholds);
cum_count_thresholds = t (cumsum (t (count_thresholds)));
log_str = "min no. bins: " + (min_num_bins + 1) + " max no. bins: " + (max_num_bins + 1) + " total no. bins: " + (total_num_bins + num_features);
print (log_str);


X_ext_left = matrix (0, rows = num_rows, cols = total_num_bins);
print ("Computing preprocessed dataset...");
parfor (i2 in 1:num_features, check = 0, log = DEBUG, opt = CONSTRAINED, mode = REMOTE_MR) {
# parfor (i2 in 1:num_features, check = 0) {
	Xi2 = X[,i2];
	count_threshold = as.scalar (count_thresholds[,i2]);
	offset_feature = 0;
	if (i2 > 1) offset_feature = as.integer (as.scalar (cum_count_thresholds[, (i2 - 1)]));
	parfor (i3 in 0:(count_threshold - 1), check = 0) {
		cur_threshold = count_threshold - i3;
		I = ppred (Xi2, as.scalar(thresholds[cur_threshold, i2]), "<");
		X_ext_left[,(offset_feature + cur_threshold)] = I;
	}
}
t_pre_end = time (1);
log_str = "...preprocessing finished " + (t_pre_end - t_pre_start) + " (ms)";
print (log_str);

# write (X_ext_left, "/user/biadmin/fmakari/mnist_train.data.processed.mtx", format = "binary");
# stop (" ");

# X_ext_left = read ("/user/biadmin/fmakari/mnist_train.data.processed.mtx");

/*
print ("X_ext_left");
[success] = print_result (X_ext_left, cum_count_thresholds, 1);
*/

# --- initialization ---
L = matrix (1, rows = num_rows, cols = 1); # last visited

# model
# leaf nodes
NC_large = matrix (0, rows = 2, cols = 1);
NC_small = matrix (0, rows = 2, cols = 1);

# internal nodes
Q_large = matrix (1, rows = 1, cols = 1); # ids of LARGE internal nodes 
Q_small = matrix (0, rows = 1, cols = 1); # ids of SMALL internal nodes

F_large = matrix (0, rows = 1, cols = 1); # features
F_small = matrix (0, rows = 1, cols = 1); # features
S_large = matrix (0, rows = 1, cols = 1); # split points
S_small = matrix (0, rows = 1, cols = 1); # split points

# initialization
cur_nodes_large = matrix (1, rows = 1, cols = 1);
# I_nodes = matrix (1, rows = 1, cols = 1);
# L_nodes = matrix (0, rows = 1, cols = 1);
cur_nodes_small_nonzero = matrix (0, rows = 2, cols = 2); # first row: ids of SMALL internal nodes, second row: no. of samples for these nodes
# Ix_left = matrix (0, rows = 1, cols = 1);

num_cur_nodes_large = 1;
num_cur_nodes_small = 0;
level = 0;
while ((num_cur_nodes_large + num_cur_nodes_small) > 0 & level < depth) {
	
	level = level + 1;
	log_str = " --- start level " + level + " --- ";
	print (log_str);
	
	if (num_cur_nodes_large > 0) { # large nodes to process
		cur_Q_large = matrix (0, rows = 1, cols = 2 * num_cur_nodes_large);
		cur_NC_large = matrix (0, rows = 2, cols = 2 * num_cur_nodes_large);
		cur_F_large = matrix (0, rows = 1, cols = num_cur_nodes_large);
		cur_S_large = cur_F_large;
		cur_nodes_small = cur_NC_large; 
	}
		
	# loop over large nodes
	t_start_large = time (1);
	parfor (i6 in 1:num_cur_nodes_large, check = 0, log = DEBUG) { 
	# for (i6 in 1:num_cur_nodes_large) {
				
		cur_node = as.scalar (cur_nodes_large[,i6]);	
		# print (" Expanding LARGE node: " + cur_node);
			
		/*
		print ("cur_nodes_large");
		[success] = print_result2 (cur_nodes_large);
		*/
			
		# --- find best split ---
		# samples that reach cur_node 
		Ix = ppred (L[,1], cur_node, "==");		
			
		cur_Y_bin = Y_bin * Ix;
		label_counts_overall = colSums (cur_Y_bin);
		label_sum_overall = sum (label_counts_overall);
		label_dist_overall = label_counts_overall / label_sum_overall;
		label_dist_zero = ppred (label_dist_overall, 0, "==");

		cur_entropy = - sum (label_dist_overall * log (label_dist_overall + label_dist_zero) / log (2)); # entropy before
			
		# main operation	
		label_counts_left = t (t (cur_Y_bin) %*% X_ext_left);  
		
		# compute left and right label distribution
		label_sum_left = rowSums (label_counts_left);
		label_dist_left = label_counts_left / label_sum_left;
		label_dist_left = replace (target = label_dist_left, pattern = "NaN", replacement = 1);
		label_dist_left = replace (target = label_dist_left, pattern = 0, replacement = 1);
		log_label_dist_left = log (label_dist_left);
		entropy_left = - rowSums (label_dist_left * log_label_dist_left);
		#
		label_counts_right = - label_counts_left + label_counts_overall;
		label_sum_right = rowSums (label_counts_right);
		label_dist_right = label_counts_right / label_sum_right;
		label_dist_right = replace (target = label_dist_right, pattern = "NaN", replacement = 1);
		label_dist_right = replace (target = label_dist_right, pattern = 0, replacement = 1);
		log_label_dist_right = log (label_dist_right);
		entropy_right = - rowSums (label_dist_right * log_label_dist_right);			
			
		I_gain = cur_entropy - ( ( label_sum_left / label_sum_overall ) * entropy_left + ( label_sum_right / label_sum_overall ) * entropy_right);
			
		Ix_label_sum_left_zero = ppred (label_sum_left, 0, "==");
		Ix_label_sum_right_zero = ppred (label_sum_right, 0, "==");
		Ix_label_sum_zero = Ix_label_sum_left_zero * Ix_label_sum_right_zero;
		I_gain = I_gain * (1 - Ix_label_sum_zero);		
		
		/*
		print ("label_counts_left at node "); 
		[success] = print_result (label_counts_left, cum_count_thresholds, num_classes);
		print ("label_counts_right at node ");
		[success] = print_result (label_counts_right, cum_count_thresholds, num_classes);
		print ("label_counts_overall at node ");
		[success] = print_result (label_counts_overall, cum_count_thresholds, num_classes);
		print ("label_dist_left at node ");
		[success] = print_result (label_dist_left, cum_count_thresholds, num_classes);
		print ("label_dist_right at node ");
		[success] = print_result (label_dist_right, cum_count_thresholds, num_classes);	
		print ("label_dist_overall at node ");		
		[success] = print_result (label_dist_overall, cum_count_thresholds, num_classes);
		
		log_str = "no. of sample at node " + cur_node + " : " + label_sum_overall;
		print (log_str);
		
		print ("I gain at node ");
		[success] = print_result (I_gain, cum_count_thresholds, 1);	
		print ("entropy_left at node ");
		[success] = print_result (entropy_left, cum_count_thresholds, 1);
		print ("entropy_right at node ");
		[success] = print_result (entropy_right, cum_count_thresholds, 1);
		*/
			
		# determine best feature to split on and the split value
		max_I_gain_ind = as.scalar (rowIndexMax (t (I_gain)));
		p = ppred (cum_count_thresholds, max_I_gain_ind, "<");
		sum_cum_count_thresholds = sum (p);
		feature = sum_cum_count_thresholds + 1;
		split = max_I_gain_ind;
		if (feature != 1) {
			split = split - as.scalar(cum_count_thresholds[,(feature - 1)]);
		}
			
		/*	
		log_str = " feature: " + feature + " split: " + split + " max_I_gain_ind: " + max_I_gain_ind;
		print (log_str);	
		*/	
			
		# --- update model ---
		cur_F_large[1,i6] = feature;
		cur_S_large[1,i6] = thresholds[split, feature];
				
		left_child = 2 * (cur_node - 1) + 1 + 1;
		right_child = 2 * (cur_node - 1) + 2 + 1;
					
		# samples going to the left subtree
		Ix_left = X_ext_left[, max_I_gain_ind];
		Ix_left = Ix * Ix_left;
					
		Ix_right = Ix * (1 - Ix_left);
			
		/*
		print ("Ix");
		[success] = print_result2 (Ix);
		print ("Ix_left");
		[success] = print_result2 (Ix_left);
		print ("Ix_right");
		[success] = print_result2 (Ix_right);			
		*/	
				
		L[,1] = L[,1] * (1 - Ix_left) + (Ix_left * left_child);
		L[,1] = L[,1] * (1 - Ix_right) + (Ix_right * right_child);				
			
		left_child_size = sum (Ix_left);
		right_child_size = sum (Ix_right);
			
		# print ("parent " + cur_node + ": size(" + left_child + ") --> " + left_child_size + " size(" + right_child + ") --> " + right_child_size);
				
		# check if left or right child is a leaf
		left_pure = FALSE;
		right_pure = FALSE;
		cur_entropy_left = as.scalar(entropy_left[max_I_gain_ind,]);
		cur_entropy_right = as.scalar(entropy_right[max_I_gain_ind,]);	
				
		if ( (left_child_size <= num_leaf | cur_entropy_left == 0 | (level == depth)) & (right_child_size <= num_leaf | cur_entropy_right == 0 | (level == depth)) ) { # both left and right nodes are leaf
				
			cur_label_counts_left = label_counts_left[max_I_gain_ind,];
			cur_NC_large[1,(2 * (i6 - 1) + 1)] = left_child; 
			cur_NC_large[2,(2 * (i6 - 1) + 1)] = as.scalar( rowIndexMax (cur_label_counts_left)); # leaf class label
			left_pure = TRUE;
			
			cur_label_counts_right = label_counts_overall - cur_label_counts_left;
			cur_NC_large[1,(2 * i6)] = right_child; 
			cur_NC_large[2,(2 * i6)] = as.scalar( rowIndexMax (cur_label_counts_right)); # leaf class label
			right_pure = TRUE;
					
		} else if (left_child_size <= num_leaf | cur_entropy_left == 0 | (level == depth)) {
				
			cur_label_counts_left = label_counts_left[max_I_gain_ind,];
			cur_NC_large[1,(2 * (i6 - 1) + 1)] = left_child; 
			cur_NC_large[2,(2 * (i6 - 1) + 1)] = as.scalar( rowIndexMax (cur_label_counts_left)); # leaf class label				
			left_pure = TRUE;	
			
		} else if (right_child_size <= num_leaf | cur_entropy_right == 0 | (level == depth)) {
	
			cur_label_counts_right = label_counts_right[max_I_gain_ind,];
			cur_NC_large[1,(2 * i6)] = right_child; 
			cur_NC_large[2,(2 * i6)] = as.scalar( rowIndexMax (cur_label_counts_right)); # leaf class label
			right_pure = TRUE;
	
		}		
									
		# add nodes to Q
		if (!left_pure) {
			if (left_child_size > threshold) {
				cur_Q_large[1,(2 * (i6 - 1)+ 1)] = left_child; 
			} else {
				cur_nodes_small[1,(2 * (i6 - 1)+ 1)] = left_child;
				cur_nodes_small[2,(2 * (i6 - 1)+ 1)] = left_child_size;
			}
		}
		if (!right_pure) {
			if (right_child_size > threshold) {
				cur_Q_large[1,(2 * i6)] = right_child;
			} else{
				cur_nodes_small[1,(2 * i6)] = right_child;
				cur_nodes_small[2,(2 * i6)] = right_child_size;					
			}	
		}	
	}
		
	# prepare model for LARGE nodes
	if (num_cur_nodes_large > 0) {
		cur_Q_large = removeEmpty (target = cur_Q_large, margin = "cols");
		if (as.scalar (cur_Q_large[1,1]) != 0) Q_large = append (Q_large, cur_Q_large);
		cur_NC_large = removeEmpty (target = cur_NC_large, margin = "cols");
		if (as.scalar (cur_NC_large[1,1]) != 0) NC_large = append (NC_large, cur_NC_large);
	
		F_large = append (F_large, cur_F_large);
		S_large = append (S_large, cur_S_large);
	
		num_cur_nodes_large_pre = 2 * num_cur_nodes_large;
		if (as.scalar (cur_Q_large[1,1]) == 0) {
			num_cur_nodes_large = 0;
		} else {
			cur_nodes_large = cur_Q_large;
			num_cur_nodes_large = ncol (cur_Q_large);
		}	
	}
	t_end_large = time (1);
		
	# -----------------------------------------------------	
	if (num_cur_nodes_small > 0) { # small nodes to process		
		reserve_len = sum (2 ^ (ceil (log (cur_nodes_small_nonzero[2,]))));
		cur_Q_small =  matrix (0, rows = 1, cols = reserve_len);
		cur_NC_small = matrix (0, rows = 2, cols = reserve_len);
		cur_F_small = cur_Q_small;
		cur_S_small = cur_Q_small;
	}
	
	t_start_small_all = time (1);
	# process small nodes
	parfor (i7 in 1:num_cur_nodes_small, check = 0, log = DEBUG) {
		  
		cur_node_small = as.scalar (cur_nodes_small_nonzero[1,i7]);
			
		print ("Expanding SMALL node " + cur_node_small);
						
		# build dataset for SMALL node
		Ix = ppred (L[,1], cur_node_small, "==");		
		X_ext_left_small = removeEmpty (target = X_ext_left * Ix, margin = "rows");
		L_small = removeEmpty (target = L * Ix, margin = "rows");
		Y_bin_small = removeEmpty (target = Y_bin * Ix, margin = "rows");
			
		# compute offset
		offsets = cumsum (t (2 ^ ceil (log (cur_nodes_small_nonzero[2,]))));
		start_ind = 1;
		if (i7 > 1) {
			start_ind = start_ind + as.scalar (offsets[(i7 - 1),]);
		}
		end_ind = as.scalar (offsets[i7,]);
			
		/*
		print ("cur_nodes_small_nonzero");
		[success] = print_result2 (cur_nodes_small_nonzero);			
		print ("offsets");
		[success] = print_result2 (offsets);
		*/
			
		Q = matrix (cur_node_small, rows = 1, cols = 1); 
		NC = matrix (0, rows = 2, cols = 1);
		F = matrix (0, rows = 1, cols = 1);
		S = matrix (0, rows = 1, cols = 1); 

		cur_nodes_ = matrix (cur_node_small, rows = 1, cols = 1);

		num_cur_nodes = 1;
		level_ = 0;
		while (num_cur_nodes > 0 & level_ < depth) {
	
			level_ = level_ + 1;
			# log_str = "    === start level " + level_ + " for SMALL nodes === ";
			# print (log_str);
	
			cur_Q = matrix (0, rows = 1, cols = 2 * num_cur_nodes);

			cur_NC = matrix (0, rows = 2, cols = 2 * num_cur_nodes);
			cur_F = matrix (0, rows = 1, cols = num_cur_nodes);
			cur_S = cur_F;
		
			# loop over large nodes
			t_start_small = time (1);
			parfor (i8 in 1:num_cur_nodes, check = 0, log = DEBUG) { #, opt = CONSTRAINED, mode = LOCAL) { 
			# for (i8 in 1:num_cur_nodes) {
			
				cur_node = as.scalar (cur_nodes_[,i8]);	
				# print ("    Expanding SMALL node: " + cur_node);
			
				/*
				print ("cur_nodes_");
				[success] = print_result2 (cur_nodes_);
				*/
			
				# --- find best split ---
				# samples that reach cur_node 
				Ix = ppred (L_small[,1], cur_node, "==");		
			
				cur_Y_bin = Y_bin_small * Ix;
				label_counts_overall = colSums (cur_Y_bin);
				label_sum_overall = sum (label_counts_overall);
				label_dist_overall = label_counts_overall / label_sum_overall;

				label_dist_zero = ppred (label_dist_overall, 0, "==");

				cur_entropy = - sum (label_dist_overall * log (label_dist_overall + label_dist_zero) / log (2)); # entropy before
			
				label_counts_left = t (t (cur_Y_bin) %*% X_ext_left_small);  
			
				# compute left and right label distribution
				label_sum_left = rowSums (label_counts_left);
				label_dist_left = label_counts_left / label_sum_left;
				label_dist_left = replace (target = label_dist_left, pattern = "NaN", replacement = 1);
				label_dist_left = replace (target = label_dist_left, pattern = 0, replacement = 1);
				log_label_dist_left = log (label_dist_left);
				entropy_left = - rowSums (label_dist_left * log_label_dist_left);
				#
				label_counts_right = - label_counts_left + label_counts_overall;
				label_sum_right = rowSums (label_counts_right);
				label_dist_right = label_counts_right / label_sum_right;
				label_dist_right = replace (target = label_dist_right, pattern = "NaN", replacement = 1);
				label_dist_right = replace (target = label_dist_right, pattern = 0, replacement = 1);
				log_label_dist_right = log (label_dist_right);
				entropy_right = - rowSums (label_dist_right * log_label_dist_right);			
			
				I_gain = cur_entropy - ( ( label_sum_left / label_sum_overall ) * entropy_left + ( label_sum_right / label_sum_overall ) * entropy_right);
			
				Ix_label_sum_left_zero = ppred (label_sum_left, 0, "==");
				Ix_label_sum_right_zero = ppred (label_sum_right, 0, "==");
				Ix_label_sum_zero = Ix_label_sum_left_zero * Ix_label_sum_right_zero;
				I_gain = I_gain * (1 - Ix_label_sum_zero);
			
		
				/*
				print ("label_counts_left at node "); 
				[success] = print_result (label_counts_left, cum_count_thresholds, num_classes);
				print ("label_counts_right at node ");
				[success] = print_result (label_counts_right, cum_count_thresholds, num_classes);
				print ("label_counts_overall at node ");
				[success] = print_result (label_counts_overall, cum_count_thresholds, num_classes);
				print ("label_dist_left at node ");
				[success] = print_result (label_dist_left, cum_count_thresholds, num_classes);
				print ("label_dist_right at node ");
				[success] = print_result (label_dist_right, cum_count_thresholds, num_classes);	
				print ("label_dist_overall at node ");		
				[success] = print_result (label_dist_overall, cum_count_thresholds, num_classes);
		
				log_str = "no. of sample at node " + cur_node + " : " + label_sum_overall;
				print (log_str);
		
				print ("I gain at node ");
				[success] = print_result (I_gain, cum_count_thresholds, 1);	
				print ("entropy_left at node ");
				[success] = print_result (entropy_left, cum_count_thresholds, 1);
				print ("entropy_right at node ");
				[success] = print_result (entropy_right, cum_count_thresholds, 1);
				*/
			

				# determine best feature to split on and the split value
				max_I_gain_ind = as.scalar (rowIndexMax (t (I_gain)));
				p = ppred (cum_count_thresholds, max_I_gain_ind, "<");
				sum_cum_count_thresholds = sum (p);
				feature = sum_cum_count_thresholds + 1;
				split = max_I_gain_ind;
				if (feature != 1) {
					split = split - as.scalar(cum_count_thresholds[,(feature - 1)]);
				}
			
				/*	
				log_str = " feature: " + feature + " split: " + split + " max_I_gain_ind: " + max_I_gain_ind;
				print (log_str);	
				*/
			
				# --- update model ---
				cur_F[1,i8] = feature; 
				cur_S[1,i8] = thresholds[split, feature];
			
				left_child = 2 * (cur_node - 1) + 1 + 1;
				right_child = 2 * (cur_node - 1) + 2 + 1;
				
		
				# samples going to the left subtree
				Ix_left = X_ext_left_small[, max_I_gain_ind];
				Ix_left = Ix * Ix_left;
			
				Ix_right = Ix * (1 - Ix_left);
			
				/*
				print ("Ix");
				[success] = print_result2 (Ix);
				print ("Ix_left");
				[success] = print_result2 (Ix_left);
				print ("Ix_right");
				[success] = print_result2 (Ix_right);			
				*/	
				
				L_small[,1] = L_small[,1] * (1 - Ix_left) + (Ix_left * left_child);
				L_small[,1] = L_small[,1] * (1 - Ix_right) + (Ix_right * right_child);				
		
				left_child_size = sum (Ix_left);
				right_child_size = sum (Ix_right);
			
				# print ("    parent " + cur_node + ": size(" + left_child + ") --> " + left_child_size + " size(" + right_child + ") --> " + right_child_size);
				
				# check if left or right child is a leaf
				left_pure = FALSE;
				right_pure = FALSE;
				cur_entropy_left = as.scalar(entropy_left[max_I_gain_ind,]);
				cur_entropy_right = as.scalar(entropy_right[max_I_gain_ind,]);	
			
		
				if ( (left_child_size <= num_leaf | cur_entropy_left == 0 | (level_ == depth)) & (right_child_size <= num_leaf | cur_entropy_right == 0 | (level_ == depth)) ) { # both left and right nodes are leaf
				
					cur_label_counts_left = label_counts_left[max_I_gain_ind,];
					cur_NC[1,(2 * (i8 - 1) + 1)] = left_child; 
					cur_NC[2,(2 * (i8 - 1) + 1)] = as.scalar( rowIndexMax (cur_label_counts_left)); # leaf class label
					left_pure = TRUE;
			
					cur_label_counts_right = label_counts_overall - cur_label_counts_left;
					cur_NC[1,(2 * i8)] = right_child; 
					cur_NC[2,(2 * i8)] = as.scalar( rowIndexMax (cur_label_counts_right)); # leaf class label
					right_pure = TRUE;
				
				} else if (left_child_size <= num_leaf | cur_entropy_left == 0 | (level_ == depth)) {
				
					cur_label_counts_left = label_counts_left[max_I_gain_ind,];
					cur_NC[1,(2 * (i8 - 1) + 1)] = left_child; 
					cur_NC[2,(2 * (i8 - 1) + 1)] = as.scalar( rowIndexMax (cur_label_counts_left)); # leaf class label				
					left_pure = TRUE;	
		
				} else if (right_child_size <= num_leaf | cur_entropy_right == 0 | (level_ == depth)) {
	
					cur_label_counts_right = label_counts_right[max_I_gain_ind,];
					cur_NC[1,(2 * i8)] = right_child; 
					cur_NC[2,(2 * i8)] = as.scalar( rowIndexMax (cur_label_counts_right)); # leaf class label
					right_pure = TRUE;
	
				}		
									
				# add nodes to Q
				if (!left_pure) {
					cur_Q[1,(2 * (i8 - 1)+ 1)] = left_child;
				}
				if (!right_pure) {
					cur_Q[1,(2 * i8)] = right_child;
				}	
			}
		
			cur_Q = removeEmpty (target = cur_Q, margin = "cols");
			if (as.scalar (cur_Q[1,1]) != 0) Q = append (Q, cur_Q);
			cur_NC = removeEmpty (target = cur_NC, margin = "cols");
			if (as.scalar (cur_NC[1,1]) != 0) NC = append (NC, cur_NC);
	
			F = append (F, cur_F);
			S = append (S, cur_S);
		
			/*
			log_str = " --- summary of SMALL nodes at level " + level_ + " --- ";
			print (log_str);
	
			print ("Q");
			[success] = print_result2 (Q);	
			print ("NC");
			[success] = print_result2 (NC);
			print ("F");
			[success] = print_result2 (F);
			print ("S");
			[success] = print_result2 (S);		
			print ("L_small");
			[success] = print_result2 (L_small);
			*/
		
			num_cur_nodes_pre = 2 * num_cur_nodes;
			if (as.scalar (cur_Q[1,1]) == 0) {
			num_cur_nodes = 0;
			} else {
				cur_nodes_ = cur_Q;
				num_cur_nodes = ncol (cur_Q);
			}
		
			t_end_small = time (1);
			# log_str = "    === end level " + level_ + " for SMALL nodes, remaining no. of SMALL nodes to expand " + num_cur_nodes + "  " + (t_end_small - t_start_small) + " (ms) --- ";
			# print (log_str);

		}
	
		cur_Q_small[,start_ind:(start_ind + ncol (Q) - 1)] = Q;
		cur_NC_small[,start_ind:(start_ind + ncol (NC) - 1 - 1)] = NC[,2:ncol (NC)];
		cur_F_small[,start_ind:(start_ind + ncol (F) - 1 - 1)] = F[,2:ncol (F)];
		cur_S_small[,start_ind:(start_ind + ncol (S) - 1 - 1)] = S[,2:ncol (S)];
			
		#-------------------------------------------------------------------------------
	}	
		
	# prepare model for SMALL nodes	
	if (num_cur_nodes_small > 0) {	# small nodes already processed
		
		cur_Q_small = removeEmpty (target = cur_Q_small, margin = "cols");
		# if (as.scalar (cur_Q_small[1,1]) != 0) 
		Q_small = append (Q_small, cur_Q_small);
		cur_NC_small = removeEmpty (target = cur_NC_small, margin = "cols");
		# if (as.scalar (cur_NC_small[1,1]) != 0) 
		NC_small = append (NC_small, cur_NC_small);
	
		cur_F_small = removeEmpty (target = cur_F_small, margin = "cols"); # 
		F_small = append (F_small, cur_F_small);
		cur_S_small = removeEmpty (target = cur_S_small, margin = "cols"); #		
		S_small = append (S_small, cur_S_small)
		
		num_cur_nodes_small = 0; # reset
	
	} else {
	
		cur_nodes_small_nonzero = removeEmpty (target = cur_nodes_small, margin = "cols");
		if (as.scalar (cur_nodes_small_nonzero[1,1]) != 0) { # if small nodes exist
			num_cur_nodes_small = ncol (cur_nodes_small_nonzero);
		}		
		# print ("cur_nodes_small");
		# [success] = print_result2 (cur_nodes_small);
		# print ("cur_nodes_small_nonzero");
		# [success] = print_result2 (cur_nodes_small_nonzero);
	}
	t_end_small_all = time (1);
	
	/*
	log_str = " --- summary of SMALL nodes at level " + level + " --- ";
	print (log_str);
	
	print ("Q_small");
	[success] = print_result2 (Q_small);	
	print ("NC_small");
	[success] = print_result2 (NC_small);
	print ("F_small");
	[success] = print_result2 (F_small);
	print ("S_small");
	[success] = print_result2 (S_small);		
	*/
	
	/*
	print ("Q_large");
	[success] = print_result2 (Q_large);	
	print ("cur_Q_large nrow " + nrow (cur_Q_large) + " ncol " + ncol (cur_Q_large));
	[success] = print_result2 (cur_Q_large);	
	print ("NC_large");
	[success] = print_result2 (NC_large);	
	print ("cur_NC_large");
	[success] = print_result2 (cur_NC_large);	
	print ("F_large");
	[success] = print_result2 (F_large);	
	print ("cur_F_large");
	[success] = print_result2 (cur_F_large);	
	print ("S_large");
	[success] = print_result2 (S_large);	
	print ("cur_S_large");
	[success] = print_result2 (cur_S_large);	
	*/
	
	
	/*
	log_str = " --- summary of LARGE nodes at level " + level + " --- ";
	print (log_str);
	
	print ("Q_large");
	[success] = print_result2 (Q_large);	
	print ("NC_large");
	[success] = print_result2 (NC_large);
	print ("F_large");
	[success] = print_result2 (F_large);
	print ("S_large");
	[success] = print_result2 (S_large);	
	print ("L_small");
	[success] = print_result2 (L_small);	
	*/
		

	log_str = " --- end level " + level + ", remaining no. of LARGE nodes to expand " + num_cur_nodes_large + "  remaining no. of SMALL nodes to expand " + num_cur_nodes_small + "  time for LARGE nodes " + (t_end_large - t_start_large) + " (ms) and time for SMALL nodes " + (t_end_small_all - t_start_small_all) + " (ms) --- ";
	print (log_str);

}


# printing functions
print_result = function (Matrix[Double] X, Matrix[Double] cum_count_thresholds, Integer num_classes) return (Boolean success) {
	out_str = " =";
	for (i in 1:nrow(X)) {
		out_line = i+ " : ";
		ix = 1;
		for (j in 1:ncol(X)) {
			if (j == as.scalar (cum_count_thresholds[1,ix]) * num_classes) {
				out_line = out_line + as.double (castAsScalar (X[i, j])) + " | ";
				ix = ix + 1;
			} else {
				out_line = out_line + as.double (castAsScalar (X[i, j])) + " ";
			}
		}
		out_str = append (out_str, out_line);	
	}
	print (out_str);
	success = TRUE;
}

print_result2 = function (Matrix[Double] X) return (Boolean success) {
	out_str = " =";
	for (i in 1:nrow(X)) {
		out_line = i+ " : ";
		level = 1;
		for (j in 1:ncol(X)) {
			if (j == level) {
				out_line = out_line + as.double (castAsScalar (X[i, j])) + " | ";
				level = level + 1;
				level = (2 * level) - 1 ;
			} else {
				out_line = out_line + as.double (castAsScalar (X[i, j])) + " ";
			}
		}
		out_str = append (out_str, out_line);	
	}
	print (out_str);
	success = TRUE;
}


